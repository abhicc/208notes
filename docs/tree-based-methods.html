<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Tree-Based Methods | 208 Course Notes</title>
  <meta name="description" content="Chapter 8 Tree-Based Methods | 208 Course Notes" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Tree-Based Methods | 208 Course Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Tree-Based Methods | 208 Course Notes" />
  
  
  

<meta name="author" content="Abhishek Chakraborty" />


<meta name="date" content="2023-03-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-model-selection-and-regularization.html"/>
<link rel="next" href="support-vector-machines-svm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">208 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2</b> What is Machine Learning?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#what-is-machine-learning-1"><i class="fa fa-check"></i><b>2.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question"><i class="fa fa-check"></i><b>2.2</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#statistical-learning-vs-machine-learning-vs-data-science"><i class="fa fa-check"></i><b>2.3</b> Statistical Learning vs Machine Learning vs Data Science</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#notations"><i class="fa fa-check"></i><b>2.4</b> Notations</a></li>
<li class="chapter" data-level="2.5" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#notations-1"><i class="fa fa-check"></i><b>2.5</b> Notations</a></li>
<li class="chapter" data-level="2.6" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question-1"><i class="fa fa-check"></i><b>2.6</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.7" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question-2"><i class="fa fa-check"></i><b>2.7</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.8" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-vs-unsupervised"><i class="fa fa-check"></i><b>2.8</b> Supervised vs Unsupervised</a></li>
<li class="chapter" data-level="2.9" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.9</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.10" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-1"><i class="fa fa-check"></i><b>2.10</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.11" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.11</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="2.12" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question-3"><i class="fa fa-check"></i><b>2.12</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.13" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-2"><i class="fa fa-check"></i><b>2.13</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.14" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-3"><i class="fa fa-check"></i><b>2.14</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.15" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-4"><i class="fa fa-check"></i><b>2.15</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.16" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-5"><i class="fa fa-check"></i><b>2.16</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.17" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-why-estimate-fmathbfx"><i class="fa fa-check"></i><b>2.17</b> Supervised Learning: Why Estimate <span class="math inline">\(f(\mathbf{X})\)</span>?</a></li>
<li class="chapter" data-level="2.18" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-prediction-and-inference"><i class="fa fa-check"></i><b>2.18</b> Supervised Learning: Prediction and Inference</a></li>
<li class="chapter" data-level="2.19" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-prediction-and-inference-1"><i class="fa fa-check"></i><b>2.19</b> Supervised Learning: Prediction and Inference</a></li>
<li class="chapter" data-level="2.20" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-prediction"><i class="fa fa-check"></i><b>2.20</b> Supervised Learning: Prediction</a></li>
<li class="chapter" data-level="2.21" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question-4"><i class="fa fa-check"></i><b>2.21</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.22" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-how-do-we-estimate-fmathbfx"><i class="fa fa-check"></i><b>2.22</b> Supervised Learning: How Do We Estimate <span class="math inline">\(f(\mathbf{X})\)</span>?</a></li>
<li class="chapter" data-level="2.23" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-parametric-methods"><i class="fa fa-check"></i><b>2.23</b> Supervised Learning: Parametric Methods</a></li>
<li class="chapter" data-level="2.24" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-parametric-methods-1"><i class="fa fa-check"></i><b>2.24</b> Supervised Learning: Parametric Methods</a></li>
<li class="chapter" data-level="2.25" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-non-parametric-methods"><i class="fa fa-check"></i><b>2.25</b> Supervised Learning: Non-parametric Methods</a></li>
<li class="chapter" data-level="2.26" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-flexibility-of-models"><i class="fa fa-check"></i><b>2.26</b> Supervised Learning: Flexibility of Models</a></li>
<li class="chapter" data-level="2.27" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-some-trade-offs"><i class="fa fa-check"></i><b>2.27</b> Supervised Learning: Some Trade-offs</a></li>
<li class="chapter" data-level="2.28" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-some-trade-offs-1"><i class="fa fa-check"></i><b>2.28</b> Supervised Learning: Some Trade-offs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html"><i class="fa fa-check"></i><b>3</b> Supervised Learning: Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="3.1" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-1"><i class="fa fa-check"></i><b>3.1</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.2" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-2"><i class="fa fa-check"></i><b>3.2</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.3" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-3"><i class="fa fa-check"></i><b>3.3</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.4" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-4"><i class="fa fa-check"></i><b>3.4</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.5" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-5"><i class="fa fa-check"></i><b>3.5</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.6" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-6"><i class="fa fa-check"></i><b>3.6</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.7" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-bias-variance-trade-off"><i class="fa fa-check"></i><b>3.7</b> Supervised Learning: Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="3.8" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-bias-variance-trade-off-1"><i class="fa fa-check"></i><b>3.8</b> Supervised Learning: Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="3.9" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-bias-variance-trade-off-2"><i class="fa fa-check"></i><b>3.9</b> Supervised Learning: Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="3.10" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-5"><i class="fa fa-check"></i><b>3.10</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.11" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#simple-linear-regression-slr"><i class="fa fa-check"></i><b>3.11</b> Simple Linear Regression (SLR)</a></li>
<li class="chapter" data-level="3.12" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-6"><i class="fa fa-check"></i><b>3.12</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.13" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-estimating-parameters"><i class="fa fa-check"></i><b>3.13</b> SLR: Estimating Parameters</a></li>
<li class="chapter" data-level="3.14" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-estimating-parameters-1"><i class="fa fa-check"></i><b>3.14</b> SLR: Estimating Parameters</a></li>
<li class="chapter" data-level="3.15" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-estimating-parameters-2"><i class="fa fa-check"></i><b>3.15</b> SLR: Estimating Parameters</a></li>
<li class="chapter" data-level="3.16" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#ames-housing-dataset"><i class="fa fa-check"></i><b>3.16</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="3.17" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#ames-housing-dataset-1"><i class="fa fa-check"></i><b>3.17</b> Ames Housing dataset</a></li>
<li class="chapter" data-level="3.18" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-estimating-parameters-3"><i class="fa fa-check"></i><b>3.18</b> SLR: Estimating Parameters</a></li>
<li class="chapter" data-level="3.19" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-model"><i class="fa fa-check"></i><b>3.19</b> SLR: Model</a></li>
<li class="chapter" data-level="3.20" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-model-1"><i class="fa fa-check"></i><b>3.20</b> SLR: Model</a></li>
<li class="chapter" data-level="3.21" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-prediction"><i class="fa fa-check"></i><b>3.21</b> SLR: Prediction</a></li>
<li class="chapter" data-level="3.22" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-interpreting-parameters"><i class="fa fa-check"></i><b>3.22</b> SLR: Interpreting Parameters</a></li>
<li class="chapter" data-level="3.23" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-assessing-accuracy-of-model"><i class="fa fa-check"></i><b>3.23</b> SLR: Assessing Accuracy of Model</a></li>
<li class="chapter" data-level="3.24" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-assessing-accuracy-of-model-1"><i class="fa fa-check"></i><b>3.24</b> SLR: Assessing Accuracy of Model</a></li>
<li class="chapter" data-level="3.25" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#your-turn"><i class="fa fa-check"></i><b>3.25</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="3.26" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-7"><i class="fa fa-check"></i><b>3.26</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.27" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-8"><i class="fa fa-check"></i><b>3.27</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.28" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-9"><i class="fa fa-check"></i><b>3.28</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.29" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#regression-conditional-averaging"><i class="fa fa-check"></i><b>3.29</b> Regression: Conditional Averaging</a></li>
<li class="chapter" data-level="3.30" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#regression-conditional-averaging-1"><i class="fa fa-check"></i><b>3.30</b> Regression: Conditional Averaging</a></li>
<li class="chapter" data-level="3.31" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#k-nearest-neighbors-regression"><i class="fa fa-check"></i><b>3.31</b> K-Nearest Neighbors Regression</a></li>
<li class="chapter" data-level="3.32" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#k-nearest-neighbors-regression-fit"><i class="fa fa-check"></i><b>3.32</b> K-Nearest Neighbors Regression: Fit</a></li>
<li class="chapter" data-level="3.33" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#k-nearest-neighbors-regression-prediction"><i class="fa fa-check"></i><b>3.33</b> K-Nearest Neighbors Regression: Prediction</a></li>
<li class="chapter" data-level="3.34" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#regression-methods-comparison"><i class="fa fa-check"></i><b>3.34</b> Regression Methods: Comparison</a></li>
<li class="chapter" data-level="3.35" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#your-turn-1"><i class="fa fa-check"></i><b>3.35</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="3.36" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-10"><i class="fa fa-check"></i><b>3.36</b> <span style="color:blue">Question!!!</span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression (MLR)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-estimating-parameters"><i class="fa fa-check"></i><b>4.1</b> MLR: Estimating Parameters</a></li>
<li class="chapter" data-level="4.2" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-estimating-parameters-1"><i class="fa fa-check"></i><b>4.2</b> MLR: Estimating Parameters</a></li>
<li class="chapter" data-level="4.3" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-estimating-parameters-2"><i class="fa fa-check"></i><b>4.3</b> MLR: Estimating Parameters</a></li>
<li class="chapter" data-level="4.4" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-interpreting-parameters"><i class="fa fa-check"></i><b>4.4</b> MLR: Interpreting Parameters</a></li>
<li class="chapter" data-level="4.5" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-prediction"><i class="fa fa-check"></i><b>4.5</b> MLR: Prediction</a></li>
<li class="chapter" data-level="4.6" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-assessing-accuracy-of-model"><i class="fa fa-check"></i><b>4.6</b> MLR: Assessing Accuracy of Model</a></li>
<li class="chapter" data-level="4.7" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#your-turn-2"><i class="fa fa-check"></i><b>4.7</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="4.8" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-assessing-accuracy-of-model-1"><i class="fa fa-check"></i><b>4.8</b> MLR: Assessing Accuracy of Model</a></li>
<li class="chapter" data-level="4.9" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#question-11"><i class="fa fa-check"></i><b>4.9</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="4.10" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#k-nearest-neighbors-regression-multiple-predictors"><i class="fa fa-check"></i><b>4.10</b> K-Nearest Neighbors Regression (multiple predictors)</a></li>
<li class="chapter" data-level="4.11" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#k-nearest-neighbors-regression-multiple-predictors-1"><i class="fa fa-check"></i><b>4.11</b> K-Nearest Neighbors Regression (multiple predictors)</a></li>
<li class="chapter" data-level="4.12" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#linear-regression-vs-k-nearest-neighbors"><i class="fa fa-check"></i><b>4.12</b> Linear Regression vs K-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.13" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#classification-problems"><i class="fa fa-check"></i><b>4.13</b> Classification Problems</a></li>
<li class="chapter" data-level="4.14" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#classification-problems-example"><i class="fa fa-check"></i><b>4.14</b> Classification Problems: Example</a></li>
<li class="chapter" data-level="4.15" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#classification-problems-example-1"><i class="fa fa-check"></i><b>4.15</b> Classification Problems: Example</a></li>
<li class="chapter" data-level="4.16" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.16</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.17" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#why-not-linear-regression-1"><i class="fa fa-check"></i><b>4.17</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.18" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression"><i class="fa fa-check"></i><b>4.18</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.19" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.19</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.20" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#your-turn-3"><i class="fa fa-check"></i><b>4.20</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="4.21" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-example"><i class="fa fa-check"></i><b>4.21</b> Logistic Regression: Example</a></li>
<li class="chapter" data-level="4.22" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-estimating-parameters"><i class="fa fa-check"></i><b>4.22</b> Logistic Regression: Estimating Parameters</a></li>
<li class="chapter" data-level="4.23" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-individual-predictions"><i class="fa fa-check"></i><b>4.23</b> Logistic Regression: Individual Predictions</a></li>
<li class="chapter" data-level="4.24" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-test-set-predictions"><i class="fa fa-check"></i><b>4.24</b> Logistic Regression: Test Set Predictions</a></li>
<li class="chapter" data-level="4.25" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-test-set-predictions-1"><i class="fa fa-check"></i><b>4.25</b> Logistic Regression: Test Set Predictions</a></li>
<li class="chapter" data-level="4.26" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-performance"><i class="fa fa-check"></i><b>4.26</b> Logistic Regression: Performance</a></li>
<li class="chapter" data-level="4.27" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#confusion-matrix-terms"><i class="fa fa-check"></i><b>4.27</b> Confusion Matrix Terms</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html"><i class="fa fa-check"></i><b>5</b> K-Nearest Neighbors Classifier</a>
<ul>
<li class="chapter" data-level="5.1" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-4"><i class="fa fa-check"></i><b>5.1</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="5.2" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-split-data"><i class="fa fa-check"></i><b>5.2</b> K-Nearest Neighbors Classifier: Split Data</a></li>
<li class="chapter" data-level="5.3" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-build-model"><i class="fa fa-check"></i><b>5.3</b> K-Nearest Neighbors Classifier: Build Model</a></li>
<li class="chapter" data-level="5.4" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-predictions"><i class="fa fa-check"></i><b>5.4</b> K-Nearest Neighbors Classifier: Predictions</a></li>
<li class="chapter" data-level="5.5" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-performance"><i class="fa fa-check"></i><b>5.5</b> K-Nearest Neighbors Classifier: Performance</a></li>
<li class="chapter" data-level="5.6" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#roc-curve-and-auc"><i class="fa fa-check"></i><b>5.6</b> ROC Curve and AUC</a></li>
<li class="chapter" data-level="5.7" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#roc-curve-and-auc-1"><i class="fa fa-check"></i><b>5.7</b> ROC Curve and AUC</a></li>
<li class="chapter" data-level="5.8" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#data-splitting"><i class="fa fa-check"></i><b>5.8</b> Data Splitting</a></li>
<li class="chapter" data-level="5.9" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#resampling-methods"><i class="fa fa-check"></i><b>5.9</b> Resampling Methods</a></li>
<li class="chapter" data-level="5.10" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#cross-validation-cv"><i class="fa fa-check"></i><b>5.10</b> Cross-Validation (CV)</a></li>
<li class="chapter" data-level="5.11" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>5.11</b> Leave-One-Out Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="5.12" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#leave-one-out-cross-validation-loocv-1"><i class="fa fa-check"></i><b>5.12</b> Leave-One-Out Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="5.13" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.13</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.14" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation"><i class="fa fa-check"></i><b>5.14</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.15" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-1"><i class="fa fa-check"></i><b>5.15</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.16" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-2"><i class="fa fa-check"></i><b>5.16</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.17" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-3"><i class="fa fa-check"></i><b>5.17</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.18" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-4"><i class="fa fa-check"></i><b>5.18</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.19" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-5"><i class="fa fa-check"></i><b>5.19</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.20" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-results"><i class="fa fa-check"></i><b>5.20</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Results</a></li>
<li class="chapter" data-level="5.21" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#final-model-and-prediction-error-estimate"><i class="fa fa-check"></i><b>5.21</b> Final Model and Prediction Error Estimate</a></li>
<li class="chapter" data-level="5.22" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#variable-importance"><i class="fa fa-check"></i><b>5.22</b> Variable Importance</a></li>
<li class="chapter" data-level="5.23" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#bias-variance-trade-off-for-loocv-and-k-fold-cv"><i class="fa fa-check"></i><b>5.23</b> Bias-Variance Trade-off for LOOCV and <span class="math inline">\(k\)</span>-fold CV</a></li>
<li class="chapter" data-level="5.24" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-5"><i class="fa fa-check"></i><b>5.24</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="5.25" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-split-data"><i class="fa fa-check"></i><b>5.25</b> <span style="color:blue">Your Turn!!!</span>: Split Data</a></li>
<li class="chapter" data-level="5.26" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-perform-cv"><i class="fa fa-check"></i><b>5.26</b> <span style="color:blue">Your Turn!!!</span>: Perform CV</a></li>
<li class="chapter" data-level="5.27" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-observe-cv-results"><i class="fa fa-check"></i><b>5.27</b> <span style="color:blue">Your Turn!!!</span>: Observe CV Results</a></li>
<li class="chapter" data-level="5.28" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-final-model"><i class="fa fa-check"></i><b>5.28</b> <span style="color:blue">Your Turn!!!</span>: Final Model</a></li>
<li class="chapter" data-level="5.29" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#mid-term-check"><i class="fa fa-check"></i><b>5.29</b> Mid-Term Check</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-of-cv.html"><a href="review-of-cv.html"><i class="fa fa-check"></i><b>6</b> Review of CV</a>
<ul>
<li class="chapter" data-level="6.1" data-path="review-of-cv.html"><a href="review-of-cv.html#data-leakage-a-serious-common-problem"><i class="fa fa-check"></i><b>6.1</b> Data Leakage (A Serious, Common Problem)</a></li>
<li class="chapter" data-level="6.2" data-path="review-of-cv.html"><a href="review-of-cv.html#data-preprocessing-and-feature-enginnering"><i class="fa fa-check"></i><b>6.2</b> Data Preprocessing and Feature Enginnering</a></li>
<li class="chapter" data-level="6.3" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-2"><i class="fa fa-check"></i><b>6.3</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.4" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-3"><i class="fa fa-check"></i><b>6.4</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.5" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-4"><i class="fa fa-check"></i><b>6.5</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.6" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-5"><i class="fa fa-check"></i><b>6.6</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.7" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-6"><i class="fa fa-check"></i><b>6.7</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.8" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-7"><i class="fa fa-check"></i><b>6.8</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.9" data-path="review-of-cv.html"><a href="review-of-cv.html#zero-variance-zv-andor-near-zero-variance-nzv-variables"><i class="fa fa-check"></i><b>6.9</b> Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables</a></li>
<li class="chapter" data-level="6.10" data-path="review-of-cv.html"><a href="review-of-cv.html#zero-variance-zv-andor-near-zero-variance-nzv-variables-1"><i class="fa fa-check"></i><b>6.10</b> Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables</a></li>
<li class="chapter" data-level="6.11" data-path="review-of-cv.html"><a href="review-of-cv.html#zero-variance-zv-andor-near-zero-variance-nzv-variables-2"><i class="fa fa-check"></i><b>6.11</b> Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables</a></li>
<li class="chapter" data-level="6.12" data-path="review-of-cv.html"><a href="review-of-cv.html#imputing-missing-entries"><i class="fa fa-check"></i><b>6.12</b> Imputing Missing Entries</a></li>
<li class="chapter" data-level="6.13" data-path="review-of-cv.html"><a href="review-of-cv.html#imputing-missing-entries-1"><i class="fa fa-check"></i><b>6.13</b> Imputing Missing Entries</a></li>
<li class="chapter" data-level="6.14" data-path="review-of-cv.html"><a href="review-of-cv.html#imputing-missing-entries-2"><i class="fa fa-check"></i><b>6.14</b> Imputing Missing Entries</a></li>
<li class="chapter" data-level="6.15" data-path="review-of-cv.html"><a href="review-of-cv.html#label-encoding-ordinal-categorical-variables"><i class="fa fa-check"></i><b>6.15</b> Label Encoding Ordinal Categorical Variables</a></li>
<li class="chapter" data-level="6.16" data-path="review-of-cv.html"><a href="review-of-cv.html#label-encoding-ordinal-categorical-variables-1"><i class="fa fa-check"></i><b>6.16</b> Label Encoding Ordinal Categorical Variables</a></li>
<li class="chapter" data-level="6.17" data-path="review-of-cv.html"><a href="review-of-cv.html#label-encoding-ordinal-categorical-variables-2"><i class="fa fa-check"></i><b>6.17</b> Label Encoding Ordinal Categorical Variables</a></li>
<li class="chapter" data-level="6.18" data-path="review-of-cv.html"><a href="review-of-cv.html#standardizing-centering-and-scaling-numeric-predictors"><i class="fa fa-check"></i><b>6.18</b> Standardizing (centering and scaling) Numeric Predictors</a></li>
<li class="chapter" data-level="6.19" data-path="review-of-cv.html"><a href="review-of-cv.html#lumping-predictors"><i class="fa fa-check"></i><b>6.19</b> Lumping Predictors</a></li>
<li class="chapter" data-level="6.20" data-path="review-of-cv.html"><a href="review-of-cv.html#one-hotdummy-encoding-categorical-predictors"><i class="fa fa-check"></i><b>6.20</b> One-hot/dummy Encoding Categorical Predictors</a></li>
<li class="chapter" data-level="6.21" data-path="review-of-cv.html"><a href="review-of-cv.html#one-hotdummy-encoding-categorical-predictors-1"><i class="fa fa-check"></i><b>6.21</b> One-hot/dummy Encoding Categorical Predictors</a></li>
<li class="chapter" data-level="6.22" data-path="review-of-cv.html"><a href="review-of-cv.html#preprocessing-steps"><i class="fa fa-check"></i><b>6.22</b> Preprocessing Steps</a></li>
<li class="chapter" data-level="6.23" data-path="review-of-cv.html"><a href="review-of-cv.html#preprocessing-with-recipes-package"><i class="fa fa-check"></i><b>6.23</b> Preprocessing With <code>recipes</code> Package</a></li>
<li class="chapter" data-level="6.24" data-path="review-of-cv.html"><a href="review-of-cv.html#preprocessing-with-recipes-package-1"><i class="fa fa-check"></i><b>6.24</b> Preprocessing With <code>recipes</code> Package</a></li>
<li class="chapter" data-level="6.25" data-path="review-of-cv.html"><a href="review-of-cv.html#training-model"><i class="fa fa-check"></i><b>6.25</b> Training Model</a></li>
<li class="chapter" data-level="6.26" data-path="review-of-cv.html"><a href="review-of-cv.html#training-model-1"><i class="fa fa-check"></i><b>6.26</b> Training Model</a></li>
<li class="chapter" data-level="6.27" data-path="review-of-cv.html"><a href="review-of-cv.html#training-model-2"><i class="fa fa-check"></i><b>6.27</b> Training Model</a></li>
<li class="chapter" data-level="6.28" data-path="review-of-cv.html"><a href="review-of-cv.html#final-model-and-test-set-error"><i class="fa fa-check"></i><b>6.28</b> Final Model and Test Set Error</a></li>
<li class="chapter" data-level="6.29" data-path="review-of-cv.html"><a href="review-of-cv.html#variable-importance-1"><i class="fa fa-check"></i><b>6.29</b> Variable Importance</a></li>
<li class="chapter" data-level="6.30" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-6"><i class="fa fa-check"></i><b>6.30</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="6.31" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1"><i class="fa fa-check"></i><b>6.31</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.32" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1-1"><i class="fa fa-check"></i><b>6.32</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.33" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1-2"><i class="fa fa-check"></i><b>6.33</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.34" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1-3"><i class="fa fa-check"></i><b>6.34</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.35" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1-4"><i class="fa fa-check"></i><b>6.35</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.36" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-2"><i class="fa fa-check"></i><b>6.36</b> <span style="color:blue">Your Turn!!!</span> Step 2</a></li>
<li class="chapter" data-level="6.37" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-3"><i class="fa fa-check"></i><b>6.37</b> <span style="color:blue">Your Turn!!!</span> Step 3</a></li>
<li class="chapter" data-level="6.38" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-4"><i class="fa fa-check"></i><b>6.38</b> <span style="color:blue">Your Turn!!!</span> Step 4</a></li>
<li class="chapter" data-level="6.39" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-4-1"><i class="fa fa-check"></i><b>6.39</b> <span style="color:blue">Your Turn!!!</span> Step 4</a></li>
<li class="chapter" data-level="6.40" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-4-2"><i class="fa fa-check"></i><b>6.40</b> <span style="color:blue">Your Turn!!!</span> Step 4</a></li>
<li class="chapter" data-level="6.41" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-5"><i class="fa fa-check"></i><b>6.41</b> <span style="color:blue">Your Turn!!!</span> Step 5</a></li>
<li class="chapter" data-level="6.42" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-5-1"><i class="fa fa-check"></i><b>6.42</b> <span style="color:blue">Your Turn!!!</span> Step 5</a></li>
<li class="chapter" data-level="6.43" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-5-2"><i class="fa fa-check"></i><b>6.43</b> <span style="color:blue">Your Turn!!!</span> Step 5</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html"><i class="fa fa-check"></i><b>7</b> Linear Model Selection and Regularization</a>
<ul>
<li class="chapter" data-level="7.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#alternatives-to-least-squares"><i class="fa fa-check"></i><b>7.1</b> Alternatives to Least Squares</a></li>
<li class="chapter" data-level="7.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#shrinkageregularization-methods"><i class="fa fa-check"></i><b>7.2</b> Shrinkage/Regularization Methods</a></li>
<li class="chapter" data-level="7.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso"><i class="fa fa-check"></i><b>7.3</b> The Lasso</a></li>
<li class="chapter" data-level="7.4" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-1"><i class="fa fa-check"></i><b>7.4</b> The Lasso</a></li>
<li class="chapter" data-level="7.5" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-scaling-of-predictors"><i class="fa fa-check"></i><b>7.5</b> The Lasso: Scaling of Predictors</a></li>
<li class="chapter" data-level="7.6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-implementation"><i class="fa fa-check"></i><b>7.6</b> The Lasso: Implementation</a></li>
<li class="chapter" data-level="7.7" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-implementation-1"><i class="fa fa-check"></i><b>7.7</b> The Lasso: Implementation</a></li>
<li class="chapter" data-level="7.8" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-implementation-2"><i class="fa fa-check"></i><b>7.8</b> The Lasso: Implementation</a></li>
<li class="chapter" data-level="7.9" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-implementation-3"><i class="fa fa-check"></i><b>7.9</b> The Lasso: Implementation</a></li>
<li class="chapter" data-level="7.10" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-implementation-4"><i class="fa fa-check"></i><b>7.10</b> The Lasso: Implementation</a></li>
<li class="chapter" data-level="7.11" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-implementation-5"><i class="fa fa-check"></i><b>7.11</b> The Lasso: Implementation</a></li>
<li class="chapter" data-level="7.12" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#the-lasso-implementation-6"><i class="fa fa-check"></i><b>7.12</b> The Lasso: Implementation</a></li>
<li class="chapter" data-level="7.13" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-7"><i class="fa fa-check"></i><b>7.13</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.14" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-8"><i class="fa fa-check"></i><b>7.14</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.15" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-9"><i class="fa fa-check"></i><b>7.15</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.16" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-10"><i class="fa fa-check"></i><b>7.16</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.17" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-11"><i class="fa fa-check"></i><b>7.17</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.18" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-12"><i class="fa fa-check"></i><b>7.18</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.19" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-13"><i class="fa fa-check"></i><b>7.19</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.20" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-14"><i class="fa fa-check"></i><b>7.20</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.21" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-15"><i class="fa fa-check"></i><b>7.21</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.22" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-16"><i class="fa fa-check"></i><b>7.22</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.23" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-17"><i class="fa fa-check"></i><b>7.23</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.24" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-18"><i class="fa fa-check"></i><b>7.24</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.25" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#multivariate-adaptive-regression-splines-mars"><i class="fa fa-check"></i><b>7.25</b> Multivariate Adaptive Regression Splines (MARS)</a></li>
<li class="chapter" data-level="7.26" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-geometry"><i class="fa fa-check"></i><b>7.26</b> MARS: Geometry</a></li>
<li class="chapter" data-level="7.27" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-implementation"><i class="fa fa-check"></i><b>7.27</b> MARS: Implementation</a></li>
<li class="chapter" data-level="7.28" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-implementation-1"><i class="fa fa-check"></i><b>7.28</b> MARS: Implementation</a></li>
<li class="chapter" data-level="7.29" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-implementation-2"><i class="fa fa-check"></i><b>7.29</b> MARS: Implementation</a></li>
<li class="chapter" data-level="7.30" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-implementation-3"><i class="fa fa-check"></i><b>7.30</b> MARS: Implementation</a></li>
<li class="chapter" data-level="7.31" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-implementation-4"><i class="fa fa-check"></i><b>7.31</b> MARS: Implementation</a></li>
<li class="chapter" data-level="7.32" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars"><i class="fa fa-check"></i><b>7.32</b> MARS</a></li>
<li class="chapter" data-level="7.33" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-without-feature-engineering-implementation"><i class="fa fa-check"></i><b>7.33</b> MARS Without Feature Engineering: Implementation</a></li>
<li class="chapter" data-level="7.34" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-without-feature-engineering-implementation-1"><i class="fa fa-check"></i><b>7.34</b> MARS Without Feature Engineering: Implementation</a></li>
<li class="chapter" data-level="7.35" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-without-feature-engineering-implementation-2"><i class="fa fa-check"></i><b>7.35</b> MARS Without Feature Engineering: Implementation</a></li>
<li class="chapter" data-level="7.36" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-without-feature-engineering-implementation-3"><i class="fa fa-check"></i><b>7.36</b> MARS Without Feature Engineering: Implementation</a></li>
<li class="chapter" data-level="7.37" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-without-feature-engineering-implementation-4"><i class="fa fa-check"></i><b>7.37</b> MARS Without Feature Engineering: Implementation</a></li>
<li class="chapter" data-level="7.38" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#mars-without-feature-engineering-implementation-5"><i class="fa fa-check"></i><b>7.38</b> MARS Without Feature Engineering: Implementation</a></li>
<li class="chapter" data-level="7.39" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-19"><i class="fa fa-check"></i><b>7.39</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.40" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-20"><i class="fa fa-check"></i><b>7.40</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.41" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-21"><i class="fa fa-check"></i><b>7.41</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.42" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-22"><i class="fa fa-check"></i><b>7.42</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.43" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-23"><i class="fa fa-check"></i><b>7.43</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.44" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-24"><i class="fa fa-check"></i><b>7.44</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.45" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-25"><i class="fa fa-check"></i><b>7.45</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.46" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-26"><i class="fa fa-check"></i><b>7.46</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.47" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-27"><i class="fa fa-check"></i><b>7.47</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.48" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-28"><i class="fa fa-check"></i><b>7.48</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="7.49" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#your-turn-29"><i class="fa fa-check"></i><b>7.49</b> <span style="color:blue">Your Turn!!!</span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>8</b> Tree-Based Methods</a>
<ul>
<li class="chapter" data-level="8.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#terminology-for-trees"><i class="fa fa-check"></i><b>8.1</b> Terminology for Trees</a></li>
<li class="chapter" data-level="8.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#terminology-for-trees-1"><i class="fa fa-check"></i><b>8.2</b> Terminology for Trees</a></li>
<li class="chapter" data-level="8.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#building-a-tree-and-prediction"><i class="fa fa-check"></i><b>8.3</b> Building a Tree and Prediction</a></li>
<li class="chapter" data-level="8.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#building-a-tree-and-prediction-1"><i class="fa fa-check"></i><b>8.4</b> Building a Tree and Prediction</a></li>
<li class="chapter" data-level="8.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#building-a-tree-and-prediction-2"><i class="fa fa-check"></i><b>8.5</b> Building a Tree and Prediction</a></li>
<li class="chapter" data-level="8.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#building-a-tree-and-prediction-3"><i class="fa fa-check"></i><b>8.6</b> Building a Tree and Prediction</a></li>
<li class="chapter" data-level="8.7" data-path="tree-based-methods.html"><a href="tree-based-methods.html#building-a-tree-and-prediction-4"><i class="fa fa-check"></i><b>8.7</b> Building a Tree and Prediction</a></li>
<li class="chapter" data-level="8.8" data-path="tree-based-methods.html"><a href="tree-based-methods.html#building-a-tree"><i class="fa fa-check"></i><b>8.8</b> Building a Tree</a></li>
<li class="chapter" data-level="8.9" data-path="tree-based-methods.html"><a href="tree-based-methods.html#building-a-tree-1"><i class="fa fa-check"></i><b>8.9</b> Building a Tree</a></li>
<li class="chapter" data-level="8.10" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-pruning"><i class="fa fa-check"></i><b>8.10</b> Tree Pruning</a></li>
<li class="chapter" data-level="8.11" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-pruning-1"><i class="fa fa-check"></i><b>8.11</b> Tree Pruning</a></li>
<li class="chapter" data-level="8.12" data-path="tree-based-methods.html"><a href="tree-based-methods.html#building-an-optimal-tree"><i class="fa fa-check"></i><b>8.12</b> Building an Optimal Tree</a></li>
<li class="chapter" data-level="8.13" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation"><i class="fa fa-check"></i><b>8.13</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.14" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-1"><i class="fa fa-check"></i><b>8.14</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.15" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-2"><i class="fa fa-check"></i><b>8.15</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.16" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-3"><i class="fa fa-check"></i><b>8.16</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.17" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-4"><i class="fa fa-check"></i><b>8.17</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.18" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-5"><i class="fa fa-check"></i><b>8.18</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.19" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-6"><i class="fa fa-check"></i><b>8.19</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.20" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-7"><i class="fa fa-check"></i><b>8.20</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.21" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-8"><i class="fa fa-check"></i><b>8.21</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.22" data-path="tree-based-methods.html"><a href="tree-based-methods.html#trees"><i class="fa fa-check"></i><b>8.22</b> Trees</a></li>
<li class="chapter" data-level="8.23" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-9"><i class="fa fa-check"></i><b>8.23</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.24" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-10"><i class="fa fa-check"></i><b>8.24</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.25" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-11"><i class="fa fa-check"></i><b>8.25</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.26" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-tree-implementation-12"><i class="fa fa-check"></i><b>8.26</b> Regression Tree: Implementation</a></li>
<li class="chapter" data-level="8.27" data-path="tree-based-methods.html"><a href="tree-based-methods.html#classification-trees"><i class="fa fa-check"></i><b>8.27</b> Classification Trees</a></li>
<li class="chapter" data-level="8.28" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-30"><i class="fa fa-check"></i><b>8.28</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.29" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-31"><i class="fa fa-check"></i><b>8.29</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.30" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-32"><i class="fa fa-check"></i><b>8.30</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.31" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-33"><i class="fa fa-check"></i><b>8.31</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.32" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-34"><i class="fa fa-check"></i><b>8.32</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.33" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-35"><i class="fa fa-check"></i><b>8.33</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.34" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-36"><i class="fa fa-check"></i><b>8.34</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.35" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-37"><i class="fa fa-check"></i><b>8.35</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.36" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-38"><i class="fa fa-check"></i><b>8.36</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.37" data-path="tree-based-methods.html"><a href="tree-based-methods.html#ensemble-methods"><i class="fa fa-check"></i><b>8.37</b> Ensemble Methods</a></li>
<li class="chapter" data-level="8.38" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging"><i class="fa fa-check"></i><b>8.38</b> Bagging</a></li>
<li class="chapter" data-level="8.39" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-1"><i class="fa fa-check"></i><b>8.39</b> Bagging</a></li>
<li class="chapter" data-level="8.40" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-2"><i class="fa fa-check"></i><b>8.40</b> Bagging</a></li>
<li class="chapter" data-level="8.41" data-path="tree-based-methods.html"><a href="tree-based-methods.html#out-of-bag-error-estimation"><i class="fa fa-check"></i><b>8.41</b> Out-of-Bag Error Estimation</a></li>
<li class="chapter" data-level="8.42" data-path="tree-based-methods.html"><a href="tree-based-methods.html#variable-importance-measures"><i class="fa fa-check"></i><b>8.42</b> Variable Importance Measures</a></li>
<li class="chapter" data-level="8.43" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-implementation"><i class="fa fa-check"></i><b>8.43</b> Bagging: Implementation</a></li>
<li class="chapter" data-level="8.44" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-implementation-1"><i class="fa fa-check"></i><b>8.44</b> Bagging: Implementation</a></li>
<li class="chapter" data-level="8.45" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-implementation-2"><i class="fa fa-check"></i><b>8.45</b> Bagging: Implementation</a></li>
<li class="chapter" data-level="8.46" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-disadvantages"><i class="fa fa-check"></i><b>8.46</b> Bagging: Disadvantages</a></li>
<li class="chapter" data-level="8.47" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-disadvantages-1"><i class="fa fa-check"></i><b>8.47</b> Bagging: Disadvantages</a></li>
<li class="chapter" data-level="8.48" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests"><i class="fa fa-check"></i><b>8.48</b> Random Forests</a></li>
<li class="chapter" data-level="8.49" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests-implementation"><i class="fa fa-check"></i><b>8.49</b> Random Forests: Implementation</a></li>
<li class="chapter" data-level="8.50" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests-implementation-1"><i class="fa fa-check"></i><b>8.50</b> Random Forests: Implementation</a></li>
<li class="chapter" data-level="8.51" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forests-implementation-2"><i class="fa fa-check"></i><b>8.51</b> Random Forests: Implementation</a></li>
<li class="chapter" data-level="8.52" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-39"><i class="fa fa-check"></i><b>8.52</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.53" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-40"><i class="fa fa-check"></i><b>8.53</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.54" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-41"><i class="fa fa-check"></i><b>8.54</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.55" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-42"><i class="fa fa-check"></i><b>8.55</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.56" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-43"><i class="fa fa-check"></i><b>8.56</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.57" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-44"><i class="fa fa-check"></i><b>8.57</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.58" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-45"><i class="fa fa-check"></i><b>8.58</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.59" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-46"><i class="fa fa-check"></i><b>8.59</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.60" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-47"><i class="fa fa-check"></i><b>8.60</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="8.61" data-path="tree-based-methods.html"><a href="tree-based-methods.html#your-turn-48"><i class="fa fa-check"></i><b>8.61</b> <span style="color:blue">Your Turn!!!</span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html"><i class="fa fa-check"></i><b>9</b> Support Vector Machines (SVM)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#hyperplane"><i class="fa fa-check"></i><b>9.1</b> Hyperplane</a></li>
<li class="chapter" data-level="9.2" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#hyperplane-1"><i class="fa fa-check"></i><b>9.2</b> Hyperplane</a></li>
<li class="chapter" data-level="9.3" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#hyperplane-2"><i class="fa fa-check"></i><b>9.3</b> Hyperplane</a></li>
<li class="chapter" data-level="9.4" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#separating-hyperplane"><i class="fa fa-check"></i><b>9.4</b> Separating Hyperplane</a></li>
<li class="chapter" data-level="9.5" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#separating-hyperplane-1"><i class="fa fa-check"></i><b>9.5</b> Separating Hyperplane</a></li>
<li class="chapter" data-level="9.6" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#optimal-separating-hyperplane"><i class="fa fa-check"></i><b>9.6</b> Optimal Separating Hyperplane</a></li>
<li class="chapter" data-level="9.7" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#optimal-separating-hyperplane-1"><i class="fa fa-check"></i><b>9.7</b> Optimal Separating Hyperplane</a></li>
<li class="chapter" data-level="9.8" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#optimal-separating-hyperplane-2"><i class="fa fa-check"></i><b>9.8</b> Optimal Separating Hyperplane</a></li>
<li class="chapter" data-level="9.9" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#optimal-separating-hyperplane-issue-1"><i class="fa fa-check"></i><b>9.9</b> Optimal Separating Hyperplane: Issue 1</a></li>
<li class="chapter" data-level="9.10" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#optimal-separating-hyperplane-issue-2"><i class="fa fa-check"></i><b>9.10</b> Optimal Separating Hyperplane: Issue 2</a></li>
<li class="chapter" data-level="9.11" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#support-vector-classifier"><i class="fa fa-check"></i><b>9.11</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="9.12" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#support-vector-classifier-1"><i class="fa fa-check"></i><b>9.12</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="9.13" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#support-vector-classifier-2"><i class="fa fa-check"></i><b>9.13</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="9.14" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#support-vector-classifier-3"><i class="fa fa-check"></i><b>9.14</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="9.15" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#support-vector-classifier-4"><i class="fa fa-check"></i><b>9.15</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="9.16" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#support-vector-classifier-5"><i class="fa fa-check"></i><b>9.16</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="9.17" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#non-linear-boundaries"><i class="fa fa-check"></i><b>9.17</b> Non-linear Boundaries</a></li>
<li class="chapter" data-level="9.18" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#feature-expansion"><i class="fa fa-check"></i><b>9.18</b> Feature Expansion</a></li>
<li class="chapter" data-level="9.19" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#feature-expansion-1"><i class="fa fa-check"></i><b>9.19</b> Feature Expansion</a></li>
<li class="chapter" data-level="9.20" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#feature-expansion-2"><i class="fa fa-check"></i><b>9.20</b> Feature Expansion</a></li>
<li class="chapter" data-level="9.21" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#non-linear-boundaries-circle-dataset"><i class="fa fa-check"></i><b>9.21</b> Non-linear Boundaries: Circle dataset</a></li>
<li class="chapter" data-level="9.22" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#non-linear-boundaries-circle-dataset-1"><i class="fa fa-check"></i><b>9.22</b> Non-linear Boundaries: Circle dataset</a></li>
<li class="chapter" data-level="9.23" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#non-linear-boundaries-circle-dataset-2"><i class="fa fa-check"></i><b>9.23</b> Non-linear Boundaries: Circle dataset</a></li>
<li class="chapter" data-level="9.24" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#non-linear-boundaries-spirals-dataset"><i class="fa fa-check"></i><b>9.24</b> Non-linear Boundaries: Spirals dataset</a></li>
<li class="chapter" data-level="9.25" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#non-linear-boundaries-spirals-dataset-1"><i class="fa fa-check"></i><b>9.25</b> Non-linear Boundaries: Spirals dataset</a></li>
<li class="chapter" data-level="9.26" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#non-linear-boundaries-spirals-dataset-2"><i class="fa fa-check"></i><b>9.26</b> Non-linear Boundaries: Spirals dataset</a></li>
<li class="chapter" data-level="9.27" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#non-linear-boundaries-spirals-dataset-3"><i class="fa fa-check"></i><b>9.27</b> Non-linear Boundaries: Spirals dataset</a></li>
<li class="chapter" data-level="9.28" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#summary"><i class="fa fa-check"></i><b>9.28</b> Summary</a></li>
<li class="chapter" data-level="9.29" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-49"><i class="fa fa-check"></i><b>9.29</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.30" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-50"><i class="fa fa-check"></i><b>9.30</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.31" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-51"><i class="fa fa-check"></i><b>9.31</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.32" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-52"><i class="fa fa-check"></i><b>9.32</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.33" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-53"><i class="fa fa-check"></i><b>9.33</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.34" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-54"><i class="fa fa-check"></i><b>9.34</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.35" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-55"><i class="fa fa-check"></i><b>9.35</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.36" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-56"><i class="fa fa-check"></i><b>9.36</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.37" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-57"><i class="fa fa-check"></i><b>9.37</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.38" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-58"><i class="fa fa-check"></i><b>9.38</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.39" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-59"><i class="fa fa-check"></i><b>9.39</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.40" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#summary-of-supervised-learning-methods"><i class="fa fa-check"></i><b>9.40</b> Summary of Supervised Learning Methods</a></li>
<li class="chapter" data-level="9.41" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks"><i class="fa fa-check"></i><b>9.41</b> Neural Networks</a></li>
<li class="chapter" data-level="9.42" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-1"><i class="fa fa-check"></i><b>9.42</b> Neural Networks</a></li>
<li class="chapter" data-level="9.43" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#biological-neural-networks"><i class="fa fa-check"></i><b>9.43</b> Biological Neural Networks</a></li>
<li class="chapter" data-level="9.44" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#artificial-neural-networks"><i class="fa fa-check"></i><b>9.44</b> Artificial Neural Networks</a></li>
<li class="chapter" data-level="9.45" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-2"><i class="fa fa-check"></i><b>9.45</b> Neural Networks</a></li>
<li class="chapter" data-level="9.46" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-3"><i class="fa fa-check"></i><b>9.46</b> Neural Networks</a></li>
<li class="chapter" data-level="9.47" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-4"><i class="fa fa-check"></i><b>9.47</b> Neural Networks</a></li>
<li class="chapter" data-level="9.48" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#your-turn-60"><i class="fa fa-check"></i><b>9.48</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="9.49" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-5"><i class="fa fa-check"></i><b>9.49</b> Neural Networks</a></li>
<li class="chapter" data-level="9.50" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-6"><i class="fa fa-check"></i><b>9.50</b> Neural Networks</a></li>
<li class="chapter" data-level="9.51" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-input-data"><i class="fa fa-check"></i><b>9.51</b> Neural Networks: Input Data</a></li>
<li class="chapter" data-level="9.52" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-network-architecture"><i class="fa fa-check"></i><b>9.52</b> Neural Networks: Network Architecture</a></li>
<li class="chapter" data-level="9.53" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-network-architecture-1"><i class="fa fa-check"></i><b>9.53</b> Neural Networks: Network Architecture</a></li>
<li class="chapter" data-level="9.54" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-network-architecture-2"><i class="fa fa-check"></i><b>9.54</b> Neural Networks: Network Architecture</a></li>
<li class="chapter" data-level="9.55" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-feedback-mechanism"><i class="fa fa-check"></i><b>9.55</b> Neural Networks: Feedback Mechanism</a></li>
<li class="chapter" data-level="9.56" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-feedback-mechanism-1"><i class="fa fa-check"></i><b>9.56</b> Neural Networks: Feedback Mechanism</a></li>
<li class="chapter" data-level="9.57" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-model-training"><i class="fa fa-check"></i><b>9.57</b> Neural Networks: Model Training</a></li>
<li class="chapter" data-level="9.58" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-model-training-1"><i class="fa fa-check"></i><b>9.58</b> Neural Networks: Model Training</a></li>
<li class="chapter" data-level="9.59" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html#neural-networks-further-topics"><i class="fa fa-check"></i><b>9.59</b> Neural Networks: Further Topics</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html"><i class="fa fa-check"></i><b>10</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#unsupervised-learning-2"><i class="fa fa-check"></i><b>10.1</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="10.2" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>10.2</b> Principal Components Analysis (PCA)</a></li>
<li class="chapter" data-level="10.3" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-example"><i class="fa fa-check"></i><b>10.3</b> PCA: Example</a></li>
<li class="chapter" data-level="10.4" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-example-1"><i class="fa fa-check"></i><b>10.4</b> PCA: Example</a></li>
<li class="chapter" data-level="10.5" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-data-requirements"><i class="fa fa-check"></i><b>10.5</b> PCA: Data Requirements</a></li>
<li class="chapter" data-level="10.6" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-toy-example"><i class="fa fa-check"></i><b>10.6</b> PCA: Toy Example</a></li>
<li class="chapter" data-level="10.7" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-toy-example-1"><i class="fa fa-check"></i><b>10.7</b> PCA: Toy Example</a></li>
<li class="chapter" data-level="10.8" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-toy-example-2"><i class="fa fa-check"></i><b>10.8</b> PCA: Toy Example</a></li>
<li class="chapter" data-level="10.9" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-first-pc"><i class="fa fa-check"></i><b>10.9</b> PCA: First PC</a></li>
<li class="chapter" data-level="10.10" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-first-pc-1"><i class="fa fa-check"></i><b>10.10</b> PCA: First PC</a></li>
<li class="chapter" data-level="10.11" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-second-pc"><i class="fa fa-check"></i><b>10.11</b> PCA: Second PC</a></li>
<li class="chapter" data-level="10.12" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#pca-how-many-pcs-to-use"><i class="fa fa-check"></i><b>10.12</b> PCA: How Many PCs to Use?</a></li>
<li class="chapter" data-level="10.13" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#your-turn-61"><i class="fa fa-check"></i><b>10.13</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="10.14" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#your-turn-62"><i class="fa fa-check"></i><b>10.14</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="10.15" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#your-turn-63"><i class="fa fa-check"></i><b>10.15</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="10.16" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#your-turn-64"><i class="fa fa-check"></i><b>10.16</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="10.17" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#your-turn-65"><i class="fa fa-check"></i><b>10.17</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="10.18" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#principal-components-regression-pcr"><i class="fa fa-check"></i><b>10.18</b> Principal Components Regression (PCR)</a></li>
<li class="chapter" data-level="10.19" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#principal-components-regression-pcr-1"><i class="fa fa-check"></i><b>10.19</b> Principal Components Regression (PCR)</a></li>
<li class="chapter" data-level="10.20" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#clustering"><i class="fa fa-check"></i><b>10.20</b> Clustering</a></li>
<li class="chapter" data-level="10.21" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#clustering-applications"><i class="fa fa-check"></i><b>10.21</b> Clustering: Applications</a></li>
<li class="chapter" data-level="10.22" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#k-means-clustering"><i class="fa fa-check"></i><b>10.22</b> K-Means Clustering</a></li>
<li class="chapter" data-level="10.23" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#k-means-clustering-1"><i class="fa fa-check"></i><b>10.23</b> K-Means Clustering</a></li>
<li class="chapter" data-level="10.24" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#k-means-clustering-2"><i class="fa fa-check"></i><b>10.24</b> K-Means Clustering</a></li>
<li class="chapter" data-level="10.25" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#k-means-clustering-formulation"><i class="fa fa-check"></i><b>10.25</b> K-Means Clustering Formulation</a></li>
<li class="chapter" data-level="10.26" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#k-means-clustering-algorithm"><i class="fa fa-check"></i><b>10.26</b> K-Means Clustering Algorithm</a></li>
<li class="chapter" data-level="10.27" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#k-means-clustering-algorithm-1"><i class="fa fa-check"></i><b>10.27</b> K-Means Clustering Algorithm</a></li>
<li class="chapter" data-level="10.28" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#k-means-clustering-algorithm-2"><i class="fa fa-check"></i><b>10.28</b> K-Means Clustering Algorithm</a></li>
<li class="chapter" data-level="10.29" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#hierarchical-clustering"><i class="fa fa-check"></i><b>10.29</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="10.30" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#hierarchical-clustering-1"><i class="fa fa-check"></i><b>10.30</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="10.31" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#hierarchical-clustering-2"><i class="fa fa-check"></i><b>10.31</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="10.32" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#hierarchical-clustering-types-of-linkage"><i class="fa fa-check"></i><b>10.32</b> Hierarchical Clustering: Types of Linkage</a></li>
<li class="chapter" data-level="10.33" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#hierarchical-clustering-algorithm"><i class="fa fa-check"></i><b>10.33</b> Hierarchical Clustering Algorithm</a></li>
<li class="chapter" data-level="10.34" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#hierarchical-clustering-choice-of-dissimilarity-measure"><i class="fa fa-check"></i><b>10.34</b> Hierarchical Clustering: Choice of Dissimilarity Measure</a></li>
<li class="chapter" data-level="10.35" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#practical-issues-in-clustering"><i class="fa fa-check"></i><b>10.35</b> Practical Issues in Clustering</a></li>
<li class="chapter" data-level="10.36" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#your-turn-66"><i class="fa fa-check"></i><b>10.36</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="10.37" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#your-turn-67"><i class="fa fa-check"></i><b>10.37</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="10.38" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#your-turn-68"><i class="fa fa-check"></i><b>10.38</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="10.39" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#to-sum-it-all-up"><i class="fa fa-check"></i><b>10.39</b> To Sum It All Up</a></li>
<li class="chapter" data-level="10.40" data-path="unsupervised-learning-1.html"><a href="unsupervised-learning-1.html#next-steps"><i class="fa fa-check"></i><b>10.40</b> Next Steps</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">208 Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-methods" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Tree-Based Methods<a href="tree-based-methods.html#tree-based-methods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li><p>Involves <strong>stratifying</strong> or <strong>segmenting</strong> the predictor space into a number of simple regions.</p></li>
<li><p>The set of splitting rules used to segment the predictor space can be summarized in a tree, thus, the name <strong>decision tree</strong> methods.</p></li>
<li><p>Can be used for both classification and regression.</p></li>
<li><p>Tree-based methods are simple and useful for interpretation, however, not the best in terms of prediction accuracy.</p></li>
<li><p>Methods such as <strong>bagging</strong>, <strong>random forests</strong>, and <strong>boosting</strong> grow multiple trees and then combine their results.</p></li>
</ul>
<!-- ## Regression Trees -->
<!-- **Hitters dataset** -->
<!-- ```{r,echo=FALSE} -->
<!-- data("Hitters") -->
<!-- Hitters=na.omit(Hitters) -->
<!-- head(Hitters) -->
<!-- ``` -->
<!-- ## Regression Trees -->
<!-- **Hitters dataset** -->
<!-- log(Salary) is color-coded from low (blue, green) to high (yellow, red). -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/SL_C8_1.png") -->
<!-- ``` -->
<!-- ## Regression Trees -->
<!-- **Hitters dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/8.1.png") -->
<!-- ``` -->
<!-- ## Regression Trees -->
<!-- **Hitters dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.2.png") -->
<!-- ``` -->
<div id="terminology-for-trees" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Terminology for Trees<a href="tree-based-methods.html#terminology-for-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Every split is considered to be a <strong>node</strong>.</p></li>
<li><p>We refer to the first node at the top of the tree as the <strong>root node</strong> (this node contains all of the training data).</p></li>
<li><p>The final nodes at the bottom of the tree are called the <strong>terminal nodes</strong> or <strong>leaves</strong>.</p></li>
</ul>
<!-- The regions $R_1$, $R_2$, and $R_3$ are known as **terminal nodes** or **leaves**. -->
<ul>
<li><p>Decision trees are typically drawn <strong>upside down</strong>, in the sense that the leaves are at the bottom of the tree.</p></li>
<li><p>The points along the tree where the predictor space is split are referred to as <strong>internal nodes</strong>, that is, every node in between the <strong>root node</strong> and <strong>terminal nodes</strong> is referred to as an <strong>internal node</strong>.</p></li>
<li><p>The segments of the trees that connect the nodes are known as <strong>branches</strong>.</p></li>
</ul>
</div>
<div id="terminology-for-trees-1" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Terminology for Trees<a href="tree-based-methods.html#terminology-for-trees-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-178"></span>
<img src="EFT/tree2.jpg" alt="Adapted from HMLR, Boehmke &amp; Greenwell" width="100%" />
<p class="caption">
Figure 8.1: Adapted from HMLR, Boehmke &amp; Greenwell
</p>
</div>
<!-- ## Interpretation of Trees -->
<!-- **Hitters dataset** -->
<!-- * **Years** is the most important factor in determining **Salary**, and players with less experience earn lower salaries than more experienced players. -->
<!-- * Given that a player is less experienced, the number of **Hits** that he made in the previous year seems to play little role in his **Salary**. -->
<!-- * But among players who have been in the major leagues for five or more years, the number of **Hits** made in the previous year does affect **Salary**, and players who made more **Hits** last year tend to have higher salaries. -->
<!-- * Surely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain. -->
</div>
<div id="building-a-tree-and-prediction" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Building a Tree and Prediction<a href="tree-based-methods.html#building-a-tree-and-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- Building a tree involves two steps. -->
<p>The steps involved are:</p>
<ol style="list-style-type: decimal">
<li><p>Dividing the predictor space, that is, the set of possible values for <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span> into <span class="math inline">\(J\)</span> distinct and non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>.</p></li>
<li><p>For every observation that falls into the region <span class="math inline">\(R_j\)</span>, make the same prediction, which is</p></li>
</ol>
<ul>
<li><p>the mean response of the training set observations in <span class="math inline">\(R_j\)</span> (for regression problems),</p></li>
<li><p>majority vote response of the training set observations in <span class="math inline">\(R_j\)</span> (for classification problems).</p></li>
</ul>
</div>
<div id="building-a-tree-and-prediction-1" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Building a Tree and Prediction<a href="tree-based-methods.html#building-a-tree-and-prediction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-179"></span>
<img src="EFT/tree1.jpg" alt="Adapted from HMLR, Boehmke &amp; Greenwell" width="100%" />
<p class="caption">
Figure 8.2: Adapted from HMLR, Boehmke &amp; Greenwell
</p>
</div>
</div>
<div id="building-a-tree-and-prediction-2" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Building a Tree and Prediction<a href="tree-based-methods.html#building-a-tree-and-prediction-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Default Dataset</strong></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-180-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="building-a-tree-and-prediction-3" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> Building a Tree and Prediction<a href="tree-based-methods.html#building-a-tree-and-prediction-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Default Dataset</strong></p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-181-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="building-a-tree-and-prediction-4" class="section level2 hasAnchor" number="8.7">
<h2><span class="header-section-number">8.7</span> Building a Tree and Prediction<a href="tree-based-methods.html#building-a-tree-and-prediction-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-182"></span>
<img src="EFT/8.3.png" alt="Adapted from ISLR, James et al." width="60%" />
<p class="caption">
Figure 8.3: Adapted from ISLR, James et al.
</p>
</div>
<!-- ## Building a Tree -->
<!-- * In theory, regions could have any shape. However, we -->
<!-- choose to divide the predictor space into **high-dimensional rectangles**, or **boxes**, for simplicity and for ease of interpretation of the resulting predictive model. -->
<!-- * **Objective**: Find boxes $R_1, R_2, \ldots R_J$ that minimize the $RSS$, -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '40%'} -->
<!-- knitr::include_graphics("EFT/e8.1.png") -->
<!-- ``` -->
<!-- where $\hat{y}_{R_j}$ is the mean response for the training observations within the $j^{th}$ box. -->
</div>
<div id="building-a-tree" class="section level2 hasAnchor" number="8.8">
<h2><span class="header-section-number">8.8</span> Building a Tree<a href="tree-based-methods.html#building-a-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>It is computationally infeasible to consider every possible partition of the feature space into <span class="math inline">\(J\)</span> boxes.</p></li>
<li><p>For this reason, we take a <strong>top-down, greedy</strong> approach known as <strong>recursive binary splitting</strong>.</p>
<ul>
<li><strong>top-down</strong> because it begins at the top of the tree and then successively splits the predictor space.</li>
<li><strong>greedy</strong> because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</li>
</ul></li>
</ul>
</div>
<div id="building-a-tree-1" class="section level2 hasAnchor" number="8.9">
<h2><span class="header-section-number">8.9</span> Building a Tree<a href="tree-based-methods.html#building-a-tree-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>First select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions
<span class="math inline">\(\{X|X_j &lt; s\}\)</span> and <span class="math inline">\(\{X|X_j \ge s\}\)</span> leads to the greatest possible reduction in RSS.</li>
</ul>
<p>For any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, define</p>
<p><img src="EFT/e8.2.png" width="67%" style="display: block; margin: auto;" /></p>
<p>Find <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that minimize</p>
<p><img src="EFT/e8.3.png" width="67%" style="display: block; margin: auto;" /></p>
<ul>
<li><p>Next, repeat the process, look for the best predictor and best cutpoint in order to split the data further.
However, this time, instead of splitting the entire predictor space, split one of the two previously identified regions.</p></li>
<li><p>The process continues until a stopping criterion is reached; say, we may continue until no region contains more than five observations.</p></li>
</ul>
<!-- ## Building a Tree -->
<!-- ```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'} -->
<!-- knitr::include_graphics("tree2.jpg") -->
<!-- ``` -->
</div>
<div id="tree-pruning" class="section level2 hasAnchor" number="8.10">
<h2><span class="header-section-number">8.10</span> Tree Pruning<a href="tree-based-methods.html#tree-pruning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>The process described above may <strong>overfit</strong> the data.</p></li>
<li><p>A smaller tree with fewer splits (that is, fewer regions <span class="math inline">\(R_1,\ldots,R_J\)</span>) might lead to lower variance and better interpretation at the cost of a little bias.</p></li>
<li><p>One possible alternative is to grow the tree only so long as the decrease in the <span class="math inline">\(RSS\)</span> due to each split exceeds some (high) threshold.</p></li>
<li><p>This strategy will result in smaller trees, but is too short-sighted: a seemingly worthless split early on in the tree might be followed by a very good split, that is, a split that leads to a large reduction in <span class="math inline">\(RSS\)</span> later on.</p></li>
</ul>
</div>
<div id="tree-pruning-1" class="section level2 hasAnchor" number="8.11">
<h2><span class="header-section-number">8.11</span> Tree Pruning<a href="tree-based-methods.html#tree-pruning-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>A better strategy is <strong>pruning</strong>.</p></li>
<li><p>Grow a very large tree <span class="math inline">\(T_0\)</span>, and then <strong>prune</strong> it back to obtain a <strong>subtree</strong>.</p></li>
<li><p>The technique uses is known as <strong>cost complexity pruning</strong> (also known as <strong>weakest link pruning</strong>).</p></li>
<li><p>Consider a sequence of trees indexed by <span class="math inline">\(\alpha\)</span>. For each <span class="math inline">\(\alpha\)</span>, consider the tree <span class="math inline">\(T \subset T_0\)</span> that minimizes</p></li>
</ul>
<p><img src="EFT/e8.4.png" width="70%" style="display: block; margin: auto;" /></p>
<ul>
<li>Choose optimal <span class="math inline">\(\alpha\)</span> by CV.</li>
</ul>
</div>
<div id="building-an-optimal-tree" class="section level2 hasAnchor" number="8.12">
<h2><span class="header-section-number">8.12</span> Building an Optimal Tree<a href="tree-based-methods.html#building-an-optimal-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><img src="EFT/algo8.1.png" width="90%" style="display: block; margin: auto;" /></p>
<!-- ## Building an Optimal Tree -->
<!-- **Hitters dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.4.png") -->
<!-- ``` -->
<!-- ## Building an Optimal Tree -->
<!-- **Hitters dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/8.5.png") -->
<!-- ``` -->
</div>
<div id="regression-tree-implementation" class="section level2 hasAnchor" number="8.13">
<h2><span class="header-section-number">8.13</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="tree-based-methods.html#cb318-1" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">&quot;AmesHousing.rds&quot;</span>)   <span class="co"># load dataset</span></span>
<span id="cb318-2"><a href="tree-based-methods.html#cb318-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb318-3"><a href="tree-based-methods.html#cb318-3" aria-hidden="true" tabindex="-1"></a>ames<span class="sc">$</span>Overall_Qual <span class="ot">&lt;-</span> <span class="fu">factor</span>(ames<span class="sc">$</span>Overall_Qual, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;Very_Poor&quot;</span>, <span class="st">&quot;Poor&quot;</span>, <span class="st">&quot;Fair&quot;</span>, <span class="st">&quot;Below_Average&quot;</span>,</span>
<span id="cb318-4"><a href="tree-based-methods.html#cb318-4" aria-hidden="true" tabindex="-1"></a>                                                  <span class="st">&quot;Average&quot;</span>, <span class="st">&quot;Above_Average&quot;</span>, <span class="st">&quot;Good&quot;</span>, <span class="st">&quot;Very_Good&quot;</span>,</span>
<span id="cb318-5"><a href="tree-based-methods.html#cb318-5" aria-hidden="true" tabindex="-1"></a>                                                  <span class="st">&quot;Excellent&quot;</span>, <span class="st">&quot;Very_Excellent&quot;</span>))</span></code></pre></div>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="tree-based-methods.html#cb319-1" aria-hidden="true" tabindex="-1"></a><span class="co"># split data</span></span>
<span id="cb319-2"><a href="tree-based-methods.html#cb319-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb319-3"><a href="tree-based-methods.html#cb319-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb319-4"><a href="tree-based-methods.html#cb319-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb319-5"><a href="tree-based-methods.html#cb319-5" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(<span class="at">y =</span> ames<span class="sc">$</span>Sale_Price, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)   <span class="co"># consider 70-30 split</span></span>
<span id="cb319-6"><a href="tree-based-methods.html#cb319-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb319-7"><a href="tree-based-methods.html#cb319-7" aria-hidden="true" tabindex="-1"></a>ames_train <span class="ot">&lt;-</span> ames[index,]   <span class="co"># training data</span></span>
<span id="cb319-8"><a href="tree-based-methods.html#cb319-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb319-9"><a href="tree-based-methods.html#cb319-9" aria-hidden="true" tabindex="-1"></a>ames_test <span class="ot">&lt;-</span> ames[<span class="sc">-</span>index,]   <span class="co"># test data</span></span></code></pre></div>
</div>
<div id="regression-tree-implementation-1" class="section level2 hasAnchor" number="8.14">
<h2><span class="header-section-number">8.14</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="tree-based-methods.html#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create recipe and blueprint, prepare and apply blueprint</span></span>
<span id="cb320-2"><a href="tree-based-methods.html#cb320-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-3"><a href="tree-based-methods.html#cb320-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb320-4"><a href="tree-based-methods.html#cb320-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-5"><a href="tree-based-methods.html#cb320-5" aria-hidden="true" tabindex="-1"></a>ames_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> ames_train)   <span class="co"># set up recipe</span></span>
<span id="cb320-6"><a href="tree-based-methods.html#cb320-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-7"><a href="tree-based-methods.html#cb320-7" aria-hidden="true" tabindex="-1"></a>blueprint <span class="ot">&lt;-</span> ames_recipe <span class="sc">%&gt;%</span></span>
<span id="cb320-8"><a href="tree-based-methods.html#cb320-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_nzv</span>(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) <span class="sc">%&gt;%</span>   <span class="co"># filter out zv/nzv predictors</span></span>
<span id="cb320-9"><a href="tree-based-methods.html#cb320-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_impute_mean</span>(Gr_Liv_Area) <span class="sc">%&gt;%</span>                                    <span class="co"># impute missing entries</span></span>
<span id="cb320-10"><a href="tree-based-methods.html#cb320-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_integer</span>(Overall_Qual) <span class="sc">%&gt;%</span>                                       <span class="co"># numeric conversion of levels of the predictors</span></span>
<span id="cb320-11"><a href="tree-based-methods.html#cb320-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_center</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span>                      <span class="co"># center (subtract mean) all numeric predictors</span></span>
<span id="cb320-12"><a href="tree-based-methods.html#cb320-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_scale</span>(<span class="fu">all_numeric</span>(), <span class="sc">-</span><span class="fu">all_outcomes</span>()) <span class="sc">%&gt;%</span>                       <span class="co"># scale (divide by standard deviation) all numeric predictors</span></span>
<span id="cb320-13"><a href="tree-based-methods.html#cb320-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_other</span>(Neighborhood, <span class="at">threshold =</span> <span class="fl">0.01</span>, <span class="at">other =</span> <span class="st">&quot;other&quot;</span>) <span class="sc">%&gt;%</span>      <span class="co"># lumping required predictors</span></span>
<span id="cb320-14"><a href="tree-based-methods.html#cb320-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_dummy</span>(<span class="fu">all_nominal</span>(), <span class="at">one_hot =</span> <span class="cn">TRUE</span>)                            <span class="co"># one-hot/dummy encode nominal categorical predictors</span></span>
<span id="cb320-15"><a href="tree-based-methods.html#cb320-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-16"><a href="tree-based-methods.html#cb320-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-17"><a href="tree-based-methods.html#cb320-17" aria-hidden="true" tabindex="-1"></a>prepare <span class="ot">&lt;-</span> <span class="fu">prep</span>(blueprint, <span class="at">data =</span> ames_train)    <span class="co"># estimate feature engineering parameters based on training data</span></span>
<span id="cb320-18"><a href="tree-based-methods.html#cb320-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-19"><a href="tree-based-methods.html#cb320-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-20"><a href="tree-based-methods.html#cb320-20" aria-hidden="true" tabindex="-1"></a>baked_train <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepare, <span class="at">new_data =</span> ames_train)   <span class="co"># apply the blueprint to training data for building final/optimal model</span></span>
<span id="cb320-21"><a href="tree-based-methods.html#cb320-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-22"><a href="tree-based-methods.html#cb320-22" aria-hidden="true" tabindex="-1"></a>baked_test <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepare, <span class="at">new_data =</span> ames_test)    <span class="co"># apply the blueprint to test data for future use</span></span></code></pre></div>
</div>
<div id="regression-tree-implementation-2" class="section level2 hasAnchor" number="8.15">
<h2><span class="header-section-number">8.15</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<p>Implement CV to tune the hyperparameter.</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="tree-based-methods.html#cb321-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb321-2"><a href="tree-based-methods.html#cb321-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-3"><a href="tree-based-methods.html#cb321-3" aria-hidden="true" tabindex="-1"></a>cv_specs <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">5</span>)   <span class="co"># CV specifications</span></span>
<span id="cb321-4"><a href="tree-based-methods.html#cb321-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-5"><a href="tree-based-methods.html#cb321-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-6"><a href="tree-based-methods.html#cb321-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb321-7"><a href="tree-based-methods.html#cb321-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-8"><a href="tree-based-methods.html#cb321-8" aria-hidden="true" tabindex="-1"></a>tree_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb321-9"><a href="tree-based-methods.html#cb321-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> ames_train,</span>
<span id="cb321-10"><a href="tree-based-methods.html#cb321-10" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,  </span>
<span id="cb321-11"><a href="tree-based-methods.html#cb321-11" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> cv_specs,</span>
<span id="cb321-12"><a href="tree-based-methods.html#cb321-12" aria-hidden="true" tabindex="-1"></a>                   <span class="at">tuneLength =</span> <span class="dv">20</span>,                   <span class="co"># considers a grid of 20 possible tuning parameter values</span></span>
<span id="cb321-13"><a href="tree-based-methods.html#cb321-13" aria-hidden="true" tabindex="-1"></a>                   <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in train_rec(rec = x, dat = data, info = trainInfo, method = models, :
## There were missing values in resampled performance measures.</code></pre>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="tree-based-methods.html#cb323-1" aria-hidden="true" tabindex="-1"></a><span class="co"># results from the CV procedure</span></span>
<span id="cb323-2"><a href="tree-based-methods.html#cb323-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb323-3"><a href="tree-based-methods.html#cb323-3" aria-hidden="true" tabindex="-1"></a>tree_cv<span class="sc">$</span>bestTune    <span class="co"># optimal hyperparameter</span></span></code></pre></div>
<pre><code>##           cp
## 1 0.00185082</code></pre>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="tree-based-methods.html#cb325-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(tree_cv<span class="sc">$</span>results<span class="sc">$</span>RMSE)   <span class="co"># optimal CV RMSE</span></span></code></pre></div>
<pre><code>## [1] 40442.78</code></pre>
</div>
<div id="regression-tree-implementation-3" class="section level2 smaller hasAnchor" number="8.16">
<h2><span class="header-section-number">8.16</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<p>Results from the CV procedure.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="tree-based-methods.html#cb327-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(tree_cv)   </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-192-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="regression-tree-implementation-4" class="section level2 hasAnchor" number="8.17">
<h2><span class="header-section-number">8.17</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="tree-based-methods.html#cb328-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build final model</span></span>
<span id="cb328-2"><a href="tree-based-methods.html#cb328-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb328-3"><a href="tree-based-methods.html#cb328-3" aria-hidden="true" tabindex="-1"></a>final_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Sale_Price <span class="sc">~</span> .,</span>
<span id="cb328-4"><a href="tree-based-methods.html#cb328-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> baked_train,</span>
<span id="cb328-5"><a href="tree-based-methods.html#cb328-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">cp =</span> tree_cv<span class="sc">$</span>bestTune<span class="sc">$</span>cp,</span>
<span id="cb328-6"><a href="tree-based-methods.html#cb328-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">xval =</span> <span class="dv">0</span>,                 <span class="co"># no further CV</span></span>
<span id="cb328-7"><a href="tree-based-methods.html#cb328-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">method =</span> <span class="st">&quot;anova&quot;</span>)         <span class="co"># for regression</span></span></code></pre></div>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="tree-based-methods.html#cb329-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain predictions and test set RMSE</span></span>
<span id="cb329-2"><a href="tree-based-methods.html#cb329-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb329-3"><a href="tree-based-methods.html#cb329-3" aria-hidden="true" tabindex="-1"></a>final_model_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model, <span class="at">newdata =</span> baked_test)    <span class="co"># obtain test set predictions</span></span>
<span id="cb329-4"><a href="tree-based-methods.html#cb329-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb329-5"><a href="tree-based-methods.html#cb329-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((final_model_preds <span class="sc">-</span> baked_test<span class="sc">$</span>Sale_Price)<span class="sc">^</span><span class="dv">2</span>))   <span class="co"># calculate test set RMSE</span></span></code></pre></div>
<pre><code>## [1] 50504.24</code></pre>
</div>
<div id="regression-tree-implementation-5" class="section level2 hasAnchor" number="8.18">
<h2><span class="header-section-number">8.18</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="tree-based-methods.html#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb331-2"><a href="tree-based-methods.html#cb331-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(final_model)    </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-195-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="regression-tree-implementation-6" class="section level2 smaller hasAnchor" number="8.19">
<h2><span class="header-section-number">8.19</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="tree-based-methods.html#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="co"># variable importance</span></span>
<span id="cb332-2"><a href="tree-based-methods.html#cb332-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-3"><a href="tree-based-methods.html#cb332-3" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(<span class="at">object =</span> tree_cv, <span class="at">num_features =</span> <span class="dv">20</span>, <span class="at">method =</span> <span class="st">&quot;model&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-196-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="regression-tree-implementation-7" class="section level2 hasAnchor" number="8.20">
<h2><span class="header-section-number">8.20</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="tree-based-methods.html#cb333-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build full grown tree (no pruning)</span></span>
<span id="cb333-2"><a href="tree-based-methods.html#cb333-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb333-3"><a href="tree-based-methods.html#cb333-3" aria-hidden="true" tabindex="-1"></a>final_model_no_prune <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Sale_Price <span class="sc">~</span> .,</span>
<span id="cb333-4"><a href="tree-based-methods.html#cb333-4" aria-hidden="true" tabindex="-1"></a>                            <span class="at">data =</span> baked_train,</span>
<span id="cb333-5"><a href="tree-based-methods.html#cb333-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">cp =</span> <span class="dv">0</span>,                   <span class="co"># no pruning</span></span>
<span id="cb333-6"><a href="tree-based-methods.html#cb333-6" aria-hidden="true" tabindex="-1"></a>                            <span class="at">xval =</span> <span class="dv">0</span>,                 <span class="co"># no CV</span></span>
<span id="cb333-7"><a href="tree-based-methods.html#cb333-7" aria-hidden="true" tabindex="-1"></a>                            <span class="at">method =</span> <span class="st">&quot;anova&quot;</span>)         <span class="co"># for regression</span></span></code></pre></div>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="tree-based-methods.html#cb334-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain predictions and test set RMSE</span></span>
<span id="cb334-2"><a href="tree-based-methods.html#cb334-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb334-3"><a href="tree-based-methods.html#cb334-3" aria-hidden="true" tabindex="-1"></a>final_model_no_prune_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model_no_prune, <span class="at">newdata =</span> baked_test)    <span class="co"># obtain test set predictions</span></span>
<span id="cb334-4"><a href="tree-based-methods.html#cb334-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb334-5"><a href="tree-based-methods.html#cb334-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((final_model_no_prune_preds <span class="sc">-</span> baked_test<span class="sc">$</span>Sale_Price)<span class="sc">^</span><span class="dv">2</span>))   <span class="co"># calculate test set RMSE</span></span></code></pre></div>
<pre><code>## [1] 50106.39</code></pre>
</div>
<div id="regression-tree-implementation-8" class="section level2 hasAnchor" number="8.21">
<h2><span class="header-section-number">8.21</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="tree-based-methods.html#cb336-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(final_model_no_prune)    </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-199-1.png" width="768" style="display: block; margin: auto;" /></p>
<!-- ## Regression Tree: Boston Data Example  {.smaller} -->
<!-- ```{r} -->
<!-- library(MASS) -->
<!-- data("Boston")   # load dataset -->
<!-- set.seed(05172022) -->
<!-- ## investigate dataset -->
<!-- str(Boston)  # check variable types -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example  {.smaller} -->
<!-- ```{r} -->
<!-- summary(Boston) -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example  {.smaller} -->
<!-- ```{r} -->
<!-- sum(is.na(Boston))   # check for missing values -->
<!-- nearZeroVar(Boston, saveMetrics = TRUE)   # check for zv/nzv features -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example   -->
<!-- ```{r} -->
<!-- # split the data into training and test sets -->
<!-- train_index <- createDataPartition(Boston$medv, p = 0.8, list = FALSE)   # 80-20 split -->
<!-- Boston_train <- Boston[train_index, ]   # training set -->
<!-- Boston_test <- Boston[-train_index, ]   # test set -->
<!-- # create feature preprocessing blueprint -->
<!-- blueprint <- recipe(medv ~ ., data = Boston_train) %>% -->
<!--   step_normalize(all_numeric_predictors()) -->
<!-- prepare <- prep(blueprint, training = Boston_train) -->
<!-- baked_train <- bake(prepare, new_data = Boston_train) -->
<!-- baked_test <- bake(prepare, new_data = Boston_test) -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example   -->
<!-- ```{r} -->
<!-- # implement CV -->
<!-- cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 5) -->
<!-- tree_cv <- train(blueprint, -->
<!--                  data = Boston_train, -->
<!--                  method = "rpart", -->
<!--                  trControl = cv_specs, -->
<!--                  tuneLength = 20,     # considers a grid of 20 possible tuning parameter values -->
<!--                  metric = "RMSE") -->
<!-- tree_cv$bestTune   # optimal tuning parameter -->
<!-- min(tree_cv$results$RMSE)   # CV RMSE -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example   -->
<!-- ```{r} -->
<!-- tree_cv$finalModel$variable.importance   # importance of features -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example   -->
<!-- ```{r} -->
<!-- # fit final optimal model -->
<!-- tree_fit <- rpart(medv ~ ., -->
<!--                   data = baked_train, -->
<!--                   cp = tree_cv$bestTune$cp, -->
<!--                   xval = 0, -->
<!--                   method = "anova") -->
<!-- preds_tree <- predict(tree_fit, newdata = baked_test)   # obtain predictions on test set -->
<!-- sqrt(mean((preds_tree - baked_test$medv)^2))   # test RMSE -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example   -->
<!-- ```{r, fig.align='center'} -->
<!-- rpart.plot(tree_fit, cex = 0.75) -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example   -->
<!-- ```{r} -->
<!-- # fit full grown tree (with no pruning) for comparison -->
<!-- tree_fit1 <- rpart(medv ~ ., -->
<!--                    data = baked_train, -->
<!--                    cp = 0,  -->
<!--                    xval = 0, -->
<!--                    method = "anova") -->
<!-- ``` -->
<!-- ## Regression Tree: Boston Data Example   -->
<!-- ```{r, fig.align='center'} -->
<!-- rpart.plot(tree_fit1, cex = 0.75) -->
<!-- ``` -->
<!-- ## Classification Trees -->
<!-- * Very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. -->
<!-- * For a classification tree, we predict that each observation belongs to the **most commonly occurring class** of training observations in the region to which it belongs. -->
<!-- * However, classification error is not sufficiently sensitive for tree-growing. -->
<!-- ## Classification Trees -->
<!-- * Another measure is the **Gini Index** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/e8.6.png") -->
<!-- ``` -->
<!-- measures the total variance across the $K$ classes. It is referred to as a measure of **node purity**. -->
<!-- * An alternative to the Gini index is **entropy** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/e8.7.png") -->
<!-- ``` -->
<!-- ## Classification Trees -->
<!-- **Heart dataset** -->
<!-- ```{r, echo=FALSE} -->
<!-- Heart=read.csv("Heart.csv") -->
<!-- Heart$X=NULL -->
<!-- head(Heart) -->
<!-- ``` -->
<!-- ## Classification Trees -->
<!-- **Heart dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->
<!-- knitr::include_graphics("EFT/8.6a.png") -->
<!-- ``` -->
<!-- ## Classification Trees -->
<!-- **Heart dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->
<!-- knitr::include_graphics("EFT/8.6b.png") -->
<!-- ``` -->
<!-- ## Trees vs Linear Models -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.7.png") -->
<!-- ``` -->
</div>
<div id="trees" class="section level2 hasAnchor" number="8.22">
<h2><span class="header-section-number">8.22</span> Trees<a href="tree-based-methods.html#trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Advantages</strong></p>
<ul>
<li><p>Easy to explain.</p></li>
<li><p>Closely mirror human decision-making.</p></li>
<li><p>Can be displayed graphically, and are easily interpreted by non-experts.</p></li>
<li><p>Handle qualitative predictors without creating dummy variables. Does not require standardization of predictors.</p></li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li><p>Do not have same level of prediction accuracy.</p></li>
<li><p>Can be very non-robust.</p></li>
</ul>
</div>
<div id="regression-tree-implementation-9" class="section level2 hasAnchor" number="8.23">
<h2><span class="header-section-number">8.23</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong> (minimal feature engineering)</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="tree-based-methods.html#cb337-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create recipe and blueprint, prepare and apply blueprint</span></span>
<span id="cb337-2"><a href="tree-based-methods.html#cb337-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-3"><a href="tree-based-methods.html#cb337-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb337-4"><a href="tree-based-methods.html#cb337-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-5"><a href="tree-based-methods.html#cb337-5" aria-hidden="true" tabindex="-1"></a>ames_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Sale_Price <span class="sc">~</span> ., <span class="at">data =</span> ames_train)   <span class="co"># set up recipe</span></span>
<span id="cb337-6"><a href="tree-based-methods.html#cb337-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-7"><a href="tree-based-methods.html#cb337-7" aria-hidden="true" tabindex="-1"></a>blueprint_new <span class="ot">&lt;-</span> ames_recipe <span class="sc">%&gt;%</span></span>
<span id="cb337-8"><a href="tree-based-methods.html#cb337-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_impute_mean</span>(Gr_Liv_Area)                                    <span class="co"># impute missing entries</span></span>
<span id="cb337-9"><a href="tree-based-methods.html#cb337-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-10"><a href="tree-based-methods.html#cb337-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-11"><a href="tree-based-methods.html#cb337-11" aria-hidden="true" tabindex="-1"></a>prepare_new <span class="ot">&lt;-</span> <span class="fu">prep</span>(blueprint_new, <span class="at">data =</span> ames_train)    <span class="co"># estimate feature engineering parameters based on training data</span></span>
<span id="cb337-12"><a href="tree-based-methods.html#cb337-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-13"><a href="tree-based-methods.html#cb337-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-14"><a href="tree-based-methods.html#cb337-14" aria-hidden="true" tabindex="-1"></a>baked_train_new <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepare_new, <span class="at">new_data =</span> ames_train)   <span class="co"># apply the blueprint to training data for building final/optimal model</span></span>
<span id="cb337-15"><a href="tree-based-methods.html#cb337-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-16"><a href="tree-based-methods.html#cb337-16" aria-hidden="true" tabindex="-1"></a>baked_test_new <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepare_new, <span class="at">new_data =</span> ames_test)    <span class="co"># apply the blueprint to test data for future use</span></span></code></pre></div>
</div>
<div id="regression-tree-implementation-10" class="section level2 hasAnchor" number="8.24">
<h2><span class="header-section-number">8.24</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<p>Implement CV to tune the hyperparameter.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="tree-based-methods.html#cb338-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb338-2"><a href="tree-based-methods.html#cb338-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb338-3"><a href="tree-based-methods.html#cb338-3" aria-hidden="true" tabindex="-1"></a>cv_specs <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">5</span>)   <span class="co"># CV specifications</span></span>
<span id="cb338-4"><a href="tree-based-methods.html#cb338-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb338-5"><a href="tree-based-methods.html#cb338-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb338-6"><a href="tree-based-methods.html#cb338-6" aria-hidden="true" tabindex="-1"></a>tree_cv_min_fe <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint_new,</span>
<span id="cb338-7"><a href="tree-based-methods.html#cb338-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> ames_train,</span>
<span id="cb338-8"><a href="tree-based-methods.html#cb338-8" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,  </span>
<span id="cb338-9"><a href="tree-based-methods.html#cb338-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> cv_specs,</span>
<span id="cb338-10"><a href="tree-based-methods.html#cb338-10" aria-hidden="true" tabindex="-1"></a>                   <span class="at">tuneLength =</span> <span class="dv">20</span>,                   <span class="co"># considers a grid of 20 possible tuning parameter values</span></span>
<span id="cb338-11"><a href="tree-based-methods.html#cb338-11" aria-hidden="true" tabindex="-1"></a>                   <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in train_rec(rec = x, dat = data, info = trainInfo, method = models, :
## There were missing values in resampled performance measures.</code></pre>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="tree-based-methods.html#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="co"># results from the CV procedure</span></span>
<span id="cb340-2"><a href="tree-based-methods.html#cb340-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-3"><a href="tree-based-methods.html#cb340-3" aria-hidden="true" tabindex="-1"></a>tree_cv_min_fe<span class="sc">$</span>bestTune    <span class="co"># optimal hyperparameter</span></span></code></pre></div>
<pre><code>##            cp
## 1 0.002515201</code></pre>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="tree-based-methods.html#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(tree_cv_min_fe<span class="sc">$</span>results<span class="sc">$</span>RMSE)   <span class="co"># optimal CV RMSE</span></span></code></pre></div>
<pre><code>## [1] 40105.36</code></pre>
</div>
<div id="regression-tree-implementation-11" class="section level2 hasAnchor" number="8.25">
<h2><span class="header-section-number">8.25</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="tree-based-methods.html#cb344-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build final model</span></span>
<span id="cb344-2"><a href="tree-based-methods.html#cb344-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb344-3"><a href="tree-based-methods.html#cb344-3" aria-hidden="true" tabindex="-1"></a>final_model_min_fe <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Sale_Price <span class="sc">~</span> .,</span>
<span id="cb344-4"><a href="tree-based-methods.html#cb344-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> baked_train_new,</span>
<span id="cb344-5"><a href="tree-based-methods.html#cb344-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">cp =</span> tree_cv_min_fe<span class="sc">$</span>bestTune<span class="sc">$</span>cp,</span>
<span id="cb344-6"><a href="tree-based-methods.html#cb344-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">xval =</span> <span class="dv">0</span>,                 <span class="co"># no CV</span></span>
<span id="cb344-7"><a href="tree-based-methods.html#cb344-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">method =</span> <span class="st">&quot;anova&quot;</span>)         <span class="co"># for regression</span></span></code></pre></div>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="tree-based-methods.html#cb345-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain predictions and test set RMSE</span></span>
<span id="cb345-2"><a href="tree-based-methods.html#cb345-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-3"><a href="tree-based-methods.html#cb345-3" aria-hidden="true" tabindex="-1"></a>final_model_min_fe_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model_min_fe, <span class="at">newdata =</span> baked_test_new)    <span class="co"># obtain test set predictions</span></span>
<span id="cb345-4"><a href="tree-based-methods.html#cb345-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-5"><a href="tree-based-methods.html#cb345-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((final_model_min_fe_preds <span class="sc">-</span> baked_test_new<span class="sc">$</span>Sale_Price)<span class="sc">^</span><span class="dv">2</span>))   <span class="co"># calculate test set RMSE</span></span></code></pre></div>
<pre><code>## [1] 52063.37</code></pre>
</div>
<div id="regression-tree-implementation-12" class="section level2 hasAnchor" number="8.26">
<h2><span class="header-section-number">8.26</span> Regression Tree: Implementation<a href="tree-based-methods.html#regression-tree-implementation-12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="tree-based-methods.html#cb347-1" aria-hidden="true" tabindex="-1"></a><span class="co"># variable importance</span></span>
<span id="cb347-2"><a href="tree-based-methods.html#cb347-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-3"><a href="tree-based-methods.html#cb347-3" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(<span class="at">object =</span> tree_cv_min_fe, <span class="at">num_features =</span> <span class="dv">20</span>, <span class="at">method =</span> <span class="st">&quot;model&quot;</span>)          </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-205-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="classification-trees" class="section level2 hasAnchor" number="8.27">
<h2><span class="header-section-number">8.27</span> Classification Trees<a href="tree-based-methods.html#classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Still use <strong>recursive binary splitting</strong> to grow a classification tree.</p></li>
<li><p><span class="math inline">\(RSS\)</span> can be replaced by</p>
<ul>
<li><strong>classification error rate</strong>, the fraction of the training observations in that region that do not belong to the most common class.</li>
</ul>
<p><span class="math display">\[E = 1 - \max_k \left(\hat{p}_{mk}\right)\]</span></p>
<ul>
<li><strong>Gini index</strong>, a measure of node purity—a small value indicates that a node contains predominantly observations from a single class.</li>
</ul>
<p><span class="math display">\[G = \displaystyle \sum_{k=1}^{K} \hat{p}_{mk}\left(1-\hat{p}_{mk}\right)\]</span></p></li>
</ul>
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/e8.5.png") -->
<!-- ``` -->
<p>Here <span class="math inline">\(\hat{p}_{mk}\)</span> represents the proportion of training observations in the <span class="math inline">\(m^{th}\)</span> region that are from the <span class="math inline">\(k^{th}\)</span> class.</p>
</div>
<div id="your-turn-30" class="section level2 smaller hasAnchor" number="8.28">
<h2><span class="header-section-number">8.28</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-30" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You will work with the <code>iris</code> dataset which contains measurements in centimeters of four variables for 50 flowers from each of 3 species of iris: setosa, versicolor, and virginica. Please load the dataset using the following code.</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="tree-based-methods.html#cb348-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)    <span class="co"># load dataset</span></span></code></pre></div>
<p>We are interested in predicting <code>Species</code> using the rest of the variables in the dataset. Compare the performance (in terms of <strong>Accuracy</strong>) of the following models:</p>
<ul>
<li><p>A logistic regression model;</p></li>
<li><p>A <span class="math inline">\(K\)</span>-NN model with optimal <span class="math inline">\(K\)</span> chosen by CV;</p></li>
<li><p>A classification tree with optimal hyperparameter chosen by CV. Use <code>tuneLength = 20</code>.</p></li>
<li><p>A classification tree with minimal feature engineering. Use CV to choose the optimal hyperparameter. Use <code>tuneLength = 20</code>.</p></li>
</ul>
<p><strong>Perform the following tasks.</strong></p>
<ul>
<li><p>Investigate the dataset and complete any necessary tasks.</p></li>
<li><p>Split the data into training and test sets (75-25).</p></li>
<li><p>Perform required data preprocessing and create the blueprint. If using <code>step_dummy()</code>, set <code>one_hot = FALSE</code>. Prepare the blueprint on the training data. Obtain the modified training and test datasets.</p></li>
<li><p>Implement 10-fold CV repeated 5 times for each of the models above.</p></li>
<li><p>Report the optimal CV Accuracy of each model. Report the optimal hyperparameters for each model. Which model performs best in this situation?</p></li>
<li><p>Build the final model. Obtain class label predictions on the test set. Create the corresponding confusion matrix and report the test set accuracy. Based on your optimal final model, consult the help page for either <code>predict.knn3</code> or <code>predict.rpart</code> functions.</p></li>
</ul>
</div>
<div id="your-turn-31" class="section level2 smaller hasAnchor" number="8.29">
<h2><span class="header-section-number">8.29</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-31" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="tree-based-methods.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(iris)   <span class="co"># all features are numerical</span></span></code></pre></div>
<pre><code>## Rows: 150
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…
## $ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…
## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…
## $ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…</code></pre>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="tree-based-methods.html#cb351-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(iris)   <span class="co"># summary of variables</span></span></code></pre></div>
<pre><code>##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## </code></pre>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="tree-based-methods.html#cb353-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(iris))  <span class="co"># no missing entries</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
</div>
<div id="your-turn-32" class="section level2 hasAnchor" number="8.30">
<h2><span class="header-section-number">8.30</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-32" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="tree-based-methods.html#cb355-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb355-2"><a href="tree-based-methods.html#cb355-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb355-3"><a href="tree-based-methods.html#cb355-3" aria-hidden="true" tabindex="-1"></a><span class="co"># split the data into training and test sets</span></span>
<span id="cb355-4"><a href="tree-based-methods.html#cb355-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb355-5"><a href="tree-based-methods.html#cb355-5" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(iris<span class="sc">$</span>Species, <span class="at">p =</span> <span class="fl">0.75</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb355-6"><a href="tree-based-methods.html#cb355-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb355-7"><a href="tree-based-methods.html#cb355-7" aria-hidden="true" tabindex="-1"></a>iris_train <span class="ot">&lt;-</span> iris[index, ]</span>
<span id="cb355-8"><a href="tree-based-methods.html#cb355-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb355-9"><a href="tree-based-methods.html#cb355-9" aria-hidden="true" tabindex="-1"></a>iris_test <span class="ot">&lt;-</span> iris[<span class="sc">-</span>index, ]</span></code></pre></div>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="tree-based-methods.html#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nearZeroVar</span>(iris_train, <span class="at">saveMetrics =</span> <span class="cn">TRUE</span>)  <span class="co"># no zv/nzv features</span></span></code></pre></div>
<pre><code>##              freqRatio percentUnique zeroVar   nzv
## Sepal.Length  1.000000     27.192982   FALSE FALSE
## Sepal.Width   2.090909     17.543860   FALSE FALSE
## Petal.Length  1.250000     35.964912   FALSE FALSE
## Petal.Width   2.300000     19.298246   FALSE FALSE
## Species       1.000000      2.631579   FALSE FALSE</code></pre>
</div>
<div id="your-turn-33" class="section level2 hasAnchor" number="8.31">
<h2><span class="header-section-number">8.31</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-33" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="tree-based-methods.html#cb358-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb358-2"><a href="tree-based-methods.html#cb358-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb358-3"><a href="tree-based-methods.html#cb358-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create recipe and blueprint, prepare and apply blueprint</span></span>
<span id="cb358-4"><a href="tree-based-methods.html#cb358-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb358-5"><a href="tree-based-methods.html#cb358-5" aria-hidden="true" tabindex="-1"></a>blueprint <span class="ot">&lt;-</span> <span class="fu">recipe</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris_train) <span class="sc">%&gt;%</span></span>
<span id="cb358-6"><a href="tree-based-methods.html#cb358-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())</span>
<span id="cb358-7"><a href="tree-based-methods.html#cb358-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb358-8"><a href="tree-based-methods.html#cb358-8" aria-hidden="true" tabindex="-1"></a>prepare <span class="ot">&lt;-</span> <span class="fu">prep</span>(blueprint, <span class="at">training =</span> iris_train)</span>
<span id="cb358-9"><a href="tree-based-methods.html#cb358-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb358-10"><a href="tree-based-methods.html#cb358-10" aria-hidden="true" tabindex="-1"></a>baked_train <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepare, <span class="at">new_data =</span> iris_train)</span>
<span id="cb358-11"><a href="tree-based-methods.html#cb358-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb358-12"><a href="tree-based-methods.html#cb358-12" aria-hidden="true" tabindex="-1"></a>baked_test <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepare, <span class="at">new_data =</span> iris_test)</span></code></pre></div>
</div>
<div id="your-turn-34" class="section level2 hasAnchor" number="8.32">
<h2><span class="header-section-number">8.32</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-34" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="tree-based-methods.html#cb359-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb359-2"><a href="tree-based-methods.html#cb359-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb359-3"><a href="tree-based-methods.html#cb359-3" aria-hidden="true" tabindex="-1"></a>cv_specs <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">5</span>)   <span class="co"># CV specifications</span></span></code></pre></div>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="tree-based-methods.html#cb360-1" aria-hidden="true" tabindex="-1"></a>logistic_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb360-2"><a href="tree-based-methods.html#cb360-2" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> iris_train, </span>
<span id="cb360-3"><a href="tree-based-methods.html#cb360-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb360-4"><a href="tree-based-methods.html#cb360-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb360-5"><a href="tree-based-methods.html#cb360-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">trControl =</span> cv_specs,</span>
<span id="cb360-6"><a href="tree-based-methods.html#cb360-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)</span>
<span id="cb360-7"><a href="tree-based-methods.html#cb360-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-8"><a href="tree-based-methods.html#cb360-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The code above will throw an error since this is a 3-class classification problem and </span></span>
<span id="cb360-9"><a href="tree-based-methods.html#cb360-9" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic regression (with family = binomial) works for a binary (2-class) problem.</span></span></code></pre></div>
</div>
<div id="your-turn-35" class="section level2 hasAnchor" number="8.33">
<h2><span class="header-section-number">8.33</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-35" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="tree-based-methods.html#cb361-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb361-2"><a href="tree-based-methods.html#cb361-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb361-3"><a href="tree-based-methods.html#cb361-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CV for KNN</span></span>
<span id="cb361-4"><a href="tree-based-methods.html#cb361-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb361-5"><a href="tree-based-methods.html#cb361-5" aria-hidden="true" tabindex="-1"></a>k_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">101</span>, <span class="at">by =</span> <span class="dv">10</span>))</span>
<span id="cb361-6"><a href="tree-based-methods.html#cb361-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb361-7"><a href="tree-based-methods.html#cb361-7" aria-hidden="true" tabindex="-1"></a>knn_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb361-8"><a href="tree-based-methods.html#cb361-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> iris_train, </span>
<span id="cb361-9"><a href="tree-based-methods.html#cb361-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb361-10"><a href="tree-based-methods.html#cb361-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> cv_specs,</span>
<span id="cb361-11"><a href="tree-based-methods.html#cb361-11" aria-hidden="true" tabindex="-1"></a>                <span class="at">tuneGrid =</span> k_grid,</span>
<span id="cb361-12"><a href="tree-based-methods.html#cb361-12" aria-hidden="true" tabindex="-1"></a>                <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="tree-based-methods.html#cb362-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb362-2"><a href="tree-based-methods.html#cb362-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-3"><a href="tree-based-methods.html#cb362-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CV with tree</span></span>
<span id="cb362-4"><a href="tree-based-methods.html#cb362-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-5"><a href="tree-based-methods.html#cb362-5" aria-hidden="true" tabindex="-1"></a>tree_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb362-6"><a href="tree-based-methods.html#cb362-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data =</span> iris_train,</span>
<span id="cb362-7"><a href="tree-based-methods.html#cb362-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb362-8"><a href="tree-based-methods.html#cb362-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trControl =</span> cv_specs,</span>
<span id="cb362-9"><a href="tree-based-methods.html#cb362-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">tuneLength =</span> <span class="dv">20</span>,</span>
<span id="cb362-10"><a href="tree-based-methods.html#cb362-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)</span></code></pre></div>
</div>
<div id="your-turn-36" class="section level2 hasAnchor" number="8.34">
<h2><span class="header-section-number">8.34</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-36" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="tree-based-methods.html#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb363-2"><a href="tree-based-methods.html#cb363-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb363-3"><a href="tree-based-methods.html#cb363-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CV with tree (minimal feature engineering, no engineering in this case)</span></span>
<span id="cb363-4"><a href="tree-based-methods.html#cb363-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb363-5"><a href="tree-based-methods.html#cb363-5" aria-hidden="true" tabindex="-1"></a>tree_cv_min_fe <span class="ot">&lt;-</span> <span class="fu">train</span>(Species <span class="sc">~</span> .,</span>
<span id="cb363-6"><a href="tree-based-methods.html#cb363-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data =</span> iris_train,</span>
<span id="cb363-7"><a href="tree-based-methods.html#cb363-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb363-8"><a href="tree-based-methods.html#cb363-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trControl =</span> cv_specs,</span>
<span id="cb363-9"><a href="tree-based-methods.html#cb363-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">tuneLength =</span> <span class="dv">20</span>,</span>
<span id="cb363-10"><a href="tree-based-methods.html#cb363-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)</span></code></pre></div>
</div>
<div id="your-turn-37" class="section level2 hasAnchor" number="8.35">
<h2><span class="header-section-number">8.35</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-37" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="tree-based-methods.html#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal CV Accuracies</span></span>
<span id="cb364-2"><a href="tree-based-methods.html#cb364-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-3"><a href="tree-based-methods.html#cb364-3" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(knn_cv<span class="sc">$</span>results<span class="sc">$</span>Accuracy)     <span class="co"># for KNN</span></span></code></pre></div>
<pre><code>## [1] 0.9688788</code></pre>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="tree-based-methods.html#cb366-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(tree_cv<span class="sc">$</span>results<span class="sc">$</span>Accuracy)     <span class="co"># for classification tree</span></span></code></pre></div>
<pre><code>## [1] 0.9353636</code></pre>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="tree-based-methods.html#cb368-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(tree_cv_min_fe<span class="sc">$</span>results<span class="sc">$</span>Accuracy)     <span class="co"># for classification tree with minimal feature engineering</span></span></code></pre></div>
<pre><code>## [1] 0.9353636</code></pre>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="tree-based-methods.html#cb370-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal hyperparameters</span></span>
<span id="cb370-2"><a href="tree-based-methods.html#cb370-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb370-3"><a href="tree-based-methods.html#cb370-3" aria-hidden="true" tabindex="-1"></a>knn_cv<span class="sc">$</span>bestTune<span class="sc">$</span>k    <span class="co"># for KNN</span></span></code></pre></div>
<pre><code>## [1] 11</code></pre>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="tree-based-methods.html#cb372-1" aria-hidden="true" tabindex="-1"></a>tree_cv<span class="sc">$</span>bestTune<span class="sc">$</span>cp    <span class="co"># for classification tree</span></span></code></pre></div>
<pre><code>## [1] 0.4210526</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="tree-based-methods.html#cb374-1" aria-hidden="true" tabindex="-1"></a>tree_cv_min_fe<span class="sc">$</span>bestTune<span class="sc">$</span>cp    <span class="co"># for classification tree with minimal feature engineering</span></span></code></pre></div>
<pre><code>## [1] 0.4210526</code></pre>
</div>
<div id="your-turn-38" class="section level2 smaller hasAnchor" number="8.36">
<h2><span class="header-section-number">8.36</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-38" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="tree-based-methods.html#cb376-1" aria-hidden="true" tabindex="-1"></a><span class="co"># final model</span></span>
<span id="cb376-2"><a href="tree-based-methods.html#cb376-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb376-3"><a href="tree-based-methods.html#cb376-3" aria-hidden="true" tabindex="-1"></a>final_model <span class="ot">&lt;-</span> <span class="fu">knn3</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> baked_train, <span class="at">k =</span> knn_cv<span class="sc">$</span>bestTune<span class="sc">$</span>k)</span></code></pre></div>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="tree-based-methods.html#cb377-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain probability and class label predictions</span></span>
<span id="cb377-2"><a href="tree-based-methods.html#cb377-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb377-3"><a href="tree-based-methods.html#cb377-3" aria-hidden="true" tabindex="-1"></a>final_model_prob_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model, <span class="at">newdata =</span> baked_test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)   <span class="co"># probability predictions</span></span>
<span id="cb377-4"><a href="tree-based-methods.html#cb377-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb377-5"><a href="tree-based-methods.html#cb377-5" aria-hidden="true" tabindex="-1"></a>final_model_class_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model, <span class="at">newdata =</span> baked_test, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)   <span class="co"># class label predictions</span></span>
<span id="cb377-6"><a href="tree-based-methods.html#cb377-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb377-7"><a href="tree-based-methods.html#cb377-7" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> final_model_class_preds, <span class="at">reference =</span> baked_test<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         12          0         0
##   versicolor      0         11         1
##   virginica       0          1        11
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9444          
##                  95% CI : (0.8134, 0.9932)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : 1.728e-14       
##                                           
##                   Kappa : 0.9167          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9167           0.9167
## Specificity                 1.0000            0.9583           0.9583
## Pos Pred Value              1.0000            0.9167           0.9167
## Neg Pred Value              1.0000            0.9583           0.9583
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3056           0.3056
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           1.0000            0.9375           0.9375</code></pre>
</div>
<div id="ensemble-methods" class="section level2 hasAnchor" number="8.37">
<h2><span class="header-section-number">8.37</span> Ensemble Methods<a href="tree-based-methods.html#ensemble-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Single regression or classification trees usually have poor predictive performance.</p>
<p><strong>Ensemble Methods</strong> use a collection of multiple trees to improve the predictive performance at the cost of interpretability.</p>
<ul>
<li><p>Bagging</p></li>
<li><p>Random Forests</p></li>
<li><p>Boosting</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-222"></span>
<img src="EFT/2.7.png" alt="Adapted from ISLR, James et al." width="70%" />
<p class="caption">
Figure 8.4: Adapted from ISLR, James et al.
</p>
</div>
</div>
<div id="bagging" class="section level2 hasAnchor" number="8.38">
<h2><span class="header-section-number">8.38</span> Bagging<a href="tree-based-methods.html#bagging" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><strong>Bootstrap aggregation</strong> or <strong>bagging</strong> is a general-purpose procedure for reducing the variance of a statistical learning method.</p></li>
<li><p><strong>Idea</strong>: Build multiple trees and average their results.</p></li>
<li><p><strong>Result</strong>: Given a set of <span class="math inline">\(n\)</span> independent observations (random variables) <span class="math inline">\(Z_1, \ldots, Z_n\)</span>, each with variance <span class="math inline">\(\sigma^2\)</span>, the variance of the mean/average <span class="math inline">\(\bar{Z} = \displaystyle \dfrac{Z_1 + Z_2 + \cdots + Z_n}{n}\)</span> of the observations is <span class="math inline">\(\sigma^2/n\)</span>.</p></li>
</ul>
<p>In other words, <strong>averaging a set of observations reduces variance</strong>.</p>
<ul>
<li>In reality, we do not have multiple training datasets.</li>
</ul>
</div>
<div id="bagging-1" class="section level2 hasAnchor" number="8.39">
<h2><span class="header-section-number">8.39</span> Bagging<a href="tree-based-methods.html#bagging-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Bootstrapping</strong></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-223"></span>
<img src="EFT/bootstrapping.PNG" alt="Adapted from ISLR, James et al." width="70%" />
<p class="caption">
Figure 8.5: Adapted from ISLR, James et al.
</p>
</div>
</div>
<div id="bagging-2" class="section level2 hasAnchor" number="8.40">
<h2><span class="header-section-number">8.40</span> Bagging<a href="tree-based-methods.html#bagging-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Take repeated bootstrap samples (say <span class="math inline">\(B\)</span>) from the original (single) available dataset.</p></li>
<li><p>Build a tree on each bootstrap sample and obtain predictions <span class="math inline">\(\hat{f}^{*b}(x), \ b=1, 2, \ldots, B\)</span>.</p></li>
<li><p>Average all the predictions.</p></li>
</ul>
<p><img src="EFT/e8.95b.png" width="40%" style="display: block; margin: auto;" /></p>
<ul>
<li><p>Individual trees are grown deep and are not pruned. They have high variance, but low bias.</p></li>
<li><p>For classification trees, take <strong>majority vote</strong>: the overall prediction is the most commonly occurring class among the <span class="math inline">\(B\)</span> predictions.</p></li>
</ul>
</div>
<div id="out-of-bag-error-estimation" class="section level2 hasAnchor" number="8.41">
<h2><span class="header-section-number">8.41</span> Out-of-Bag Error Estimation<a href="tree-based-methods.html#out-of-bag-error-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>A straightforward way to estimate the test error of a bagged model, without performing CV.</p></li>
<li><p>It can be shown that on average, each bagged tree (constructed on each bootstrap sample) makes use of around two-thirds of the observations.</p></li>
</ul>
<!-- (Exercise 2 from Chapter 5) -->
<ul>
<li><p>Remaining one-third observations not used train a bagged tree are referred to as <strong>out-of-bag (OOB)</strong> observations.</p></li>
<li><p>For <span class="math inline">\(i^{th}\)</span> observation, use the trees in which that observation was OOB. This will yield around <span class="math inline">\(B/3\)</span> predictions for the <span class="math inline">\(i^{th}\)</span> observation. Take their average to obtain a single prediction.</p></li>
<li><p>Equivalent to LOOCV if <span class="math inline">\(B\)</span> is large.</p></li>
</ul>
</div>
<div id="variable-importance-measures" class="section level2 hasAnchor" number="8.42">
<h2><span class="header-section-number">8.42</span> Variable Importance Measures<a href="tree-based-methods.html#variable-importance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Bagging improves prediction accuracy at the expense of interpretability.</p></li>
<li><p>However, one can still obtain an overall summary of the importance of each predictor.</p></li>
</ul>
<!-- ## Variable Importance Measures -->
<ul>
<li><p>To measure feature importance, the reduction in the loss function (e.g., RSS) attributed to each variable at each split is tabulated. In some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance.</p></li>
<li><p>A large value indicates an important predictor.</p></li>
</ul>
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/8.9.png") -->
<!-- ``` -->
</div>
<div id="bagging-implementation" class="section level2 hasAnchor" number="8.43">
<h2><span class="header-section-number">8.43</span> Bagging: Implementation<a href="tree-based-methods.html#bagging-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<p>Data splitting and feature engineering has been done in the previous slides.</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="tree-based-methods.html#cb379-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb379-2"><a href="tree-based-methods.html#cb379-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb379-3"><a href="tree-based-methods.html#cb379-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ipred)</span>
<span id="cb379-4"><a href="tree-based-methods.html#cb379-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb379-5"><a href="tree-based-methods.html#cb379-5" aria-hidden="true" tabindex="-1"></a>bag_fit <span class="ot">&lt;-</span> <span class="fu">bagging</span>(Sale_Price <span class="sc">~</span> ., </span>
<span id="cb379-6"><a href="tree-based-methods.html#cb379-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> baked_train,</span>
<span id="cb379-7"><a href="tree-based-methods.html#cb379-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">nbagg =</span> <span class="dv">500</span>,                             <span class="co"># number of trees to grow (bootstrap samples) usually 500</span></span>
<span id="cb379-8"><a href="tree-based-methods.html#cb379-8" aria-hidden="true" tabindex="-1"></a>                  <span class="at">coob =</span> <span class="cn">TRUE</span>,                             <span class="co"># yes to computing OOB error estimate</span></span>
<span id="cb379-9"><a href="tree-based-methods.html#cb379-9" aria-hidden="true" tabindex="-1"></a>                  <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">2</span>,    <span class="co"># split a node if at least 2 observations present</span></span>
<span id="cb379-10"><a href="tree-based-methods.html#cb379-10" aria-hidden="true" tabindex="-1"></a>                                          <span class="at">cp =</span> <span class="dv">0</span>,          <span class="co"># no pruning (let the trees grow tall)</span></span>
<span id="cb379-11"><a href="tree-based-methods.html#cb379-11" aria-hidden="true" tabindex="-1"></a>                                          <span class="at">xval =</span> <span class="dv">0</span>))       <span class="co"># no CV </span></span>
<span id="cb379-12"><a href="tree-based-methods.html#cb379-12" aria-hidden="true" tabindex="-1"></a>                  </span>
<span id="cb379-13"><a href="tree-based-methods.html#cb379-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb379-14"><a href="tree-based-methods.html#cb379-14" aria-hidden="true" tabindex="-1"></a>bag_fit</span></code></pre></div>
<pre><code>## 
## Bagging regression trees with 500 bootstrap replications 
## 
## Call: bagging.data.frame(formula = Sale_Price ~ ., data = baked_train, 
##     nbagg = 500, coob = TRUE, control = rpart.control(minsplit = 2, 
##         cp = 0, xval = 0))
## 
## Out-of-bag estimate of root mean squared error:  28762.96</code></pre>
</div>
<div id="bagging-implementation-1" class="section level2 hasAnchor" number="8.44">
<h2><span class="header-section-number">8.44</span> Bagging: Implementation<a href="tree-based-methods.html#bagging-implementation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<p>CV with bagging (NOT recommended since computationally expensive)</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="tree-based-methods.html#cb381-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb381-2"><a href="tree-based-methods.html#cb381-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-3"><a href="tree-based-methods.html#cb381-3" aria-hidden="true" tabindex="-1"></a>cv_specs <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">5</span>)   <span class="co"># CV specifications</span></span>
<span id="cb381-4"><a href="tree-based-methods.html#cb381-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-5"><a href="tree-based-methods.html#cb381-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ipred)</span>
<span id="cb381-6"><a href="tree-based-methods.html#cb381-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb381-7"><a href="tree-based-methods.html#cb381-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-8"><a href="tree-based-methods.html#cb381-8" aria-hidden="true" tabindex="-1"></a>bagging_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb381-9"><a href="tree-based-methods.html#cb381-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> ames_train,</span>
<span id="cb381-10"><a href="tree-based-methods.html#cb381-10" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&quot;treebag&quot;</span>,  </span>
<span id="cb381-11"><a href="tree-based-methods.html#cb381-11" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> cv_specs,</span>
<span id="cb381-12"><a href="tree-based-methods.html#cb381-12" aria-hidden="true" tabindex="-1"></a>                   <span class="at">nbagg =</span> <span class="dv">500</span>,                   </span>
<span id="cb381-13"><a href="tree-based-methods.html#cb381-13" aria-hidden="true" tabindex="-1"></a>                   <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">2</span>, <span class="at">cp =</span> <span class="dv">0</span>),    </span>
<span id="cb381-14"><a href="tree-based-methods.html#cb381-14" aria-hidden="true" tabindex="-1"></a>                   <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</span></code></pre></div>
<!-- ```{r, echo=FALSE} -->
<!-- data("Boston") -->
<!-- train_index <- createDataPartition(Boston$medv, p = 0.8, list = FALSE) -->
<!-- Boston_train <- Boston[train_index, ] -->
<!-- Boston_test <- Boston[-train_index, ] -->
<!-- blueprint <- recipe(medv ~ ., data = Boston_train) %>% -->
<!--   step_normalize(all_numeric_predictors()) -->
<!-- prepare <- prep(blueprint, training = Boston_train) -->
<!-- baked_train <- bake(prepare, new_data = Boston_train) -->
<!-- baked_test <- bake(prepare, new_data = Boston_test) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- bag_fit <- bagging(medv ~ .,  -->
<!--                   data = baked_train, -->
<!--                   nbagg = 500,   # number of trees to grow (bootstrap replications) -->
<!--                   coob = TRUE,   # yes to computing OOB error estimate -->
<!--                   control = rpart.control(minsplit = 2, cp = 0, xval = 0))  # details of each tree -->
<!-- bag_fit -->
<!-- ``` -->
</div>
<div id="bagging-implementation-2" class="section level2 hasAnchor" number="8.45">
<h2><span class="header-section-number">8.45</span> Bagging: Implementation<a href="tree-based-methods.html#bagging-implementation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="tree-based-methods.html#cb382-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain predictions on the test set</span></span>
<span id="cb382-2"><a href="tree-based-methods.html#cb382-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb382-3"><a href="tree-based-methods.html#cb382-3" aria-hidden="true" tabindex="-1"></a>final_model_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(bag_fit, <span class="at">newdata =</span> baked_test)     <span class="co"># use &#39;type = &quot;class&quot;&#39; for classification trees</span></span>
<span id="cb382-4"><a href="tree-based-methods.html#cb382-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb382-5"><a href="tree-based-methods.html#cb382-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((final_model_preds <span class="sc">-</span> baked_test<span class="sc">$</span>Sale_Price)<span class="sc">^</span><span class="dv">2</span>))   <span class="co"># test set RMSE</span></span></code></pre></div>
<pre><code>## [1] 43202.54</code></pre>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="tree-based-methods.html#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="co"># variable importance</span></span>
<span id="cb384-2"><a href="tree-based-methods.html#cb384-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb384-3"><a href="tree-based-methods.html#cb384-3" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">varImp</span>(bag_fit)      <span class="co"># look at the object created</span></span></code></pre></div>
</div>
<div id="bagging-disadvantages" class="section level2 hasAnchor" number="8.46">
<h2><span class="header-section-number">8.46</span> Bagging: Disadvantages<a href="tree-based-methods.html#bagging-disadvantages" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Bagging improves the prediction accuracy for high variance (and low bias) models at the expense of interpretability and computational speed.</p></li>
<li><p>However, although the model building steps are independent, the trees in bagging are not completely independent of each other since all the original features are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to any underlying strong relationships.</p></li>
<li><p>This characteristic is known as <strong>tree correlation</strong> and prevents bagging from further reducing the variance of the individual models. <strong>Random forests</strong> extend and improve upon bagged decision trees by reducing this correlation and thereby improving the accuracy of the overall ensemble.</p></li>
</ul>
</div>
<div id="bagging-disadvantages-1" class="section level2 hasAnchor" number="8.47">
<h2><span class="header-section-number">8.47</span> Bagging: Disadvantages<a href="tree-based-methods.html#bagging-disadvantages-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Tree Correlation in Bagging</strong></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-232"></span>
<img src="EFT/tcBagging.jpg" alt="Adapted from HMLR, Boehmke &amp; Greenwell" width="100%" />
<p class="caption">
Figure 8.6: Adapted from HMLR, Boehmke &amp; Greenwell
</p>
</div>
</div>
<div id="random-forests" class="section level2 hasAnchor" number="8.48">
<h2><span class="header-section-number">8.48</span> Random Forests<a href="tree-based-methods.html#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Provide an improvement over bagged trees by reducing the variance further (by <strong>decorrelating</strong>) when we average the trees.</p></li>
<li><p>As in bagging, we build a number of decision trees on bootstrapped training samples.</p></li>
<li><p>For each tree, each time a split is considered, a <strong>random selection of <span class="math inline">\(m\)</span> predictors</strong> is chosen (split candidates) from the full set of <span class="math inline">\(p\)</span> predictors. The split is allowed to use only one of those <span class="math inline">\(m\)</span> predictors. Note that in bagging, each split for each tree considers all <span class="math inline">\(p\)</span> predictors as split candidates.</p></li>
<li><p>A fresh sample of <span class="math inline">\(m\)</span> predictors is taken at each split. Typical default values are <span class="math inline">\(m = p/3\)</span> (regression) and <span class="math inline">\(m = \sqrt{p}\)</span> (classification) but this should be considered a tuning parameter, to be chosen by CV.</p></li>
</ul>
</div>
<div id="random-forests-implementation" class="section level2 smaller hasAnchor" number="8.49">
<h2><span class="header-section-number">8.49</span> Random Forests: Implementation<a href="tree-based-methods.html#random-forests-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<p>Data splitting and feature engineering has been done in the previous slides.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="tree-based-methods.html#cb385-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb385-2"><a href="tree-based-methods.html#cb385-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb385-3"><a href="tree-based-methods.html#cb385-3" aria-hidden="true" tabindex="-1"></a>cv_specs <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>)   <span class="co"># CV specifications</span></span>
<span id="cb385-4"><a href="tree-based-methods.html#cb385-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb385-5"><a href="tree-based-methods.html#cb385-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb385-6"><a href="tree-based-methods.html#cb385-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger)</span>
<span id="cb385-7"><a href="tree-based-methods.html#cb385-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb385-8"><a href="tree-based-methods.html#cb385-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb385-9"><a href="tree-based-methods.html#cb385-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb385-10"><a href="tree-based-methods.html#cb385-10" aria-hidden="true" tabindex="-1"></a>param_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">mtry =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">30</span>, <span class="dv">1</span>),     <span class="co"># sequence of 1 to at least half the number of predictors</span></span>
<span id="cb385-11"><a href="tree-based-methods.html#cb385-11" aria-hidden="true" tabindex="-1"></a>                          <span class="at">splitrule =</span> <span class="st">&quot;variance&quot;</span>,   <span class="co"># use &quot;gini&quot; for classification</span></span>
<span id="cb385-12"><a href="tree-based-methods.html#cb385-12" aria-hidden="true" tabindex="-1"></a>                          <span class="at">min.node.size =</span> <span class="dv">2</span>)        <span class="co"># for each tree</span></span>
<span id="cb385-13"><a href="tree-based-methods.html#cb385-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb385-14"><a href="tree-based-methods.html#cb385-14" aria-hidden="true" tabindex="-1"></a>rf_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb385-15"><a href="tree-based-methods.html#cb385-15" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> ames_train,</span>
<span id="cb385-16"><a href="tree-based-methods.html#cb385-16" aria-hidden="true" tabindex="-1"></a>               <span class="at">method =</span> <span class="st">&quot;ranger&quot;</span>,</span>
<span id="cb385-17"><a href="tree-based-methods.html#cb385-17" aria-hidden="true" tabindex="-1"></a>               <span class="at">trControl =</span> cv_specs,</span>
<span id="cb385-18"><a href="tree-based-methods.html#cb385-18" aria-hidden="true" tabindex="-1"></a>               <span class="at">tuneGrid =</span> param_grid,</span>
<span id="cb385-19"><a href="tree-based-methods.html#cb385-19" aria-hidden="true" tabindex="-1"></a>               <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</span>
<span id="cb385-20"><a href="tree-based-methods.html#cb385-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb385-21"><a href="tree-based-methods.html#cb385-21" aria-hidden="true" tabindex="-1"></a>rf_cv<span class="sc">$</span>bestTune<span class="sc">$</span>mtry   <span class="co"># optimal tuning parameter</span></span></code></pre></div>
<pre><code>## [1] 28</code></pre>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="tree-based-methods.html#cb387-1" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(rf_cv<span class="sc">$</span>results<span class="sc">$</span>RMSE)   <span class="co"># optimal CV RMSE</span></span></code></pre></div>
<pre><code>## [1] 30948.63</code></pre>
</div>
<div id="random-forests-implementation-1" class="section level2 hasAnchor" number="8.50">
<h2><span class="header-section-number">8.50</span> Random Forests: Implementation<a href="tree-based-methods.html#random-forests-implementation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="tree-based-methods.html#cb389-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit final model</span></span>
<span id="cb389-2"><a href="tree-based-methods.html#cb389-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb389-3"><a href="tree-based-methods.html#cb389-3" aria-hidden="true" tabindex="-1"></a>final_model <span class="ot">&lt;-</span> <span class="fu">ranger</span>(Sale_Price <span class="sc">~</span> .,</span>
<span id="cb389-4"><a href="tree-based-methods.html#cb389-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data =</span> baked_train,</span>
<span id="cb389-5"><a href="tree-based-methods.html#cb389-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">num.trees =</span> <span class="dv">500</span>,</span>
<span id="cb389-6"><a href="tree-based-methods.html#cb389-6" aria-hidden="true" tabindex="-1"></a>                      <span class="at">mtry =</span> rf_cv<span class="sc">$</span>bestTune<span class="sc">$</span>mtry,</span>
<span id="cb389-7"><a href="tree-based-methods.html#cb389-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">splitrule =</span> <span class="st">&quot;variance&quot;</span>,</span>
<span id="cb389-8"><a href="tree-based-methods.html#cb389-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">min.node.size =</span> <span class="dv">2</span>, </span>
<span id="cb389-9"><a href="tree-based-methods.html#cb389-9" aria-hidden="true" tabindex="-1"></a>                      <span class="at">importance =</span> <span class="st">&quot;impurity&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="tree-based-methods.html#cb390-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain predictions on the test set</span></span>
<span id="cb390-2"><a href="tree-based-methods.html#cb390-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb390-3"><a href="tree-based-methods.html#cb390-3" aria-hidden="true" tabindex="-1"></a>final_model_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model, <span class="at">data =</span> baked_test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)  <span class="co"># predictions on test set</span></span>
<span id="cb390-4"><a href="tree-based-methods.html#cb390-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb390-5"><a href="tree-based-methods.html#cb390-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((final_model_preds<span class="sc">$</span>predictions <span class="sc">-</span> baked_test<span class="sc">$</span>Sale_Price)<span class="sc">^</span><span class="dv">2</span>))  <span class="co"># test set RMSE</span></span></code></pre></div>
<pre><code>## [1] 36257.25</code></pre>
</div>
<div id="random-forests-implementation-2" class="section level2 smaller hasAnchor" number="8.51">
<h2><span class="header-section-number">8.51</span> Random Forests: Implementation<a href="tree-based-methods.html#random-forests-implementation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing Dataset</strong></p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="tree-based-methods.html#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="co"># variable importance</span></span>
<span id="cb392-2"><a href="tree-based-methods.html#cb392-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb392-3"><a href="tree-based-methods.html#cb392-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">sort</span>(final_model<span class="sc">$</span>variable.importance, <span class="at">decreasing =</span> <span class="cn">TRUE</span>), <span class="dv">10</span>)      <span class="co"># top 10 most important features</span></span></code></pre></div>
<pre><code>##  Overall_Qual  First_Flr_SF   Garage_Cars   Gr_Liv_Area    Year_Built 
##  1.794339e+12  6.989855e+11  6.325767e+11  4.106476e+11  3.467222e+11 
##   Garage_Area      Lot_Area Second_Flr_SF  Lot_Frontage Open_Porch_SF 
##  2.715779e+11  1.557609e+11  1.146867e+11  6.614335e+10  4.534856e+10</code></pre>
</div>
<div id="your-turn-39" class="section level2 smaller hasAnchor" number="8.52">
<h2><span class="header-section-number">8.52</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-39" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You will work with the <code>vowels.rds</code> data. The task is to predict <code>letter</code> (five vowels) using the rest of the variables in the data (predictors).</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="tree-based-methods.html#cb394-1" aria-hidden="true" tabindex="-1"></a>vowels <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">&quot;vowels.rds&quot;</span>)    <span class="co"># load dataset</span></span></code></pre></div>
<p>Compare the performance (in terms of <strong>Accuracy</strong>) of the following models. Choose the optimal hyperparameters using CV.</p>
<ul>
<li><p>A <span class="math inline">\(K\)</span>-NN classifier</p></li>
<li><p>A single classification tree (use <code>tuneLength = 20</code>)</p></li>
<li><p>A bagged model</p></li>
<li><p>A random forest model</p></li>
</ul>
<p><strong>Perform the following tasks.</strong></p>
<ul>
<li><p>Investigate the dataset and complete any necessary tasks.</p></li>
<li><p>Split the data into training and test sets (70-30).</p></li>
<li><p>Perform required data preprocessing and create the blueprint. If using <code>step_dummy()</code>, set <code>one_hot = FALSE</code>. Prepare the blueprint on the training data. Obtain the modified training and test datasets.</p></li>
<li><p>Implement 5-fold CV repeated 5 times for each of the models above.</p></li>
<li><p>Report the optimal CV (or, OOB) Accuracy of each model. Report the optimal hyperparameters for each model. Which model performs best in this situation?</p></li>
<li><p>Build the final model. Obtain class label predictions on the test set. Create the corresponding confusion matrix and report the test set accuracy.</p></li>
</ul>
</div>
<div id="your-turn-40" class="section level2 hasAnchor" number="8.53">
<h2><span class="header-section-number">8.53</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-40" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="tree-based-methods.html#cb395-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(vowels)   <span class="co"># all features are numerical</span></span></code></pre></div>
<pre><code>## Rows: 1,941
## Columns: 17
## $ letter &lt;fct&gt; I, O, E, E, E, A, E, O, U, I, E, I, U, A, O, E, U, I, A, A, E, …
## $ xbox   &lt;int&gt; 5, 3, 3, 3, 6, 3, 4, 4, 7, 2, 3, 3, 4, 2, 5, 1, 3, 2, 2, 8, 3, …
## $ ybox   &lt;int&gt; 12, 4, 4, 7, 9, 7, 8, 7, 11, 9, 8, 9, 7, 1, 10, 0, 3, 6, 8, 15,…
## $ width  &lt;int&gt; 3, 4, 3, 4, 4, 5, 5, 5, 8, 3, 3, 4, 4, 4, 4, 1, 3, 2, 4, 7, 3, …
## $ height &lt;int&gt; 7, 3, 6, 5, 4, 5, 6, 5, 9, 7, 6, 7, 5, 2, 5, 1, 1, 4, 6, 8, 6, …
## $ onpix  &lt;int&gt; 2, 2, 2, 4, 2, 3, 4, 3, 4, 2, 2, 3, 2, 1, 3, 1, 1, 1, 2, 4, 3, …
## $ xbar   &lt;int&gt; 10, 8, 3, 7, 7, 12, 7, 8, 3, 8, 3, 7, 7, 8, 6, 4, 5, 9, 12, 8, …
## $ ybar   &lt;int&gt; 5, 7, 8, 7, 7, 2, 7, 7, 9, 7, 7, 7, 5, 1, 6, 7, 8, 5, 2, 2, 6, …
## $ x2bar  &lt;int&gt; 5, 7, 6, 5, 4, 3, 4, 8, 6, 0, 6, 0, 13, 2, 6, 5, 5, 0, 4, 3, 5,…
## $ y2bar  &lt;int&gt; 4, 5, 10, 8, 7, 2, 8, 5, 7, 7, 10, 7, 5, 2, 2, 8, 7, 6, 3, 2, 9…
## $ xybar  &lt;int&gt; 13, 7, 7, 8, 10, 10, 11, 10, 11, 13, 7, 13, 7, 7, 9, 7, 10, 13,…
## $ x2ybar &lt;int&gt; 3, 6, 6, 8, 6, 2, 8, 6, 11, 6, 6, 6, 14, 2, 6, 6, 9, 5, 2, 5, 4…
## $ xy2bar &lt;int&gt; 9, 8, 15, 9, 10, 9, 9, 8, 10, 9, 14, 8, 7, 8, 9, 13, 8, 9, 10, …
## $ xedge  &lt;int&gt; 2, 2, 0, 3, 1, 2, 2, 3, 3, 0, 0, 0, 3, 2, 5, 0, 3, 0, 3, 5, 0, …
## $ xegvy  &lt;int&gt; 8, 8, 8, 9, 9, 6, 9, 8, 9, 8, 8, 8, 9, 5, 9, 8, 10, 8, 6, 4, 8,…
## $ yedge  &lt;int&gt; 4, 3, 7, 6, 7, 3, 5, 3, 2, 1, 7, 1, 0, 2, 4, 6, 2, 1, 3, 5, 6, …
## $ yegvx  &lt;int&gt; 10, 8, 8, 9, 9, 8, 7, 8, 6, 8, 8, 8, 8, 7, 8, 10, 6, 8, 9, 5, 9…</code></pre>
</div>
<div id="your-turn-41" class="section level2 smaller hasAnchor" number="8.54">
<h2><span class="header-section-number">8.54</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-41" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="tree-based-methods.html#cb397-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(vowels))  <span class="co"># no missing entries</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="tree-based-methods.html#cb399-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(vowels)   <span class="co"># summary of variables</span></span></code></pre></div>
<pre><code>##  letter       xbox             ybox            width            height     
##  A:395   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   :0.000  
##  E:384   1st Qu.: 2.000   1st Qu.: 5.000   1st Qu.: 3.000   1st Qu.:4.000  
##  I:378   Median : 3.000   Median : 7.000   Median : 5.000   Median :5.000  
##  O:377   Mean   : 3.594   Mean   : 6.968   Mean   : 4.561   Mean   :5.185  
##  U:407   3rd Qu.: 5.000   3rd Qu.: 9.000   3rd Qu.: 6.000   3rd Qu.:7.000  
##          Max.   :12.000   Max.   :15.000   Max.   :10.000   Max.   :9.000  
##      onpix             xbar             ybar            x2bar       
##  Min.   : 0.000   Min.   : 2.000   Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 2.000   1st Qu.: 7.000   1st Qu.: 6.000   1st Qu.: 2.000  
##  Median : 3.000   Median : 7.000   Median : 7.000   Median : 4.000  
##  Mean   : 3.044   Mean   : 7.171   Mean   : 6.366   Mean   : 4.648  
##  3rd Qu.: 4.000   3rd Qu.: 8.000   3rd Qu.: 8.000   3rd Qu.: 6.000  
##  Max.   :12.000   Max.   :14.000   Max.   :11.000   Max.   :15.000  
##      y2bar            xybar            x2ybar           xy2bar     
##  Min.   : 0.000   Min.   : 3.000   Min.   : 0.000   Min.   : 4.00  
##  1st Qu.: 4.000   1st Qu.: 7.000   1st Qu.: 5.000   1st Qu.: 8.00  
##  Median : 5.000   Median : 7.000   Median : 6.000   Median : 8.00  
##  Mean   : 5.277   Mean   : 8.258   Mean   : 6.013   Mean   : 8.61  
##  3rd Qu.: 7.000   3rd Qu.:10.000   3rd Qu.: 7.000   3rd Qu.: 9.00  
##  Max.   :12.000   Max.   :14.000   Max.   :15.000   Max.   :15.00  
##      xedge            xegvy            yedge            yegvx       
##  Min.   : 0.000   Min.   : 1.000   Min.   : 0.000   Min.   : 1.000  
##  1st Qu.: 2.000   1st Qu.: 8.000   1st Qu.: 2.000   1st Qu.: 7.000  
##  Median : 3.000   Median : 8.000   Median : 3.000   Median : 8.000  
##  Mean   : 2.527   Mean   : 7.931   Mean   : 3.245   Mean   : 7.734  
##  3rd Qu.: 3.000   3rd Qu.: 9.000   3rd Qu.: 5.000   3rd Qu.: 8.000  
##  Max.   :11.000   Max.   :13.000   Max.   :11.000   Max.   :13.000</code></pre>
</div>
<div id="your-turn-42" class="section level2 smaller hasAnchor" number="8.55">
<h2><span class="header-section-number">8.55</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-42" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="tree-based-methods.html#cb401-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb401-2"><a href="tree-based-methods.html#cb401-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-3"><a href="tree-based-methods.html#cb401-3" aria-hidden="true" tabindex="-1"></a><span class="co"># split the data into training and test sets</span></span>
<span id="cb401-4"><a href="tree-based-methods.html#cb401-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-5"><a href="tree-based-methods.html#cb401-5" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(vowels<span class="sc">$</span>letter, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb401-6"><a href="tree-based-methods.html#cb401-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-7"><a href="tree-based-methods.html#cb401-7" aria-hidden="true" tabindex="-1"></a>vowels_train <span class="ot">&lt;-</span> vowels[index, ]</span>
<span id="cb401-8"><a href="tree-based-methods.html#cb401-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-9"><a href="tree-based-methods.html#cb401-9" aria-hidden="true" tabindex="-1"></a>vowels_test <span class="ot">&lt;-</span> vowels[<span class="sc">-</span>index, ]</span></code></pre></div>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="tree-based-methods.html#cb402-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nearZeroVar</span>(vowels_train, <span class="at">saveMetrics =</span> <span class="cn">TRUE</span>)  <span class="co"># no zv/nzv features</span></span></code></pre></div>
<pre><code>##        freqRatio percentUnique zeroVar   nzv
## letter  1.028881     0.3676471   FALSE FALSE
## xbox    1.171756     0.8823529   FALSE FALSE
## ybox    1.116959     1.1764706   FALSE FALSE
## width   1.042254     0.8088235   FALSE FALSE
## height  1.121849     0.7352941   FALSE FALSE
## onpix   1.473251     0.9558824   FALSE FALSE
## xbar    1.798680     0.9558824   FALSE FALSE
## ybar    1.726950     0.8823529   FALSE FALSE
## x2bar   1.010695     1.1764706   FALSE FALSE
## y2bar   1.012712     0.9558824   FALSE FALSE
## xybar   3.465839     0.8823529   FALSE FALSE
## x2ybar  2.462766     1.1764706   FALSE FALSE
## xy2bar  1.745098     0.8823529   FALSE FALSE
## xedge   1.550162     0.8823529   FALSE FALSE
## xegvy   2.433213     0.9558824   FALSE FALSE
## yedge   1.189189     0.8823529   FALSE FALSE
## yegvx   2.688976     0.8823529   FALSE FALSE</code></pre>
</div>
<div id="your-turn-43" class="section level2 hasAnchor" number="8.56">
<h2><span class="header-section-number">8.56</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-43" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="tree-based-methods.html#cb404-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb404-2"><a href="tree-based-methods.html#cb404-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-3"><a href="tree-based-methods.html#cb404-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create recipe and blueprint, prepare and apply blueprint</span></span>
<span id="cb404-4"><a href="tree-based-methods.html#cb404-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-5"><a href="tree-based-methods.html#cb404-5" aria-hidden="true" tabindex="-1"></a>blueprint <span class="ot">&lt;-</span> <span class="fu">recipe</span>(letter <span class="sc">~</span> ., <span class="at">data =</span> vowels_train) <span class="sc">%&gt;%</span></span>
<span id="cb404-6"><a href="tree-based-methods.html#cb404-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())</span>
<span id="cb404-7"><a href="tree-based-methods.html#cb404-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-8"><a href="tree-based-methods.html#cb404-8" aria-hidden="true" tabindex="-1"></a>prepare <span class="ot">&lt;-</span> <span class="fu">prep</span>(blueprint, <span class="at">training =</span> vowels_train)</span>
<span id="cb404-9"><a href="tree-based-methods.html#cb404-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-10"><a href="tree-based-methods.html#cb404-10" aria-hidden="true" tabindex="-1"></a>baked_train <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepare, <span class="at">new_data =</span> vowels_train)</span>
<span id="cb404-11"><a href="tree-based-methods.html#cb404-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-12"><a href="tree-based-methods.html#cb404-12" aria-hidden="true" tabindex="-1"></a>baked_test <span class="ot">&lt;-</span> <span class="fu">bake</span>(prepare, <span class="at">new_data =</span> vowels_test)</span></code></pre></div>
</div>
<div id="your-turn-44" class="section level2 hasAnchor" number="8.57">
<h2><span class="header-section-number">8.57</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-44" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="tree-based-methods.html#cb405-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb405-2"><a href="tree-based-methods.html#cb405-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb405-3"><a href="tree-based-methods.html#cb405-3" aria-hidden="true" tabindex="-1"></a>cv_specs <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">5</span>)   <span class="co"># CV specifications</span></span></code></pre></div>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="tree-based-methods.html#cb406-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb406-2"><a href="tree-based-methods.html#cb406-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb406-3"><a href="tree-based-methods.html#cb406-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CV for KNN</span></span>
<span id="cb406-4"><a href="tree-based-methods.html#cb406-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb406-5"><a href="tree-based-methods.html#cb406-5" aria-hidden="true" tabindex="-1"></a>k_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">101</span>, <span class="at">by =</span> <span class="dv">10</span>))</span>
<span id="cb406-6"><a href="tree-based-methods.html#cb406-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb406-7"><a href="tree-based-methods.html#cb406-7" aria-hidden="true" tabindex="-1"></a>knn_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb406-8"><a href="tree-based-methods.html#cb406-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> vowels_train, </span>
<span id="cb406-9"><a href="tree-based-methods.html#cb406-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb406-10"><a href="tree-based-methods.html#cb406-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> cv_specs,</span>
<span id="cb406-11"><a href="tree-based-methods.html#cb406-11" aria-hidden="true" tabindex="-1"></a>                <span class="at">tuneGrid =</span> k_grid,</span>
<span id="cb406-12"><a href="tree-based-methods.html#cb406-12" aria-hidden="true" tabindex="-1"></a>                <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="tree-based-methods.html#cb407-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb407-2"><a href="tree-based-methods.html#cb407-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb407-3"><a href="tree-based-methods.html#cb407-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CV with tree</span></span>
<span id="cb407-4"><a href="tree-based-methods.html#cb407-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb407-5"><a href="tree-based-methods.html#cb407-5" aria-hidden="true" tabindex="-1"></a>tree_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb407-6"><a href="tree-based-methods.html#cb407-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data =</span> vowels_train,</span>
<span id="cb407-7"><a href="tree-based-methods.html#cb407-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb407-8"><a href="tree-based-methods.html#cb407-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trControl =</span> cv_specs,</span>
<span id="cb407-9"><a href="tree-based-methods.html#cb407-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">tuneLength =</span> <span class="dv">20</span>,</span>
<span id="cb407-10"><a href="tree-based-methods.html#cb407-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)</span></code></pre></div>
</div>
<div id="your-turn-45" class="section level2 hasAnchor" number="8.58">
<h2><span class="header-section-number">8.58</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-45" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="tree-based-methods.html#cb408-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb408-2"><a href="tree-based-methods.html#cb408-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb408-3"><a href="tree-based-methods.html#cb408-3" aria-hidden="true" tabindex="-1"></a><span class="co"># bagging</span></span>
<span id="cb408-4"><a href="tree-based-methods.html#cb408-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb408-5"><a href="tree-based-methods.html#cb408-5" aria-hidden="true" tabindex="-1"></a>bag_fit <span class="ot">&lt;-</span> <span class="fu">bagging</span>(letter <span class="sc">~</span> ., </span>
<span id="cb408-6"><a href="tree-based-methods.html#cb408-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> baked_train,</span>
<span id="cb408-7"><a href="tree-based-methods.html#cb408-7" aria-hidden="true" tabindex="-1"></a>                   <span class="at">nbagg =</span> <span class="dv">500</span>,   </span>
<span id="cb408-8"><a href="tree-based-methods.html#cb408-8" aria-hidden="true" tabindex="-1"></a>                   <span class="at">coob =</span> <span class="cn">TRUE</span>,   </span>
<span id="cb408-9"><a href="tree-based-methods.html#cb408-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">2</span>,</span>
<span id="cb408-10"><a href="tree-based-methods.html#cb408-10" aria-hidden="true" tabindex="-1"></a>                                           <span class="at">cp =</span> <span class="dv">0</span>,    </span>
<span id="cb408-11"><a href="tree-based-methods.html#cb408-11" aria-hidden="true" tabindex="-1"></a>                                           <span class="at">xval =</span> <span class="dv">0</span>)) </span></code></pre></div>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="tree-based-methods.html#cb409-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">022123</span>)   <span class="co"># set seed</span></span>
<span id="cb409-2"><a href="tree-based-methods.html#cb409-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-3"><a href="tree-based-methods.html#cb409-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CV with random forests</span></span>
<span id="cb409-4"><a href="tree-based-methods.html#cb409-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-5"><a href="tree-based-methods.html#cb409-5" aria-hidden="true" tabindex="-1"></a>param_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">mtry =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">16</span>, <span class="dv">1</span>),    <span class="co"># 16 features in baked_train</span></span>
<span id="cb409-6"><a href="tree-based-methods.html#cb409-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">splitrule =</span> <span class="st">&quot;gini&quot;</span>,   </span>
<span id="cb409-7"><a href="tree-based-methods.html#cb409-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">min.node.size =</span> <span class="dv">2</span>)        </span>
<span id="cb409-8"><a href="tree-based-methods.html#cb409-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb409-9"><a href="tree-based-methods.html#cb409-9" aria-hidden="true" tabindex="-1"></a>rf_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(blueprint,</span>
<span id="cb409-10"><a href="tree-based-methods.html#cb409-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> vowels_train,</span>
<span id="cb409-11"><a href="tree-based-methods.html#cb409-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">method =</span> <span class="st">&quot;ranger&quot;</span>,</span>
<span id="cb409-12"><a href="tree-based-methods.html#cb409-12" aria-hidden="true" tabindex="-1"></a>               <span class="at">trControl =</span> cv_specs,</span>
<span id="cb409-13"><a href="tree-based-methods.html#cb409-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">tuneGrid =</span> param_grid,</span>
<span id="cb409-14"><a href="tree-based-methods.html#cb409-14" aria-hidden="true" tabindex="-1"></a>               <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)</span></code></pre></div>
</div>
<div id="your-turn-46" class="section level2 smaller hasAnchor" number="8.59">
<h2><span class="header-section-number">8.59</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-46" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="tree-based-methods.html#cb410-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal CV Accuracies</span></span>
<span id="cb410-2"><a href="tree-based-methods.html#cb410-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb410-3"><a href="tree-based-methods.html#cb410-3" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(knn_cv<span class="sc">$</span>results<span class="sc">$</span>Accuracy)     <span class="co"># for KNN</span></span></code></pre></div>
<pre><code>## [1] 0.9866132</code></pre>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="tree-based-methods.html#cb412-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(tree_cv<span class="sc">$</span>results<span class="sc">$</span>Accuracy)     <span class="co"># for classification tree</span></span></code></pre></div>
<pre><code>## [1] 0.9194068</code></pre>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="tree-based-methods.html#cb414-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>bag_fit<span class="sc">$</span>err     <span class="co"># for bagging</span></span></code></pre></div>
<pre><code>## [1] 0.9772059</code></pre>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="tree-based-methods.html#cb416-1" aria-hidden="true" tabindex="-1"></a><span class="fu">max</span>(rf_cv<span class="sc">$</span>results<span class="sc">$</span>Accuracy)     <span class="co"># for random forests</span></span></code></pre></div>
<pre><code>## [1] 0.9910256</code></pre>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="tree-based-methods.html#cb418-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal hyperparameters</span></span>
<span id="cb418-2"><a href="tree-based-methods.html#cb418-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb418-3"><a href="tree-based-methods.html#cb418-3" aria-hidden="true" tabindex="-1"></a>knn_cv<span class="sc">$</span>bestTune<span class="sc">$</span>k    <span class="co"># for KNN</span></span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="tree-based-methods.html#cb420-1" aria-hidden="true" tabindex="-1"></a>tree_cv<span class="sc">$</span>bestTune<span class="sc">$</span>cp    <span class="co"># for classification tree</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="tree-based-methods.html#cb422-1" aria-hidden="true" tabindex="-1"></a>rf_cv<span class="sc">$</span>bestTune<span class="sc">$</span>mtry    <span class="co"># for random forests</span></span></code></pre></div>
<pre><code>## [1] 8</code></pre>
</div>
<div id="your-turn-47" class="section level2 hasAnchor" number="8.60">
<h2><span class="header-section-number">8.60</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-47" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="tree-based-methods.html#cb424-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build final model</span></span>
<span id="cb424-2"><a href="tree-based-methods.html#cb424-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb424-3"><a href="tree-based-methods.html#cb424-3" aria-hidden="true" tabindex="-1"></a>final_model <span class="ot">&lt;-</span> <span class="fu">ranger</span>(letter <span class="sc">~</span> .,</span>
<span id="cb424-4"><a href="tree-based-methods.html#cb424-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data =</span> baked_train,</span>
<span id="cb424-5"><a href="tree-based-methods.html#cb424-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">num.trees =</span> <span class="dv">500</span>,</span>
<span id="cb424-6"><a href="tree-based-methods.html#cb424-6" aria-hidden="true" tabindex="-1"></a>                      <span class="at">mtry =</span> rf_cv<span class="sc">$</span>bestTune<span class="sc">$</span>mtry,</span>
<span id="cb424-7"><a href="tree-based-methods.html#cb424-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">splitrule =</span> <span class="st">&quot;gini&quot;</span>,</span>
<span id="cb424-8"><a href="tree-based-methods.html#cb424-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">min.node.size =</span> <span class="dv">2</span>, </span>
<span id="cb424-9"><a href="tree-based-methods.html#cb424-9" aria-hidden="true" tabindex="-1"></a>                      <span class="at">importance =</span> <span class="st">&quot;impurity&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="tree-based-methods.html#cb425-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain predictions on test data</span></span>
<span id="cb425-2"><a href="tree-based-methods.html#cb425-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb425-3"><a href="tree-based-methods.html#cb425-3" aria-hidden="true" tabindex="-1"></a>final_model_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model, <span class="at">data =</span> baked_test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)  <span class="co"># predictions on test set</span></span></code></pre></div>
</div>
<div id="your-turn-48" class="section level2 smaller hasAnchor" number="8.61">
<h2><span class="header-section-number">8.61</span> <span style="color:blue">Your Turn!!!</span><a href="tree-based-methods.html#your-turn-48" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="tree-based-methods.html#cb426-1" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix</span></span>
<span id="cb426-2"><a href="tree-based-methods.html#cb426-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb426-3"><a href="tree-based-methods.html#cb426-3" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> final_model_preds<span class="sc">$</span>predictions, <span class="at">reference =</span> baked_test<span class="sc">$</span>letter)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   E   I   O   U
##          A 118   0   1   0   1
##          E   0 115   4   1   0
##          I   0   0 108   0   0
##          O   0   0   0 110   0
##          U   0   0   0   2 121
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9845          
##                  95% CI : (0.9708, 0.9929)
##     No Information Rate : 0.21            
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9806          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: E Class: I Class: O Class: U
## Sensitivity            1.0000   1.0000   0.9558   0.9735   0.9918
## Specificity            0.9957   0.9893   1.0000   1.0000   0.9956
## Pos Pred Value         0.9833   0.9583   1.0000   1.0000   0.9837
## Neg Pred Value         1.0000   1.0000   0.9894   0.9936   0.9978
## Prevalence             0.2031   0.1979   0.1945   0.1945   0.2100
## Detection Rate         0.2031   0.1979   0.1859   0.1893   0.2083
## Detection Prevalence   0.2065   0.2065   0.1859   0.1893   0.2117
## Balanced Accuracy      0.9978   0.9946   0.9779   0.9867   0.9937</code></pre>
<!-- * Step 1: Investigate the dataset -->
<!-- * Step 2: Split the data into training and test sets (a 70-30) -->
<!-- * Step 3: Perform required data preprocessing and create the blueprint.  -->
<!-- * Step 4: Implement 5-fold CV repeated 5 times to compare the performance of the following models. Use `metric = "Accuracy"`. -->
<!--     - a single classification tree  -->
<!--     - a bagged model -->
<!--     - a random forests model -->
<!-- How do the models compare in terms of the `Accuracy` obtained from CV/OOB? -->
<!-- Typically $m \approx \sqrt{p}$. -->
<!-- ## Bagging and Random Forests -->
<!-- **Heart dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.8.png") -->
<!-- ``` -->
<!-- ## Bagging and Random Forests -->
<!-- **Cancer Gene Expression dataset** -->
<!-- * High-dimensional dataset with $p=4718$ genes and $n=349$ patients. -->
<!-- * Classification problem with $K=15$. -->
<!-- * Consider 500 genes that have the largest variance in the training set. -->
<!-- * Randomly divide available data into training and test set. Random forests applied to training set with different values of $m$. -->
<!-- ## Bagging and Random Forests -->
<!-- **Cancer Gene Expression dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.10.png") -->
<!-- ``` -->
<!-- ## Boosting -->
<!-- * Boosting is a general approach that can be applied to many statistical learning methods for regression or classification. -->
<!-- * In boosting trees are grown **sequentially**: each tree is grown using information from previously grown trees. -->
<!-- * Boosting <span style="color:red">**does not**</span> involve bootstrap sampling, instead each tree is fit on a modified version of the original dataset. -->
<!-- ## Boosting -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '86%'} -->
<!-- knitr::include_graphics("EFT/algo8.2.png") -->
<!-- ``` -->
<!-- ## Boosting -->
<!-- * Growing a single large tree **fits the data hard** and tends to **overfit**. Boosting instead **learns slowly**. -->
<!-- * Fit a decision tree to the residuals from the current model (tree). Add this new decision tree into the fitted function in order to update the residuals. -->
<!-- * Each tree can be rather small, with just a few -->
<!-- terminal nodes, determined by the parameter $d$ in the -->
<!-- algorithm. -->
<!-- * Fitting small trees to the residuals slowly improves $\hat{f}$ in areas where it does not perform well. The shrinkage parameter $\lambda$ slows the process down even further, allowing more and different shaped trees to attack the residuals. -->
<!-- * Construction of each tree depends strongly on the trees that have already been grown. -->
<!-- ## Boosting -->
<!-- Boosting has three **tuning parameters**. -->
<!-- * Number of trees $B$: Can overfit if $B$ is too large. Use CV. -->
<!-- * Shrinkage parameter $\lambda$: Small positive number, controls the pace of the boosting process. Typically, $\lambda=0.01$ or $\lambda=0.001$. Small $\lambda$ requires a large $B$. -->
<!-- * Depth $d$: Number of splits in each tree. Controls complexity of the ensemble. $d=1$ refers to a single split (**stump**). More generally $d$ is known as the **interaction depth**, and controls the interaction order of the boosted model, since $d$ splits can involve at most $d$ variables. -->
<!-- ## Boosting -->
<!-- **Cancer Gene Expression dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.11.png") -->
<!-- ``` -->
<!-- ## Chapter 8 Summary -->
<!-- * Decision trees are simple and interpretable models for -->
<!-- regression and classification. -->
<!-- * However they are often not competitive with other -->
<!-- methods in terms of prediction accuracy. -->
<!-- * Bagging, random forests and boosting are good methods -->
<!-- for improving the prediction accuracy of trees. They work -->
<!-- by growing many trees on the training data and then -->
<!-- combining the predictions of the resulting ensemble of trees. -->
<!-- * Random forests and Boosting are among the state-of-the-art methods for supervised learning. However their results can be difficult to interpret. -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-model-selection-and-regularization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="support-vector-machines-svm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/abhicc/208notes/edit/master/07-Slides8.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/abhicc/208notes/blob/master/07-Slides8.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
