<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 K-Nearest Neighbors Classifier | 208 Course Notes</title>
  <meta name="description" content="Chapter 5 K-Nearest Neighbors Classifier | 208 Course Notes" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 K-Nearest Neighbors Classifier | 208 Course Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 K-Nearest Neighbors Classifier | 208 Course Notes" />
  
  
  

<meta name="author" content="Abhishek Chakraborty" />


<meta name="date" content="2023-03-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-linear-regression-mlr.html"/>
<link rel="next" href="review-of-cv.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">208 Course Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2</b> What is Machine Learning?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#what-is-machine-learning-1"><i class="fa fa-check"></i><b>2.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question"><i class="fa fa-check"></i><b>2.2</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#statistical-learning-vs-machine-learning-vs-data-science"><i class="fa fa-check"></i><b>2.3</b> Statistical Learning vs Machine Learning vs Data Science</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#notations"><i class="fa fa-check"></i><b>2.4</b> Notations</a></li>
<li class="chapter" data-level="2.5" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#notations-1"><i class="fa fa-check"></i><b>2.5</b> Notations</a></li>
<li class="chapter" data-level="2.6" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question-1"><i class="fa fa-check"></i><b>2.6</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.7" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question-2"><i class="fa fa-check"></i><b>2.7</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.8" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-vs-unsupervised"><i class="fa fa-check"></i><b>2.8</b> Supervised vs Unsupervised</a></li>
<li class="chapter" data-level="2.9" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>2.9</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.10" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-1"><i class="fa fa-check"></i><b>2.10</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.11" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.11</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="2.12" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question-3"><i class="fa fa-check"></i><b>2.12</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.13" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-2"><i class="fa fa-check"></i><b>2.13</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.14" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-3"><i class="fa fa-check"></i><b>2.14</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.15" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-4"><i class="fa fa-check"></i><b>2.15</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.16" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-5"><i class="fa fa-check"></i><b>2.16</b> Supervised Learning</a></li>
<li class="chapter" data-level="2.17" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-why-estimate-fmathbfx"><i class="fa fa-check"></i><b>2.17</b> Supervised Learning: Why Estimate <span class="math inline">\(f(\mathbf{X})\)</span>?</a></li>
<li class="chapter" data-level="2.18" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-prediction-and-inference"><i class="fa fa-check"></i><b>2.18</b> Supervised Learning: Prediction and Inference</a></li>
<li class="chapter" data-level="2.19" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-prediction-and-inference-1"><i class="fa fa-check"></i><b>2.19</b> Supervised Learning: Prediction and Inference</a></li>
<li class="chapter" data-level="2.20" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-prediction"><i class="fa fa-check"></i><b>2.20</b> Supervised Learning: Prediction</a></li>
<li class="chapter" data-level="2.21" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#question-4"><i class="fa fa-check"></i><b>2.21</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="2.22" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-how-do-we-estimate-fmathbfx"><i class="fa fa-check"></i><b>2.22</b> Supervised Learning: How Do We Estimate <span class="math inline">\(f(\mathbf{X})\)</span>?</a></li>
<li class="chapter" data-level="2.23" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-parametric-methods"><i class="fa fa-check"></i><b>2.23</b> Supervised Learning: Parametric Methods</a></li>
<li class="chapter" data-level="2.24" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-parametric-methods-1"><i class="fa fa-check"></i><b>2.24</b> Supervised Learning: Parametric Methods</a></li>
<li class="chapter" data-level="2.25" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-non-parametric-methods"><i class="fa fa-check"></i><b>2.25</b> Supervised Learning: Non-parametric Methods</a></li>
<li class="chapter" data-level="2.26" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-flexibility-of-models"><i class="fa fa-check"></i><b>2.26</b> Supervised Learning: Flexibility of Models</a></li>
<li class="chapter" data-level="2.27" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-some-trade-offs"><i class="fa fa-check"></i><b>2.27</b> Supervised Learning: Some Trade-offs</a></li>
<li class="chapter" data-level="2.28" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning-some-trade-offs-1"><i class="fa fa-check"></i><b>2.28</b> Supervised Learning: Some Trade-offs</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html"><i class="fa fa-check"></i><b>3</b> Supervised Learning: Assessing Model Accuracy</a>
<ul>
<li class="chapter" data-level="3.1" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-1"><i class="fa fa-check"></i><b>3.1</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.2" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-2"><i class="fa fa-check"></i><b>3.2</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.3" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-3"><i class="fa fa-check"></i><b>3.3</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.4" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-4"><i class="fa fa-check"></i><b>3.4</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.5" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-5"><i class="fa fa-check"></i><b>3.5</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.6" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-assessing-model-accuracy-6"><i class="fa fa-check"></i><b>3.6</b> Supervised Learning: Assessing Model Accuracy</a></li>
<li class="chapter" data-level="3.7" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-bias-variance-trade-off"><i class="fa fa-check"></i><b>3.7</b> Supervised Learning: Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="3.8" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-bias-variance-trade-off-1"><i class="fa fa-check"></i><b>3.8</b> Supervised Learning: Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="3.9" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#supervised-learning-bias-variance-trade-off-2"><i class="fa fa-check"></i><b>3.9</b> Supervised Learning: Bias-Variance Trade-off</a></li>
<li class="chapter" data-level="3.10" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-5"><i class="fa fa-check"></i><b>3.10</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.11" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#simple-linear-regression-slr"><i class="fa fa-check"></i><b>3.11</b> Simple Linear Regression (SLR)</a></li>
<li class="chapter" data-level="3.12" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-6"><i class="fa fa-check"></i><b>3.12</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.13" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-estimating-parameters"><i class="fa fa-check"></i><b>3.13</b> SLR: Estimating Parameters</a></li>
<li class="chapter" data-level="3.14" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-estimating-parameters-1"><i class="fa fa-check"></i><b>3.14</b> SLR: Estimating Parameters</a></li>
<li class="chapter" data-level="3.15" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-estimating-parameters-2"><i class="fa fa-check"></i><b>3.15</b> SLR: Estimating Parameters</a></li>
<li class="chapter" data-level="3.16" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#ames-housing-dataset"><i class="fa fa-check"></i><b>3.16</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="3.17" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#ames-housing-dataset-1"><i class="fa fa-check"></i><b>3.17</b> Ames Housing dataset</a></li>
<li class="chapter" data-level="3.18" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-estimating-parameters-3"><i class="fa fa-check"></i><b>3.18</b> SLR: Estimating Parameters</a></li>
<li class="chapter" data-level="3.19" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-model"><i class="fa fa-check"></i><b>3.19</b> SLR: Model</a></li>
<li class="chapter" data-level="3.20" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-model-1"><i class="fa fa-check"></i><b>3.20</b> SLR: Model</a></li>
<li class="chapter" data-level="3.21" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-prediction"><i class="fa fa-check"></i><b>3.21</b> SLR: Prediction</a></li>
<li class="chapter" data-level="3.22" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-interpreting-parameters"><i class="fa fa-check"></i><b>3.22</b> SLR: Interpreting Parameters</a></li>
<li class="chapter" data-level="3.23" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-assessing-accuracy-of-model"><i class="fa fa-check"></i><b>3.23</b> SLR: Assessing Accuracy of Model</a></li>
<li class="chapter" data-level="3.24" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#slr-assessing-accuracy-of-model-1"><i class="fa fa-check"></i><b>3.24</b> SLR: Assessing Accuracy of Model</a></li>
<li class="chapter" data-level="3.25" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#your-turn"><i class="fa fa-check"></i><b>3.25</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="3.26" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-7"><i class="fa fa-check"></i><b>3.26</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.27" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-8"><i class="fa fa-check"></i><b>3.27</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.28" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-9"><i class="fa fa-check"></i><b>3.28</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="3.29" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#regression-conditional-averaging"><i class="fa fa-check"></i><b>3.29</b> Regression: Conditional Averaging</a></li>
<li class="chapter" data-level="3.30" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#regression-conditional-averaging-1"><i class="fa fa-check"></i><b>3.30</b> Regression: Conditional Averaging</a></li>
<li class="chapter" data-level="3.31" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#k-nearest-neighbors-regression"><i class="fa fa-check"></i><b>3.31</b> K-Nearest Neighbors Regression</a></li>
<li class="chapter" data-level="3.32" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#k-nearest-neighbors-regression-fit"><i class="fa fa-check"></i><b>3.32</b> K-Nearest Neighbors Regression: Fit</a></li>
<li class="chapter" data-level="3.33" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#k-nearest-neighbors-regression-prediction"><i class="fa fa-check"></i><b>3.33</b> K-Nearest Neighbors Regression: Prediction</a></li>
<li class="chapter" data-level="3.34" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#regression-methods-comparison"><i class="fa fa-check"></i><b>3.34</b> Regression Methods: Comparison</a></li>
<li class="chapter" data-level="3.35" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#your-turn-1"><i class="fa fa-check"></i><b>3.35</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="3.36" data-path="supervised-learning-assessing-model-accuracy.html"><a href="supervised-learning-assessing-model-accuracy.html#question-10"><i class="fa fa-check"></i><b>3.36</b> <span style="color:blue">Question!!!</span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression (MLR)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-estimating-parameters"><i class="fa fa-check"></i><b>4.1</b> MLR: Estimating Parameters</a></li>
<li class="chapter" data-level="4.2" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-estimating-parameters-1"><i class="fa fa-check"></i><b>4.2</b> MLR: Estimating Parameters</a></li>
<li class="chapter" data-level="4.3" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-estimating-parameters-2"><i class="fa fa-check"></i><b>4.3</b> MLR: Estimating Parameters</a></li>
<li class="chapter" data-level="4.4" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-interpreting-parameters"><i class="fa fa-check"></i><b>4.4</b> MLR: Interpreting Parameters</a></li>
<li class="chapter" data-level="4.5" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-prediction"><i class="fa fa-check"></i><b>4.5</b> MLR: Prediction</a></li>
<li class="chapter" data-level="4.6" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-assessing-accuracy-of-model"><i class="fa fa-check"></i><b>4.6</b> MLR: Assessing Accuracy of Model</a></li>
<li class="chapter" data-level="4.7" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#your-turn-2"><i class="fa fa-check"></i><b>4.7</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="4.8" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#mlr-assessing-accuracy-of-model-1"><i class="fa fa-check"></i><b>4.8</b> MLR: Assessing Accuracy of Model</a></li>
<li class="chapter" data-level="4.9" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#question-11"><i class="fa fa-check"></i><b>4.9</b> <span style="color:blue">Question!!!</span></a></li>
<li class="chapter" data-level="4.10" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#k-nearest-neighbors-regression-multiple-predictors"><i class="fa fa-check"></i><b>4.10</b> K-Nearest Neighbors Regression (multiple predictors)</a></li>
<li class="chapter" data-level="4.11" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#k-nearest-neighbors-regression-multiple-predictors-1"><i class="fa fa-check"></i><b>4.11</b> K-Nearest Neighbors Regression (multiple predictors)</a></li>
<li class="chapter" data-level="4.12" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#linear-regression-vs-k-nearest-neighbors"><i class="fa fa-check"></i><b>4.12</b> Linear Regression vs K-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.13" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#classification-problems"><i class="fa fa-check"></i><b>4.13</b> Classification Problems</a></li>
<li class="chapter" data-level="4.14" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#classification-problems-example"><i class="fa fa-check"></i><b>4.14</b> Classification Problems: Example</a></li>
<li class="chapter" data-level="4.15" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#classification-problems-example-1"><i class="fa fa-check"></i><b>4.15</b> Classification Problems: Example</a></li>
<li class="chapter" data-level="4.16" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#why-not-linear-regression"><i class="fa fa-check"></i><b>4.16</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.17" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#why-not-linear-regression-1"><i class="fa fa-check"></i><b>4.17</b> Why Not Linear Regression?</a></li>
<li class="chapter" data-level="4.18" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression"><i class="fa fa-check"></i><b>4.18</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.19" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-1"><i class="fa fa-check"></i><b>4.19</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.20" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#your-turn-3"><i class="fa fa-check"></i><b>4.20</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="4.21" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-example"><i class="fa fa-check"></i><b>4.21</b> Logistic Regression: Example</a></li>
<li class="chapter" data-level="4.22" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-estimating-parameters"><i class="fa fa-check"></i><b>4.22</b> Logistic Regression: Estimating Parameters</a></li>
<li class="chapter" data-level="4.23" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-individual-predictions"><i class="fa fa-check"></i><b>4.23</b> Logistic Regression: Individual Predictions</a></li>
<li class="chapter" data-level="4.24" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-test-set-predictions"><i class="fa fa-check"></i><b>4.24</b> Logistic Regression: Test Set Predictions</a></li>
<li class="chapter" data-level="4.25" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-test-set-predictions-1"><i class="fa fa-check"></i><b>4.25</b> Logistic Regression: Test Set Predictions</a></li>
<li class="chapter" data-level="4.26" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#logistic-regression-performance"><i class="fa fa-check"></i><b>4.26</b> Logistic Regression: Performance</a></li>
<li class="chapter" data-level="4.27" data-path="multiple-linear-regression-mlr.html"><a href="multiple-linear-regression-mlr.html#confusion-matrix-terms"><i class="fa fa-check"></i><b>4.27</b> Confusion Matrix Terms</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html"><i class="fa fa-check"></i><b>5</b> K-Nearest Neighbors Classifier</a>
<ul>
<li class="chapter" data-level="5.1" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-4"><i class="fa fa-check"></i><b>5.1</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="5.2" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-split-data"><i class="fa fa-check"></i><b>5.2</b> K-Nearest Neighbors Classifier: Split Data</a></li>
<li class="chapter" data-level="5.3" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-build-model"><i class="fa fa-check"></i><b>5.3</b> K-Nearest Neighbors Classifier: Build Model</a></li>
<li class="chapter" data-level="5.4" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-predictions"><i class="fa fa-check"></i><b>5.4</b> K-Nearest Neighbors Classifier: Predictions</a></li>
<li class="chapter" data-level="5.5" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-performance"><i class="fa fa-check"></i><b>5.5</b> K-Nearest Neighbors Classifier: Performance</a></li>
<li class="chapter" data-level="5.6" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#roc-curve-and-auc"><i class="fa fa-check"></i><b>5.6</b> ROC Curve and AUC</a></li>
<li class="chapter" data-level="5.7" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#roc-curve-and-auc-1"><i class="fa fa-check"></i><b>5.7</b> ROC Curve and AUC</a></li>
<li class="chapter" data-level="5.8" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#data-splitting"><i class="fa fa-check"></i><b>5.8</b> Data Splitting</a></li>
<li class="chapter" data-level="5.9" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#resampling-methods"><i class="fa fa-check"></i><b>5.9</b> Resampling Methods</a></li>
<li class="chapter" data-level="5.10" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#cross-validation-cv"><i class="fa fa-check"></i><b>5.10</b> Cross-Validation (CV)</a></li>
<li class="chapter" data-level="5.11" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>5.11</b> Leave-One-Out Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="5.12" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#leave-one-out-cross-validation-loocv-1"><i class="fa fa-check"></i><b>5.12</b> Leave-One-Out Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="5.13" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>5.13</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="5.14" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation"><i class="fa fa-check"></i><b>5.14</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.15" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-1"><i class="fa fa-check"></i><b>5.15</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.16" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-2"><i class="fa fa-check"></i><b>5.16</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.17" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-3"><i class="fa fa-check"></i><b>5.17</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.18" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-4"><i class="fa fa-check"></i><b>5.18</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.19" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-5"><i class="fa fa-check"></i><b>5.19</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation</a></li>
<li class="chapter" data-level="5.20" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-results"><i class="fa fa-check"></i><b>5.20</b> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Results</a></li>
<li class="chapter" data-level="5.21" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#final-model-and-prediction-error-estimate"><i class="fa fa-check"></i><b>5.21</b> Final Model and Prediction Error Estimate</a></li>
<li class="chapter" data-level="5.22" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#variable-importance"><i class="fa fa-check"></i><b>5.22</b> Variable Importance</a></li>
<li class="chapter" data-level="5.23" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#bias-variance-trade-off-for-loocv-and-k-fold-cv"><i class="fa fa-check"></i><b>5.23</b> Bias-Variance Trade-off for LOOCV and <span class="math inline">\(k\)</span>-fold CV</a></li>
<li class="chapter" data-level="5.24" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-5"><i class="fa fa-check"></i><b>5.24</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="5.25" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-split-data"><i class="fa fa-check"></i><b>5.25</b> <span style="color:blue">Your Turn!!!</span>: Split Data</a></li>
<li class="chapter" data-level="5.26" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-perform-cv"><i class="fa fa-check"></i><b>5.26</b> <span style="color:blue">Your Turn!!!</span>: Perform CV</a></li>
<li class="chapter" data-level="5.27" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-observe-cv-results"><i class="fa fa-check"></i><b>5.27</b> <span style="color:blue">Your Turn!!!</span>: Observe CV Results</a></li>
<li class="chapter" data-level="5.28" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#your-turn-final-model"><i class="fa fa-check"></i><b>5.28</b> <span style="color:blue">Your Turn!!!</span>: Final Model</a></li>
<li class="chapter" data-level="5.29" data-path="k-nearest-neighbors-classifier.html"><a href="k-nearest-neighbors-classifier.html#mid-term-check"><i class="fa fa-check"></i><b>5.29</b> Mid-Term Check</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-of-cv.html"><a href="review-of-cv.html"><i class="fa fa-check"></i><b>6</b> Review of CV</a>
<ul>
<li class="chapter" data-level="6.1" data-path="review-of-cv.html"><a href="review-of-cv.html#data-leakage-a-serious-common-problem"><i class="fa fa-check"></i><b>6.1</b> Data Leakage (A Serious, Common Problem)</a></li>
<li class="chapter" data-level="6.2" data-path="review-of-cv.html"><a href="review-of-cv.html#data-preprocessing-and-feature-enginnering"><i class="fa fa-check"></i><b>6.2</b> Data Preprocessing and Feature Enginnering</a></li>
<li class="chapter" data-level="6.3" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-2"><i class="fa fa-check"></i><b>6.3</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.4" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-3"><i class="fa fa-check"></i><b>6.4</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.5" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-4"><i class="fa fa-check"></i><b>6.5</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.6" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-5"><i class="fa fa-check"></i><b>6.6</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.7" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-6"><i class="fa fa-check"></i><b>6.7</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.8" data-path="review-of-cv.html"><a href="review-of-cv.html#ames-housing-dataset-7"><i class="fa fa-check"></i><b>6.8</b> Ames Housing Dataset</a></li>
<li class="chapter" data-level="6.9" data-path="review-of-cv.html"><a href="review-of-cv.html#zero-variance-zv-andor-near-zero-variance-nzv-variables"><i class="fa fa-check"></i><b>6.9</b> Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables</a></li>
<li class="chapter" data-level="6.10" data-path="review-of-cv.html"><a href="review-of-cv.html#zero-variance-zv-andor-near-zero-variance-nzv-variables-1"><i class="fa fa-check"></i><b>6.10</b> Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables</a></li>
<li class="chapter" data-level="6.11" data-path="review-of-cv.html"><a href="review-of-cv.html#zero-variance-zv-andor-near-zero-variance-nzv-variables-2"><i class="fa fa-check"></i><b>6.11</b> Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables</a></li>
<li class="chapter" data-level="6.12" data-path="review-of-cv.html"><a href="review-of-cv.html#imputing-missing-entries"><i class="fa fa-check"></i><b>6.12</b> Imputing Missing Entries</a></li>
<li class="chapter" data-level="6.13" data-path="review-of-cv.html"><a href="review-of-cv.html#imputing-missing-entries-1"><i class="fa fa-check"></i><b>6.13</b> Imputing Missing Entries</a></li>
<li class="chapter" data-level="6.14" data-path="review-of-cv.html"><a href="review-of-cv.html#imputing-missing-entries-2"><i class="fa fa-check"></i><b>6.14</b> Imputing Missing Entries</a></li>
<li class="chapter" data-level="6.15" data-path="review-of-cv.html"><a href="review-of-cv.html#label-encoding-ordinal-categorical-variables"><i class="fa fa-check"></i><b>6.15</b> Label Encoding Ordinal Categorical Variables</a></li>
<li class="chapter" data-level="6.16" data-path="review-of-cv.html"><a href="review-of-cv.html#label-encoding-ordinal-categorical-variables-1"><i class="fa fa-check"></i><b>6.16</b> Label Encoding Ordinal Categorical Variables</a></li>
<li class="chapter" data-level="6.17" data-path="review-of-cv.html"><a href="review-of-cv.html#label-encoding-ordinal-categorical-variables-2"><i class="fa fa-check"></i><b>6.17</b> Label Encoding Ordinal Categorical Variables</a></li>
<li class="chapter" data-level="6.18" data-path="review-of-cv.html"><a href="review-of-cv.html#standardizing-centering-and-scaling-numeric-predictors"><i class="fa fa-check"></i><b>6.18</b> Standardizing (centering and scaling) Numeric Predictors</a></li>
<li class="chapter" data-level="6.19" data-path="review-of-cv.html"><a href="review-of-cv.html#lumping-predictors"><i class="fa fa-check"></i><b>6.19</b> Lumping Predictors</a></li>
<li class="chapter" data-level="6.20" data-path="review-of-cv.html"><a href="review-of-cv.html#one-hotdummy-encoding-categorical-predictors"><i class="fa fa-check"></i><b>6.20</b> One-hot/dummy Encoding Categorical Predictors</a></li>
<li class="chapter" data-level="6.21" data-path="review-of-cv.html"><a href="review-of-cv.html#one-hotdummy-encoding-categorical-predictors-1"><i class="fa fa-check"></i><b>6.21</b> One-hot/dummy Encoding Categorical Predictors</a></li>
<li class="chapter" data-level="6.22" data-path="review-of-cv.html"><a href="review-of-cv.html#preprocessing-steps"><i class="fa fa-check"></i><b>6.22</b> Preprocessing Steps</a></li>
<li class="chapter" data-level="6.23" data-path="review-of-cv.html"><a href="review-of-cv.html#preprocessing-with-recipes-package"><i class="fa fa-check"></i><b>6.23</b> Preprocessing With <code>recipes</code> Package</a></li>
<li class="chapter" data-level="6.24" data-path="review-of-cv.html"><a href="review-of-cv.html#preprocessing-with-recipes-package-1"><i class="fa fa-check"></i><b>6.24</b> Preprocessing With <code>recipes</code> Package</a></li>
<li class="chapter" data-level="6.25" data-path="review-of-cv.html"><a href="review-of-cv.html#training-model"><i class="fa fa-check"></i><b>6.25</b> Training Model</a></li>
<li class="chapter" data-level="6.26" data-path="review-of-cv.html"><a href="review-of-cv.html#training-model-1"><i class="fa fa-check"></i><b>6.26</b> Training Model</a></li>
<li class="chapter" data-level="6.27" data-path="review-of-cv.html"><a href="review-of-cv.html#training-model-2"><i class="fa fa-check"></i><b>6.27</b> Training Model</a></li>
<li class="chapter" data-level="6.28" data-path="review-of-cv.html"><a href="review-of-cv.html#final-model-and-test-set-error"><i class="fa fa-check"></i><b>6.28</b> Final Model and Test Set Error</a></li>
<li class="chapter" data-level="6.29" data-path="review-of-cv.html"><a href="review-of-cv.html#variable-importance-1"><i class="fa fa-check"></i><b>6.29</b> Variable Importance</a></li>
<li class="chapter" data-level="6.30" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-6"><i class="fa fa-check"></i><b>6.30</b> <span style="color:blue">Your Turn!!!</span></a></li>
<li class="chapter" data-level="6.31" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1"><i class="fa fa-check"></i><b>6.31</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.32" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1-1"><i class="fa fa-check"></i><b>6.32</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.33" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1-2"><i class="fa fa-check"></i><b>6.33</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.34" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1-3"><i class="fa fa-check"></i><b>6.34</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.35" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-1-4"><i class="fa fa-check"></i><b>6.35</b> <span style="color:blue">Your Turn!!!</span> Step 1</a></li>
<li class="chapter" data-level="6.36" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-2"><i class="fa fa-check"></i><b>6.36</b> <span style="color:blue">Your Turn!!!</span> Step 2</a></li>
<li class="chapter" data-level="6.37" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-3"><i class="fa fa-check"></i><b>6.37</b> <span style="color:blue">Your Turn!!!</span> Step 3</a></li>
<li class="chapter" data-level="6.38" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-4"><i class="fa fa-check"></i><b>6.38</b> <span style="color:blue">Your Turn!!!</span> Step 4</a></li>
<li class="chapter" data-level="6.39" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-4-1"><i class="fa fa-check"></i><b>6.39</b> <span style="color:blue">Your Turn!!!</span> Step 4</a></li>
<li class="chapter" data-level="6.40" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-4-2"><i class="fa fa-check"></i><b>6.40</b> <span style="color:blue">Your Turn!!!</span> Step 4</a></li>
<li class="chapter" data-level="6.41" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-5"><i class="fa fa-check"></i><b>6.41</b> <span style="color:blue">Your Turn!!!</span> Step 5</a></li>
<li class="chapter" data-level="6.42" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-5-1"><i class="fa fa-check"></i><b>6.42</b> <span style="color:blue">Your Turn!!!</span> Step 5</a></li>
<li class="chapter" data-level="6.43" data-path="review-of-cv.html"><a href="review-of-cv.html#your-turn-step-5-2"><i class="fa fa-check"></i><b>6.43</b> <span style="color:blue">Your Turn!!!</span> Step 5</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">208 Course Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="k-nearest-neighbors-classifier" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> K-Nearest Neighbors Classifier<a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Given a value for <span class="math inline">\(K\)</span> and a test data point <span class="math inline">\(x_0\)</span>,
<span class="math display">\[P(Y=j | X=x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} I(y_i = j)\]</span></p>
<p>where <span class="math inline">\(\mathcal{N}_0\)</span> is known as the <strong>neighborhood</strong> of <span class="math inline">\(x_0\)</span>.</p>
<div id="your-turn-4" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> <span style="color:blue">Your Turn!!!</span><a href="k-nearest-neighbors-classifier.html#your-turn-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Build a 10-NN classifier with the <code>Default</code> dataset. Consider <code>default</code> as the response variable and <code>balance</code> as the predictor. Follow the steps below.</p>
<ul>
<li><p>Load the dataset into R. Observe the dataset, specifically the response variable.</p></li>
<li><p>Perform a 70-30 split of the original dataset.</p></li>
<li><p>With the training data, construct a 10-NN classifier. See help page of function <code>knn3</code>.</p></li>
<li><p>Predict class probabilities on the test data points. See help page of function <code>predict.knn3</code>. Look carefully at the object that is created.</p></li>
<li><p>Obtain predicted class labels for a threshold of 0.3.</p></li>
<li><p>Create the confusion matrix between the observed and predicted class labels.</p></li>
</ul>
</div>
<div id="k-nearest-neighbors-classifier-split-data" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> K-Nearest Neighbors Classifier: Split Data<a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-split-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Default dataset</strong></p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="k-nearest-neighbors-classifier.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)   <span class="co"># load library</span></span>
<span id="cb56-2"><a href="k-nearest-neighbors-classifier.html#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="k-nearest-neighbors-classifier.html#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Default&quot;</span>)   <span class="co"># load dataset</span></span></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="k-nearest-neighbors-classifier.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">012423</span>)  <span class="co"># fix the random number generator for reproducibility</span></span>
<span id="cb57-2"><a href="k-nearest-neighbors-classifier.html#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="k-nearest-neighbors-classifier.html#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)  <span class="co"># load library</span></span>
<span id="cb57-4"><a href="k-nearest-neighbors-classifier.html#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="k-nearest-neighbors-classifier.html#cb57-5" aria-hidden="true" tabindex="-1"></a>train_index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(<span class="at">y =</span> Default<span class="sc">$</span>default, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>) <span class="co"># split available data into 70% training and 30% test datasets</span></span>
<span id="cb57-6"><a href="k-nearest-neighbors-classifier.html#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="k-nearest-neighbors-classifier.html#cb57-7" aria-hidden="true" tabindex="-1"></a>Default_train <span class="ot">&lt;-</span> Default[train_index,]   <span class="co"># training data, use this dataset to build model</span></span>
<span id="cb57-8"><a href="k-nearest-neighbors-classifier.html#cb57-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-9"><a href="k-nearest-neighbors-classifier.html#cb57-9" aria-hidden="true" tabindex="-1"></a>Default_test <span class="ot">&lt;-</span> Default[<span class="sc">-</span>train_index,]   <span class="co"># test data, use this dataset to evaluate model&#39;s performance</span></span></code></pre></div>
</div>
<div id="k-nearest-neighbors-classifier-build-model" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> K-Nearest Neighbors Classifier: Build Model<a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-build-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Default dataset</strong></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="k-nearest-neighbors-classifier.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)   <span class="co"># load package &#39;caret&#39;</span></span>
<span id="cb58-2"><a href="k-nearest-neighbors-classifier.html#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="k-nearest-neighbors-classifier.html#cb58-3" aria-hidden="true" tabindex="-1"></a>knnfit <span class="ot">&lt;-</span> <span class="fu">knn3</span>(default <span class="sc">~</span> balance, <span class="at">data =</span> Default_train, <span class="at">k =</span> <span class="dv">10</span>)   <span class="co"># fit 10-nn model</span></span></code></pre></div>
</div>
<div id="k-nearest-neighbors-classifier-predictions" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> K-Nearest Neighbors Classifier: Predictions<a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Default dataset</strong></p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="k-nearest-neighbors-classifier.html#cb59-1" aria-hidden="true" tabindex="-1"></a>knn_prob_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(knnfit, <span class="at">newdata =</span> Default_test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)   <span class="co"># obtain predictions as probabilities</span></span></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="k-nearest-neighbors-classifier.html#cb60-1" aria-hidden="true" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> <span class="fl">0.3</span>   <span class="co"># set threshold</span></span>
<span id="cb60-2"><a href="k-nearest-neighbors-classifier.html#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="k-nearest-neighbors-classifier.html#cb60-3" aria-hidden="true" tabindex="-1"></a>knn_class_preds <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">ifelse</span>(knn_prob_preds[,<span class="dv">2</span>] <span class="sc">&gt;</span> threshold, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>))   <span class="co"># obtain predictions as class labels</span></span></code></pre></div>
</div>
<div id="k-nearest-neighbors-classifier-performance" class="section level2 smaller hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> K-Nearest Neighbors Classifier: Performance<a href="k-nearest-neighbors-classifier.html#k-nearest-neighbors-classifier-performance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Default dataset</strong></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="k-nearest-neighbors-classifier.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create confusion matrix</span></span>
<span id="cb61-2"><a href="k-nearest-neighbors-classifier.html#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="k-nearest-neighbors-classifier.html#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> <span class="fu">relevel</span>(knn_class_preds, <span class="at">ref =</span> <span class="st">&quot;Yes&quot;</span>), </span>
<span id="cb61-4"><a href="k-nearest-neighbors-classifier.html#cb61-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">reference =</span> <span class="fu">relevel</span>(Default_test<span class="sc">$</span>default, <span class="at">ref =</span> <span class="st">&quot;Yes&quot;</span>))  </span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  Yes   No
##        Yes   39   34
##        No    60 2866
##                                           
##                Accuracy : 0.9687          
##                  95% CI : (0.9618, 0.9746)
##     No Information Rate : 0.967           
##     P-Value [Acc &gt; NIR] : 0.327332        
##                                           
##                   Kappa : 0.4377          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.009922        
##                                           
##             Sensitivity : 0.39394         
##             Specificity : 0.98828         
##          Pos Pred Value : 0.53425         
##          Neg Pred Value : 0.97949         
##              Prevalence : 0.03301         
##          Detection Rate : 0.01300         
##    Detection Prevalence : 0.02434         
##       Balanced Accuracy : 0.69111         
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
</div>
<div id="roc-curve-and-auc" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> ROC Curve and AUC<a href="k-nearest-neighbors-classifier.html#roc-curve-and-auc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>ROC (Receiver Operating Characteristics)</strong> curve is a popular graphic for comparing different classifiers across all possible thresholds. The ROC curve plots the Specificity (1-false positive rate) along the x-axis and the Sensitivity (true positive rate) along the y-axis.</p>
<p>Another popular metric for comparing classifiers is the <strong>AUC (Area Under the ROC Curve)</strong>. An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier.</p>
</div>
<div id="roc-curve-and-auc-1" class="section level2 smaller hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> ROC Curve and AUC<a href="k-nearest-neighbors-classifier.html#roc-curve-and-auc-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Default dataset</strong></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="k-nearest-neighbors-classifier.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC)   <span class="co"># load library</span></span></code></pre></div>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="k-nearest-neighbors-classifier.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create object for ROC curve for KNN fit</span></span>
<span id="cb67-2"><a href="k-nearest-neighbors-classifier.html#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="k-nearest-neighbors-classifier.html#cb67-3" aria-hidden="true" tabindex="-1"></a>roc_object_knn <span class="ot">&lt;-</span> <span class="fu">roc</span>(<span class="at">response =</span> Default_test<span class="sc">$</span>default, <span class="at">predictor =</span> knn_prob_preds[,<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## Setting levels: control = No, case = Yes</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="k-nearest-neighbors-classifier.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot ROC curve</span></span>
<span id="cb70-2"><a href="k-nearest-neighbors-classifier.html#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="k-nearest-neighbors-classifier.html#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(roc_object_knn, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-57-1.png" width="384" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="k-nearest-neighbors-classifier.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain AUC&#39;s</span></span>
<span id="cb71-2"><a href="k-nearest-neighbors-classifier.html#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="k-nearest-neighbors-classifier.html#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="fu">auc</span>(roc_object_knn)</span></code></pre></div>
<pre><code>## Area under the curve: 0.8564</code></pre>
<!-- ## K-Nearest Neighbors Classifier {.smaller} -->
<!-- **Default dataset** -->
<!-- ```{r} -->
<!-- # knn classification with multiple predictors -->
<!-- # scale the inputs -->
<!-- Default_scaled <- Default %>% mutate(balance_scaled = scale(balance), -->
<!--                                      income_scaled = scale(income)) -->
<!-- knnfit <- knn3(default ~ balance_scaled + income_scaled, data = Default_scaled, k = 100)   # fit 100-nn model -->
<!-- # obtain predictions and evaluate performance -->
<!-- threshold = 0.5   # set threshold -->
<!-- knn_preds <- predict(knnfit, newdata = Default_scaled)   # obtain predictions as probabilities -->
<!-- Default_scaled$knn_preds <- ifelse(knn_preds[,2] > threshold, "Yes", "No")   # obtain predictions as class labels -->
<!-- table(true = Default_scaled$default, predicted = Default_scaled$knn_preds)  # create confusion matrix -->
<!-- mean(Default_scaled$default != Default_scaled$knn_preds)    # calculate misclassification rate -->
<!-- ``` -->
<!-- ## Discriminant Analysis -->
<!-- * Logistic regression involves directly modeling $P(Y = k|X = \mathbf{x})$ using the logistic function. -->
<!-- * In **discriminant analysis**, we model the distribution of the predictors $X$ separately in each of the response classes (that is, for each value of $Y$). We then use Bayes theorem to flip these around into estimates for $P(Y = k|X = \mathbf{x})$. -->
<!-- ## Why Discriminant Analysis? -->
<!-- * When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem. -->
<!-- * When $n$ is small and the distribution of the predictors $X$ is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model. -->
<!-- * Linear discriminant analysis is popular when we have more than two response classes. -->
<!-- ## Univariate Normal/Gaussian Distribution -->
<!-- A continuous random variable $X$ is said to have the **normal distribution** with mean $\mu$ and standard deviation $\sigma$ if its pdf is given by -->
<!-- $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}, \ \text{where} \ \ -\infty < x < \infty$$ -->
<!-- Notation: $X\sim \text{N}(\mu,\sigma^2)$ -->
<!-- ## Univariate Normal Distribution -->
<!-- ```{r, echo=FALSE, fig.width=7, fig.height=7, warning=FALSE, fig.align='center'} -->
<!-- # library(ggplot2) -->
<!-- p <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) -->
<!-- fun.1 <- function(x) dnorm(x, mean=0, sd=1) -->
<!-- p1 <- p + stat_function(fun = fun.1) + xlim(-4,4)+ylim(0,.65)+ggtitle(expression(paste( mu, "=0, ", sigma, "=1"))) + ylab("f(x)") -->
<!-- fun.1 <- function(x) dnorm(x, mean=2, sd=1) -->
<!-- p2 <- p + stat_function(fun = fun.1) + xlim(-2,6)+ylim(0,.65)+ggtitle(expression(paste( mu, "=2, ", sigma, "=1"))) + ylab("f(x)") -->
<!-- fun.1 <- function(x) dnorm(x, mean=0, sd=2) -->
<!-- p3 <- p + stat_function(fun = fun.1) + xlim(-4,4)+ylim(0,.65)+ggtitle(expression(paste( mu, "=0, ", sigma, "=2"))) + ylab("f(x)") -->
<!-- fun.1 <- function(x) dnorm(x, mean=0, sd=0.7) -->
<!-- p4 <- p + stat_function(fun = fun.1) + xlim(-4,4)+ylim(0,.65)+ggtitle(expression(paste( mu, "=0, ", sigma, "=0.7"))) + ylab("f(x)") -->
<!-- # library(gridExtra) -->
<!-- grid.arrange(p1,p2,p3,p4, ncol=2) -->
<!-- ``` -->
<!-- ## Multivariate Normal Distribution -->
<!-- A $p$-dimensional random variable $X$ has a multivariate normal/Gaussian distribution with mean vector $\mathbf{\mu}$ (a $p$-dimensional vector) and covariance matrix $\mathbf{\Sigma}$ (a $p$ x $p$ dimensional matrix) if its pdf is given by -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/e4.18.png") -->
<!-- ``` -->
<!-- Notation: $X\sim \text{N}(\mathbf{\mu},\mathbf{\Sigma})$ -->
<!-- ## Multivariate Normal Distribution -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/4.5.png") -->
<!-- ``` -->
<!-- This [website](http://socr.umich.edu/HTML5/BivariateNormal/) gives a visualization of the bivariate normal distribution. -->
<!-- ## Bayes' Theorem -->
<!-- Thomas Bayes was a famous mathematician whose name -->
<!-- represents a big subfield of statistical and probabilistic modeling. -->
<!-- Bayes' Theorem states, -->
<!-- $$P(Y=k|X=x)=\dfrac{P(X=x|Y=k) \cdot P(Y=k)}{P(X=x)}$$ -->
<!-- $$\implies P(Y=k|X=x)=\dfrac{P(X=x|Y=k) \cdot P(Y=k)}{\displaystyle \sum_l P(X=x|Y=l) \cdot P(Y=l)}$$ -->
<!-- ## Linear Discriminant Analysis (LDA) -->
<!-- Uses Bayes' theorem and assuming $P(X=x|Y=k)$ follows a normal distribution. -->
<!-- **Notations** -->
<!-- * $f_k(x) = P(X=x|Y=k)$: **density** of $X$ in class $k$ -->
<!-- * $\pi_k = P(Y=k)$: **prior** probability for class $k$. -->
<!-- * $p_k(x)$: **posterior** probability of an observation $X=x$ belonging to class $k$. -->
<!-- We have, -->
<!-- $$p_k(x)=P(Y=k|X=x)=\dfrac{P(X=x|Y=k) \cdot P(Y=k)}{\displaystyle \sum_l P(X=x|Y=l) \cdot P(Y=l)}$$ -->
<!-- $$\implies p_k(x)=P(Y=k|X=x)=\dfrac{f_k(x) \cdot \pi_k}{\displaystyle \sum_l f_l(x) \cdot \pi_l}$$ -->
<!-- ## LDA for $p=1$ -->
<!-- Suppose $p=1$ (one predictor). -->
<!-- $$p_k(x)=P(Y=k|X=x)=\dfrac{f_k(x) \cdot \pi_k}{\displaystyle \sum_l f_l(x) \cdot \pi_l}$$ -->
<!-- Let $\mu_k$ and $\sigma^2_k$ be mean and variance for $k^{th}$ class. Further, assume $\sigma^2_k=\sigma^2$ for all $k$. Then, -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/e4.12.png") -->
<!-- ``` -->
<!-- ## LDA for $p=1$ -->
<!-- Consider a one-dimensional two-class problem. -->
<!-- **Discriminant function** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/e4.13.png") -->
<!-- ``` -->
<!-- **Decision boundary** -->
<!-- $$x=\dfrac{\mu^2_1-\mu^2_2}{2\left(\mu_1-\mu_2\right)}=\dfrac{\mu_1+\mu_2}{2}$$ -->
<!-- ## LDA for $p=1$: Estimating Parameters -->
<!-- **Parameters**: $\mu_1, \ldots, \mu_k, \sigma^2, \pi_1, \ldots, \pi_k$ -->
<!-- $$\hat{\pi}_k=\dfrac{n_k}{n}$$ -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/e4.15.png") -->
<!-- ``` -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '55%'} -->
<!-- knitr::include_graphics("EFT/e4.17.png") -->
<!-- ``` -->
<!-- ## LDA for $p=1$ -->
<!-- **Default dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', fig.width=10, fig.height=6} -->
<!-- ggplot(data = Default, aes(x = balance)) +  -->
<!--   geom_histogram(aes(y = after_stat(density)), fill = "lightblue") + -->
<!--   geom_density() + -->
<!--   facet_grid(~ default) -->
<!-- # knitr::include_graphics("EFT/4.4.png") -->
<!-- ``` -->
<!-- Example with $\mu_1=-1.25, \mu_2=1.25, \pi_1=\pi_2=0.5, \sigma^2=1$ -->
<!-- ## LDA for $p=1$ -->
<!-- **Default dataset** -->
<!-- ```{r, eval=FALSE} -->
<!-- # install.packages("MASS")   # install package 'MASS' -->
<!-- library(MASS)   # load package 'MASS' -->
<!-- ?lda   # help function for function 'lda' -->
<!-- ``` -->
<!-- ```{r} -->
<!-- ldafit = lda(default ~ balance, data = Default)   # fit LDA -->
<!-- ldafit   # summary of the fit -->
<!-- ``` -->
<!-- ## LDA for $p=1$: Class Predictions -->
<!-- **Default dataset** -->
<!-- ```{r} -->
<!-- threshold = 0.5   # set threshold -->
<!-- preds <- predict(ldafit, newdata = Default)   # obtain predictions as probabilities -->
<!-- Default$ldapreds <- ifelse(preds$posterior[,2] > threshold, "Yes", "No")   # obtain predictions as class labels -->
<!-- table(true = Default$default, predictions = Default$ldapreds)   # create confusion matrix -->
<!-- mean(Default$default != Default$ldapreds)   # calculate misclassification rate -->
<!-- ``` -->
<!-- ## LDA for $p>1$ -->
<!-- Use multivariate normal. -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/e4.18.png") -->
<!-- ``` -->
<!-- **Discriminant function** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/e4.19.png") -->
<!-- ``` -->
<!-- ## <span style="color:blue">Your Turn!!!</span> -->
<!-- Perform LDA on the **Default** dataset with  -->
<!-- ## LDA for $p>1$ -->
<!-- **Decision boundary** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/e4.20.png") -->
<!-- ``` -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/4.6.png") -->
<!-- ``` -->
<!-- ## LDA: Why Discriminant Analysis? -->
<!-- * When the classes are well-separated, the parameter -->
<!-- estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem. -->
<!-- * When $n$ is small and the distribution of the predictors $X$ is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model. -->
<!-- * Linear discriminant analysis is popular when we have more than two response classes. -->
<!-- ## LDA: Class Predictions -->
<!-- **Default dataset** -->
<!-- ```{r} -->
<!-- table(true = Default$default, predicted = Default$class_preds)   # create confusion matrix -->
<!-- ``` -->
<!-- ## <span style="color:blue">Your Turn!!!</span> -->
<!-- Perform LDA with **default** as the response, and **balance** and **income** as predictors. -->
<!-- Obtain class predictions by for a threshold of 0.2. Calculate the **misclassification rate**, **false positive rate**, and **false negative rate**. -->
<!-- ## <span style="color:blue">Your Turn!!!</span> -->
<!-- ```{r} -->
<!-- # LDA with multiple predictors -->
<!-- ldafit1 = lda(default ~ balance + income, data = Default)   # fit LDA -->
<!-- ldafit1  # summary of the fit -->
<!-- ``` -->
<!-- ## <span style="color:blue">Your Turn!!!</span> -->
<!-- ```{r} -->
<!-- # obtain predictions and evaluate performance -->
<!-- threshold = 0.2   # set threshold -->
<!-- preds1 <- predict(ldafit1, newdata = Default)   # obtain predictions as probabilities -->
<!-- Default$ldapreds1 <- ifelse(preds1$posterior[,2] > threshold, "Yes", "No")   # obtain predictions as class labels -->
<!-- table(true = Default$default, predictions = Default$ldapreds1)   # create confusion matrix -->
<!-- mean(Default$default != Default$ldapreds1)   # calculate misclassification rate -->
<!-- ``` -->
<!-- We use $\hat{Y} = 1$ if $P(Y=1|X)> 0.5$, then -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/t4.4.png") -->
<!-- ``` -->
<!-- * Misclassification rate $=\frac{23+252}{10000}=2.75\%$ -->
<!-- * If we always classified $\text{No}$, then error rate 3.33%. -->
<!-- * For true $\text{No}$'s, error rate 0.2%, for true $\text{Yes}$'s error rate 75.7%. -->
<!-- ## Types of Errors -->
<!-- **False positive rate**: fraction of true $0$'s that are classified $1$ - 0.2% for the dataset. -->
<!-- **False negative rate**: fraction of true $1$'s that are classified $0$ - 75.7% for the dataset. -->
<!-- We can change the two error rates by changing the threshold from 0.5 to some other value in [0,1]. -->
<!-- If we use $\hat{Y} = 1$ if $P(Y=1|X)> 0.2$, then -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/t4.5.png") -->
<!-- ``` -->
<!-- ## Types of Errors -->
<!-- <center> -->
<!-- |  | True class labels | -->
<!-- |-----------|----------|----------- -->
<!-- |  |  0 | 1 | -->
<!-- |-----------|----------|----------- -->
<!-- | 0 | True Negative |  False Negative  | -->
<!-- | 1 | False Positive |  True Positive | -->
<!-- </center> -->
<!-- \ -->
<!-- ## Performance of Binary Classifiers: Types of Errors -->
<!-- **Default dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/SL6.png") -->
<!-- ``` -->
<!-- ## Performance of Binary Classifiers: ROC Curve -->
<!-- The ROC curve is a popular graphic for simultaneously displaying two types of errors for all possible thresholds. The name "ROC" is an acronym for **receiver operating characteristics**. The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the curve (AUC). -->
<!-- ```{r, eval=FALSE} -->
<!-- install.packages("pROC")   # install package 'pROC' -->
<!-- library(pROC)   # load package 'pROC' -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # create an object by specifying observed class labels and  -->
<!-- # vector of predicted probabilities for each class -->
<!-- r <- roc(Default$default, preds$posterior[,2])    -->
<!-- r   # obtain AUC -->
<!-- ``` -->
<!-- ## Performance of Binary Classifiers: ROC Curve -->
<!-- ```{r, fig.align='center'} -->
<!-- plot(r)   # plot ROC curve -->
<!-- ``` -->
<!-- ## <span style="color:blue">Your Turn!!!</span> -->
<!-- Consider the **iris** dataset. This is a multi-class classification problem. -->
<!-- ```{r, eval=FALSE} -->
<!-- data("iris")   # load the iris dataset -->
<!-- ?iris   # help page for iris dataset -->
<!-- ``` -->
<!-- Implement KNN with $K=1, 10, 50$ (remember to scale the inputs) and LDA. Obtain the respective misclassification rates on the training data. -->
<!-- ## <span style="color:blue">Your Turn!!!</span> -->
<!-- ```{r} -->
<!-- # multi-class classification problem -->
<!-- # LDA -->
<!-- ldafit_iris = lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris)   # fit LDA -->
<!-- # obtain predictions and evaluate performance -->
<!-- lda_preds_iris <- predict(ldafit_iris, newdata = iris)   # obtain predictions as probabilities -->
<!-- lda_class_preds_iris <- lda_preds_iris$class   # obtain predictions as class labels -->
<!-- table(true = iris$Species, predictions = lda_class_preds_iris)   # create confusion matrix -->
<!-- mean(iris$Species != lda_class_preds_iris)   # calculate misclassification rate -->
<!-- ``` -->
<!-- ## <span style="color:blue">Your Turn!!!</span> {.smaller} -->
<!-- ```{r} -->
<!-- # multi-class classification problem -->
<!-- # KNN -->
<!-- # scale the inputs -->
<!-- iris_scaled <- iris %>% mutate(Sepal.Length_scaled = scale(Sepal.Length), -->
<!--                                Sepal.Width_scaled = scale(Sepal.Width), -->
<!--                                Petal.Length_scaled = scale(Petal.Length), -->
<!--                                Petal.Width_scaled = scale(Petal.Width)) -->
<!-- knnfit_iris = knn3(Species ~ Sepal.Length_scaled + Sepal.Width_scaled +  -->
<!--                     Petal.Length_scaled + Petal.Width_scaled, data = iris_scaled, k = 10)   # fit 10-nn -->
<!-- # obtain predictions and evaluate performance -->
<!-- knn_preds_iris <- predict(knnfit_iris, newdata = iris_scaled)   # obtain predictions as probabilities -->
<!-- knn_class_preds_iris <- predict(knnfit_iris, newdata = iris_scaled, type = "class")   # obtain predictions as class labels -->
<!-- table(true = iris$Species, predictions = knn_class_preds_iris)   # create confusion matrix -->
<!-- mean(iris$Species != knn_class_preds_iris)   # calculate misclassification rate -->
<!-- ``` -->
<!-- **Default dataset** -->
<!-- Receiver Operating Characteristics Curve -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/4.8.png") -->
<!-- ``` -->
<!-- AUC (area under the curve) is used to summarize the overall performance. -->
<!-- ## Quadratic Discriminant Analysis (QDA) -->
<!-- Assumes each class has its own covariance matrix $\mathbf{\Sigma}_k$. The discriminant function takes the form -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '80%'} -->
<!-- knitr::include_graphics("EFT/e4.23.png") -->
<!-- ``` -->
<!-- ## Quadratic Discriminant Analysis (QDA) -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/4.9.png") -->
<!-- ``` -->
<!-- ## K-Nearest Neighbors Classifier -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/SL3.png") -->
<!-- ``` -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/2.14.png") -->
<!-- ``` -->
<!-- ## K-Nearest Neighbors Classifier -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/2.15.png") -->
<!-- ``` -->
<!-- ## K-Nearest Neighbors Classifier -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->
<!-- knitr::include_graphics("EFT/2.16.png") -->
<!-- ``` -->
<!-- ## K-Nearest Neighbors Classifier -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '80%'} -->
<!-- knitr::include_graphics("EFT/2.17.png") -->
<!-- ``` -->
<!-- ## Classification Summary -->
<!-- * K-NN delivers good performance for most classification problems, however, its performance degrades for large $p$. -->
<!-- * Logistic regression is also a popular tool for classification, especially when $K = 2$. -->
<!-- * LDA is useful when $n$ is small, or the classes are well separated, and Gaussian assumptions are reasonable. Also when $K > 2$. -->
<!-- * Naive Bayes is useful when $p$ is very large. -->
<!-- * Section 4.5 compares logistic regression, LDA, and K-NN. -->
<!-- # create ROC curve -->
<!-- install.packages("pROC")   # install package 'pROC' -->
<!-- library(pROC)   # load package 'pROC' -->
<!-- ?roc   # help page for function 'roc' -->
<!-- # create an object by specifying observed class labels and  -->
<!-- # vector of predicted probabilities for each class -->
<!-- r <- roc(Default$default, preds$posterior[,2])    -->
<!-- r   # obtain AUC -->
<!-- plot(r)   # plot ROC curve -->
</div>
<div id="data-splitting" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Data Splitting<a href="k-nearest-neighbors-classifier.html#data-splitting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Available data split into <strong>training</strong> and <strong>test</strong> datasets.</p>
<ul>
<li><p><strong>Training set:</strong> these data are used to develop feature sets, train our algorithms, tune parameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production).</p></li>
<li><p><strong>Test set:</strong> having chosen a final model, these data are used to obtain an unbiased estimate of the models performance.</p></li>
</ul>
<p><strong>It is critical that the test set not be used prior to selecting your final model.</strong> Assessing results on the test set prior to final model selection biases the model selection process since the testing data will have become part of the model development process.</p>
<!-- ## Resampling Methods -->
<!-- ```{r} -->
<!-- # load required packages -->
<!-- library(tidyverse) -->
<!-- library(caret) -->
<!-- library(ISLR) -->
<!-- data("Auto")   # load 'Auto' daatset -->
<!-- # split available data into training and test data -->
<!-- set.seed(04192022)   # fix the random number generator for reproducibility -->
<!-- # response: 'mpg' -->
<!-- train_index <- createDataPartition(Auto$mpg, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets -->
<!-- Auto_train <- Auto[train_index,]   # training data, we will work with this to choose our final model -->
<!-- Auto_test <- Auto[-train_index,]   # test data, KEEP IT ASIDE, use only after choosing final model -->
<!-- ``` -->
</div>
<div id="resampling-methods" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Resampling Methods<a href="k-nearest-neighbors-classifier.html#resampling-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><strong>Idea:</strong> Repeatedly draw samples from the training data and refit a model on each sample, and evaluate its performance on the other parts.</p></li>
<li><p><strong>Objective:</strong> To obtain additional information about the fitted model.</p></li>
<li><p><strong>Cross-Validation (CV)</strong> is probably the most widely used resampling method. It is a general approach that can be applied to almost any statistical learning method.</p></li>
</ul>
</div>
<div id="cross-validation-cv" class="section level2 hasAnchor" number="5.10">
<h2><span class="header-section-number">5.10</span> Cross-Validation (CV)<a href="k-nearest-neighbors-classifier.html#cross-validation-cv" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Used for</p>
<ul>
<li><p><strong>model selection</strong>: select the optimum level of flexibility (tune hyperparameters) or compare different models to choose the best one</p></li>
<li><p><strong>model assessment</strong>: evaluate the performance of a model (estimate its test error)</p></li>
</ul>
<p>We will talk about</p>
<!-- * Validation Set Approach -->
<ul>
<li><p>Leave-One-Out Cross-Validation (LOOCV)</p></li>
<li><p><span class="math inline">\(k\)</span>-Fold Cross-Validation</p></li>
</ul>
<!-- ## Training Error vs Test Error -->
<!-- * **Training Error**: Calculated by applying the statistical learning method to the observations used in its training. -->
<!-- * **Test Error**: Average error that results from using a -->
<!-- statistical learning method to predict the response on a new unseen observation. -->
<!-- **Test Error Estimates** -->
<!-- * From a large designated test set. -->
<!-- * Making a mathematical adjustment to the training error. (Chapter 6) -->
<!-- * By **holding out** a subset of the training dataset, then assessing model performance on the held out observations. -->
<!-- ## Training Error vs Test Error -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/SL_C5_1.PNG") -->
<!-- ``` -->
<!-- ## Validation Set Approach -->
<!-- * Randomly divide the available set of observations into: a **training set** and a **validation/hold-out set**. -->
<!-- * Model fit on the training set. Fitted model is used to predict the responses for the observations in the -->
<!-- validation set. -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/5.1.PNG") -->
<!-- ``` -->
<!-- ## Validation Set Approach -->
<!-- **Auto dataset** -->
<!-- ```{r, echo=FALSE, fig.align='center'} -->
<!-- data("Auto") -->
<!-- plot(Auto$horsepower,Auto$mpg,ylab="mpg",xlab="horsepower") -->
<!-- ``` -->
<!-- Randomly split the 392 observations into two sets, a training set containing 196 data points, and a validation set containing the remaining 196 observations. -->
<!-- ## Validation Set Approach -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/5.2.PNG") -->
<!-- ``` -->
<!-- Potential drawbacks: -->
<!-- * The validation set estimate of the test error can be highly variable. -->
<!-- * Only a subset of the observations (those that are in the training set) are used to fit the model. This suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set. -->
<!-- * Yields different results due to randomness in training and validation datasets. -->
</div>
<div id="leave-one-out-cross-validation-loocv" class="section level2 hasAnchor" number="5.11">
<h2><span class="header-section-number">5.11</span> Leave-One-Out Cross-Validation (LOOCV)<a href="k-nearest-neighbors-classifier.html#leave-one-out-cross-validation-loocv" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- * Closely related to the validation set approach. Attempts to address its drawbacks. -->
<p><img src="EFT/5.3.PNG" width="70%" style="display: block; margin: auto;" /></p>
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '30%'} -->
<!-- knitr::include_graphics("EFT/e5.1.PNG") -->
<!-- ``` -->
<!-- ## Leave-One-Out Cross-Validation (LOOCV) -->
<!-- ```{r, fig.align='center', fig.height=6, fig.width=8} -->
<!-- # comparing 4 polynomial (regression) models with LOOCV (response: 'mpg', predictor: 'horsepower') -->
<!-- ggplot(data = Auto, aes(x = horsepower, y = mpg)) +   # quick visual check -->
<!--   geom_point() -->
<!-- ``` -->
<!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->
<!-- ```{r,message=FALSE} -->
<!-- ames <- readRDS("AmesHousing.rds")   # load dataset -->
<!-- ``` -->
<!-- Consider `Sale_Price` as the response variable. Split the data into training and test data. -->
<!-- ```{r, message = FALSE} -->
<!-- set.seed(012423)  # fix the random number generator for reproducibility -->
<!-- library(caret)  # load library -->
<!-- train_index <- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets -->
<!-- ames_train <- ames[train_index,]   # training data, use this dataset to build model -->
<!-- ames_test <- ames[-train_index,]   # test data, use this dataset to evaluate model's performance -->
<!-- ``` -->
<!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->
<!-- Define CV specifications. -->
<!-- ```{r} -->
<!-- cv_specs_loocv <- trainControl(method = "LOOCV")   # specify CV method -->
<!-- ``` -->
<!-- We will compare the following three linear regression models: -->
<!-- * with `Garage_Area` as the only predictor; -->
<!-- * with `Overall_Qual` as the only predictor; -->
<!-- * with `Garage_Area`, `Year_Built`, and `Overall_Qual` as predictors. -->
<!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->
<!-- Implement LOOCV with the first model. -->
<!-- ```{r} -->
<!-- m1 <- train(form = Sale_Price ~ Garage_Area,    # specify model -->
<!--             data = ames_train,   # specify dataset -->
<!--             method = "lm",       # specify type of model -->
<!--             trControl = cv_specs_loocv, # CV specifications -->
<!--             metric = "RMSE")   # metric to evaluate model -->
<!-- m1   # summary of LOOCV -->
<!-- ``` -->
<!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->
<!-- Implement LOOCV with the second model. -->
<!-- ```{r} -->
<!-- m2 <- train(form = Sale_Price ~ Overall_Qual,   -->
<!--             data = ames_train,           -->
<!--             method = "lm",               -->
<!--             trControl = cv_specs_loocv,        -->
<!--             metric = "RMSE")            -->
<!-- m2 -->
<!-- ``` -->
<!-- ## Leave-One-Out Cross-Validation (LOOCV): Implementation -->
<!-- Implement LOOCV with the third model. -->
<!-- ```{r} -->
<!-- m3 <- train(form = Sale_Price ~ Garage_Area + Year_Built + Overall_Qual,   -->
<!--             data = ames_train, -->
<!--             method = "lm", -->
<!--             trControl = cv_specs_loocv, -->
<!--             metric = "RMSE") -->
<!-- m3 -->
<!-- ``` -->
<!-- ## Leave-One-Out Cross-Validation (LOOCV): Results -->
<!-- Compare LOOCV results for different models. -->
<!-- ```{r, fig.align='center', fig.height=6, fig.width=8} -->
<!-- # create data frame to plot results -->
<!-- df <- data.frame(model_number = 1:3, RMSE = c(m1$results$RMSE,   -->
<!--                                              m2$results$RMSE, -->
<!--                                              m3$results$RMSE)) -->
<!-- # plot results from LOOCV -->
<!-- ggplot(data = df, aes(x = model_number, y =  RMSE)) +    -->
<!--   geom_point() + geom_line() -->
<!-- ``` -->
</div>
<div id="leave-one-out-cross-validation-loocv-1" class="section level2 hasAnchor" number="5.12">
<h2><span class="header-section-number">5.12</span> Leave-One-Out Cross-Validation (LOOCV)<a href="k-nearest-neighbors-classifier.html#leave-one-out-cross-validation-loocv-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Advantages</strong></p>
<ul>
<li>LOOCV will give approximately unbiased estimates of the test error, since each training set contains <span class="math inline">\(n1\)</span> observations, which is almost as many as the number of observations in the full training dataset.</li>
</ul>
<!-- * Has far less bias than the validation set approach, since the training sets (used to fit the model) are almost as big as the original dataset. -->
<ul>
<li>Performing LOOCV multiple times will always yield the same results.</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li><p>Can be potentially expensive to implement, specially for large <span class="math inline">\(n\)</span>.</p></li>
<li><p>LOOCV error estimate can have high variance.</p></li>
</ul>
</div>
<div id="k-fold-cross-validation" class="section level2 hasAnchor" number="5.13">
<h2><span class="header-section-number">5.13</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation<a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Randomly divide the training data into <span class="math inline">\(k\)</span> groups or <strong>folds</strong> (approximately equal size).</p></li>
<li><p>Consider one of these folds as the validation set. Fit the model on the remaining <span class="math inline">\(k-1\)</span> folds combined, and obtain predictions for the <span class="math inline">\(k^{th}\)</span> fold. Repeat for all <span class="math inline">\(k\)</span> folds.</p></li>
</ul>
<p><img src="EFT/5.5.PNG" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="k-fold-cross-validation-implementation" class="section level2 hasAnchor" number="5.14">
<h2><span class="header-section-number">5.14</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation<a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="k-nearest-neighbors-classifier.html#cb73-1" aria-hidden="true" tabindex="-1"></a>ames <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">&quot;AmesHousing.rds&quot;</span>)   <span class="co"># load dataset</span></span></code></pre></div>
<p>Consider <code>Sale_Price</code> as the response variable. We will compare the following three linear regression models:</p>
<ul>
<li><p>with <code>Garage_Area</code> as the only predictor;</p></li>
<li><p>with <code>Overall_Qual</code> as the only predictor;</p></li>
<li><p>with <code>Garage_Area</code>, <code>Year_Built</code>, and <code>Overall_Qual</code> as predictors.</p></li>
</ul>
</div>
<div id="k-fold-cross-validation-implementation-1" class="section level2 hasAnchor" number="5.15">
<h2><span class="header-section-number">5.15</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation<a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<p>Split the data into training and test data.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="k-nearest-neighbors-classifier.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">012423</span>)  <span class="co"># fix the random number generator for reproducibility</span></span>
<span id="cb74-2"><a href="k-nearest-neighbors-classifier.html#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="k-nearest-neighbors-classifier.html#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)  <span class="co"># load library</span></span>
<span id="cb74-4"><a href="k-nearest-neighbors-classifier.html#cb74-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-5"><a href="k-nearest-neighbors-classifier.html#cb74-5" aria-hidden="true" tabindex="-1"></a>train_index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(<span class="at">y =</span> ames<span class="sc">$</span>Sale_Price, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>) <span class="co"># split available data into 80% training and 20% test datasets</span></span>
<span id="cb74-6"><a href="k-nearest-neighbors-classifier.html#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="k-nearest-neighbors-classifier.html#cb74-7" aria-hidden="true" tabindex="-1"></a>ames_train <span class="ot">&lt;-</span> ames[train_index,]   <span class="co"># training data, use this dataset to build model</span></span>
<span id="cb74-8"><a href="k-nearest-neighbors-classifier.html#cb74-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-9"><a href="k-nearest-neighbors-classifier.html#cb74-9" aria-hidden="true" tabindex="-1"></a>ames_test <span class="ot">&lt;-</span> ames[<span class="sc">-</span>train_index,]   <span class="co"># test data, use this dataset to evaluate model&#39;s performance</span></span></code></pre></div>
</div>
<div id="k-fold-cross-validation-implementation-2" class="section level2 hasAnchor" number="5.16">
<h2><span class="header-section-number">5.16</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation<a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<p>Define CV specifications.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="k-nearest-neighbors-classifier.html#cb75-1" aria-hidden="true" tabindex="-1"></a>cv_specs_kcv <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,   <span class="co"># CV method</span></span>
<span id="cb75-2"><a href="k-nearest-neighbors-classifier.html#cb75-2" aria-hidden="true" tabindex="-1"></a>                             <span class="at">number =</span> <span class="dv">10</span>,    <span class="co"># number of folds</span></span>
<span id="cb75-3"><a href="k-nearest-neighbors-classifier.html#cb75-3" aria-hidden="true" tabindex="-1"></a>                             <span class="at">repeats =</span> <span class="dv">5</span>)     <span class="co"># each repeated 5 times</span></span></code></pre></div>
</div>
<div id="k-fold-cross-validation-implementation-3" class="section level2 hasAnchor" number="5.17">
<h2><span class="header-section-number">5.17</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation<a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<p>Implement <span class="math inline">\(k\)</span>-fold CV with the first model.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="k-nearest-neighbors-classifier.html#cb76-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">form =</span> Sale_Price <span class="sc">~</span> Garage_Area,    <span class="co"># specify model</span></span>
<span id="cb76-2"><a href="k-nearest-neighbors-classifier.html#cb76-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> ames_train,   <span class="co"># specify dataset</span></span>
<span id="cb76-3"><a href="k-nearest-neighbors-classifier.html#cb76-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,       <span class="co"># specify type of model</span></span>
<span id="cb76-4"><a href="k-nearest-neighbors-classifier.html#cb76-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> cv_specs_kcv, <span class="co"># CV specifications</span></span>
<span id="cb76-5"><a href="k-nearest-neighbors-classifier.html#cb76-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)   <span class="co"># metric to evaluate model</span></span>
<span id="cb76-6"><a href="k-nearest-neighbors-classifier.html#cb76-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-7"><a href="k-nearest-neighbors-classifier.html#cb76-7" aria-hidden="true" tabindex="-1"></a>m1   <span class="co"># summary of LOOCV</span></span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 706 samples
##   1 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 636, 635, 637, 636, 635, 634, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   62931.53  0.4437306  43590.34
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="k-nearest-neighbors-classifier.html#cb78-1" aria-hidden="true" tabindex="-1"></a>m1<span class="sc">$</span>results  <span class="co"># estimate and variability of metrics</span></span></code></pre></div>
<pre><code>##   intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD
## 1      TRUE 62931.53 0.4437306 43590.34 12411.41  0.1024043 5478.664</code></pre>
</div>
<div id="k-fold-cross-validation-implementation-4" class="section level2 hasAnchor" number="5.18">
<h2><span class="header-section-number">5.18</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation<a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<p>Implement <span class="math inline">\(k\)</span>-fold CV with the second model.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="k-nearest-neighbors-classifier.html#cb80-1" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">form =</span> Sale_Price <span class="sc">~</span> Overall_Qual,  </span>
<span id="cb80-2"><a href="k-nearest-neighbors-classifier.html#cb80-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> ames_train,          </span>
<span id="cb80-3"><a href="k-nearest-neighbors-classifier.html#cb80-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,              </span>
<span id="cb80-4"><a href="k-nearest-neighbors-classifier.html#cb80-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> cv_specs_kcv,       </span>
<span id="cb80-5"><a href="k-nearest-neighbors-classifier.html#cb80-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)           </span></code></pre></div>
<pre><code>## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit
## may be misleading

## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit
## may be misleading</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="k-nearest-neighbors-classifier.html#cb82-1" aria-hidden="true" tabindex="-1"></a>m2</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 706 samples
##   1 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 635, 636, 635, 636, 636, 636, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   46268.31  0.6968243  31912.75
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="k-nearest-neighbors-classifier.html#cb84-1" aria-hidden="true" tabindex="-1"></a>m2<span class="sc">$</span>results</span></code></pre></div>
<pre><code>##   intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD
## 1      TRUE 46268.31 0.6968243 31912.75 10192.63 0.09720067 4282.579</code></pre>
</div>
<div id="k-fold-cross-validation-implementation-5" class="section level2 hasAnchor" number="5.19">
<h2><span class="header-section-number">5.19</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Implementation<a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-implementation-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<p>Implement <span class="math inline">\(k\)</span>-fold CV with the third model.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="k-nearest-neighbors-classifier.html#cb86-1" aria-hidden="true" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">form =</span> Sale_Price <span class="sc">~</span> Garage_Area <span class="sc">+</span> Year_Built <span class="sc">+</span> Overall_Qual,  </span>
<span id="cb86-2"><a href="k-nearest-neighbors-classifier.html#cb86-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> ames_train,</span>
<span id="cb86-3"><a href="k-nearest-neighbors-classifier.html#cb86-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb86-4"><a href="k-nearest-neighbors-classifier.html#cb86-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">trControl =</span> cv_specs_kcv,</span>
<span id="cb86-5"><a href="k-nearest-neighbors-classifier.html#cb86-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</span>
<span id="cb86-6"><a href="k-nearest-neighbors-classifier.html#cb86-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-7"><a href="k-nearest-neighbors-classifier.html#cb86-7" aria-hidden="true" tabindex="-1"></a>m3</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 706 samples
##   3 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 636, 634, 635, 636, 636, 636, ... 
## Resampling results:
## 
##   RMSE      Rsquared  MAE     
##   42626.49  0.745911  28441.93
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="k-nearest-neighbors-classifier.html#cb88-1" aria-hidden="true" tabindex="-1"></a>m3<span class="sc">$</span>results</span></code></pre></div>
<pre><code>##   intercept     RMSE Rsquared      MAE   RMSESD RsquaredSD    MAESD
## 1      TRUE 42626.49 0.745911 28441.93 10024.47  0.1022967 3484.786</code></pre>
</div>
<div id="k-fold-cross-validation-results" class="section level2 hasAnchor" number="5.20">
<h2><span class="header-section-number">5.20</span> <span class="math inline">\(k\)</span>-Fold Cross-Validation: Results<a href="k-nearest-neighbors-classifier.html#k-fold-cross-validation-results" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<p>Compare <span class="math inline">\(k\)</span>-fold CV results for different models.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="k-nearest-neighbors-classifier.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create data frame to plot results</span></span>
<span id="cb90-2"><a href="k-nearest-neighbors-classifier.html#cb90-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">model_number =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">RMSE =</span> <span class="fu">c</span>(m1<span class="sc">$</span>results<span class="sc">$</span>RMSE,  </span>
<span id="cb90-3"><a href="k-nearest-neighbors-classifier.html#cb90-3" aria-hidden="true" tabindex="-1"></a>                                             m2<span class="sc">$</span>results<span class="sc">$</span>RMSE,</span>
<span id="cb90-4"><a href="k-nearest-neighbors-classifier.html#cb90-4" aria-hidden="true" tabindex="-1"></a>                                             m3<span class="sc">$</span>results<span class="sc">$</span>RMSE))</span>
<span id="cb90-5"><a href="k-nearest-neighbors-classifier.html#cb90-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-6"><a href="k-nearest-neighbors-classifier.html#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot results from LOOCV</span></span>
<span id="cb90-7"><a href="k-nearest-neighbors-classifier.html#cb90-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> model_number, <span class="at">y =</span>  RMSE)) <span class="sc">+</span>   </span>
<span id="cb90-8"><a href="k-nearest-neighbors-classifier.html#cb90-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_line</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-66-1.png" width="768" style="display: block; margin: auto;" /></p>
<!-- ## $k$-Fold Cross-Validation -->
<!-- ```{r} -->
<!-- # comparing 4 polynomial (regression) models with k-fold CV (response: 'mpg', predictor: 'horsepower') -->
<!-- set.seed(041920221)   # fix the random number generator for reproducibility -->
<!-- # CV specifications (method, number of folds k, number of repetitions) -->
<!-- cv_specs <- trainControl(method = "repeatedcv", number = 10, repeats = 5)  -->
<!-- m1 <- train(mpg ~ horsepower,   # model: y = beta_0 + beta_1 x -->
<!--             data = Auto_train, -->
<!--             method = "lm", -->
<!--             trControl = cv_specs, -->
<!--             metric = "RMSE") -->
<!-- m1   # summary of model with LOOCV -->
<!-- ``` -->
<!-- ## $k$-Fold Cross-Validation -->
<!-- ```{r} -->
<!-- m2 <- train(mpg ~ poly(horsepower,2),   # model: y = beta_0 + beta_1 x + beta_2 x^2 -->
<!--             data = Auto_train, -->
<!--             method = "lm", -->
<!--             trControl = cv_specs, -->
<!--             metric = "RMSE") -->
<!-- m3 <- train(mpg ~ poly(horsepower, 3),   # model: y = beta_0 + beta_1 x + beta_2 x^2 + beta_3 x^3 -->
<!--             data = Auto_train, -->
<!--             method = "lm", -->
<!--             trControl = cv_specs, -->
<!--             metric = "RMSE") -->
<!-- m4 <- train(mpg ~ poly(horsepower, 4),   # model: y = beta_0 + beta_1 x + beta_2 x^2 + beta_3 x^3 + beta_4 x^4 -->
<!--             data = Auto_train, -->
<!--             method = "lm", -->
<!--             trControl = cv_specs, -->
<!--             metric = "RMSE") -->
<!-- ``` -->
<!-- ## $k$-Fold Cross-Validation -->
<!-- ```{r, fig.align='center', fig.height=6, fig.width=8} -->
<!-- df <- data.frame(poly_degree = 1:4, RMSE = c(m1$results$RMSE,   # create data frame to plot results -->
<!--                                              m2$results$RMSE, -->
<!--                                              m3$results$RMSE, -->
<!--                                              m4$results$RMSE)) -->
<!-- ggplot(data = df, aes(x = poly_degree, y =  RMSE)) +    # plot results from LOOCV -->
<!--   geom_point() + geom_line() -->
<!-- ``` -->
</div>
<div id="final-model-and-prediction-error-estimate" class="section level2 hasAnchor" number="5.21">
<h2><span class="header-section-number">5.21</span> Final Model and Prediction Error Estimate<a href="k-nearest-neighbors-classifier.html#final-model-and-prediction-error-estimate" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="k-nearest-neighbors-classifier.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># after choosing final (optimal) model, refit final model using ALL training data, and obtain estimate of prediction error from test data</span></span>
<span id="cb91-2"><a href="k-nearest-neighbors-classifier.html#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="k-nearest-neighbors-classifier.html#cb91-3" aria-hidden="true" tabindex="-1"></a>m3<span class="sc">$</span>finalModel    <span class="co"># final model</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Coefficients:
##                (Intercept)                 Garage_Area  
##                 -504482.40                       78.99  
##                 Year_Built         Overall_QualAverage  
##                     320.09                   -18028.34  
##  Overall_QualBelow_Average       Overall_QualExcellent  
##                  -34620.70                   163226.47  
##           Overall_QualFair            Overall_QualGood  
##                  -63928.81                    30780.92  
##           Overall_QualPoor  Overall_QualVery_Excellent  
##                  -76426.87                   261135.65  
##      Overall_QualVery_Good       Overall_QualVery_Poor  
##                   81711.39                   -69470.99</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="k-nearest-neighbors-classifier.html#cb93-1" aria-hidden="true" tabindex="-1"></a>final_model_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(m3, <span class="at">newdata =</span> ames_test)   <span class="co"># obtain predictions on test data</span></span>
<span id="cb93-2"><a href="k-nearest-neighbors-classifier.html#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="k-nearest-neighbors-classifier.html#cb93-3" aria-hidden="true" tabindex="-1"></a>pred_error_est <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((ames_test<span class="sc">$</span>Sale_Price <span class="sc">-</span> final_model_preds)<span class="sc">^</span><span class="dv">2</span>))    <span class="co"># calculate RMSE (estimate of prediction error) from test data</span></span>
<span id="cb93-4"><a href="k-nearest-neighbors-classifier.html#cb93-4" aria-hidden="true" tabindex="-1"></a>pred_error_est</span></code></pre></div>
<pre><code>## [1] 34006.68</code></pre>
</div>
<div id="variable-importance" class="section level2 hasAnchor" number="5.22">
<h2><span class="header-section-number">5.22</span> Variable Importance<a href="k-nearest-neighbors-classifier.html#variable-importance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Ames Housing dataset</strong></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="k-nearest-neighbors-classifier.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># variable importance</span></span>
<span id="cb95-2"><a href="k-nearest-neighbors-classifier.html#cb95-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-3"><a href="k-nearest-neighbors-classifier.html#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;vip&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:utils&#39;:
## 
##     vi</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="k-nearest-neighbors-classifier.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(<span class="at">object =</span> m3,         <span class="co"># CV object </span></span>
<span id="cb98-2"><a href="k-nearest-neighbors-classifier.html#cb98-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">num_features =</span> <span class="dv">20</span>,   <span class="co"># maximum number of predictors to show importance for</span></span>
<span id="cb98-3"><a href="k-nearest-neighbors-classifier.html#cb98-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;model&quot;</span>)            <span class="co"># model-specific VI scores</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-69-1.png" width="768" style="display: block; margin: auto;" /></p>
<!-- ## $k$-Fold Cross-Validation -->
<!-- Let $n_i$ be the number of observations in  $i^{th}, (i=1,\ldots,k)$ fold. If $n$ is a multiple of $k$, then $n_i=\frac{n}{k}$. -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '40%'} -->
<!-- knitr::include_graphics("EFT/e5.3.PNG") -->
<!-- ``` -->
<!-- If the folds are not of equal size, -->
<!-- $$CV_{(k)}=\displaystyle \sum_{i=1}^k \dfrac{n_i}{n} MSE_{i}$$ -->
<!-- A good choice is $k=5$ or $10$. LOOCV is a special case of $k$-fold CV. -->
<!-- ## LOOCV and $k$-Fold CV -->
<!-- **Auto dataset** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->
<!-- knitr::include_graphics("EFT/5.4.PNG") -->
<!-- ``` -->
<!-- ## Performance of LOOCV and $k$-Fold CV -->
<!-- <!-- **Figures 2.9-2.11 in Chapter 2** -->
<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->
<!-- knitr::include_graphics("EFT/5.6.PNG") -->
<!-- ``` -->
</div>
<div id="bias-variance-trade-off-for-loocv-and-k-fold-cv" class="section level2 hasAnchor" number="5.23">
<h2><span class="header-section-number">5.23</span> Bias-Variance Trade-off for LOOCV and <span class="math inline">\(k\)</span>-fold CV<a href="k-nearest-neighbors-classifier.html#bias-variance-trade-off-for-loocv-and-k-fold-cv" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>LOOCV has very less bias. Using <span class="math inline">\(k=5\)</span> or <span class="math inline">\(10\)</span> yields more bias than LOOCV.</li>
</ul>
<!-- , but less than validation set approach. -->
<ul>
<li><p>For LOOCV, the error estimates for each fold are highly (positively) correlated. <span class="math inline">\(k\)</span>-fold CV error estimates are somewhat less correlated. LOOCV error estimate has higher variance than <span class="math inline">\(k\)</span>-fold CV error estimate.</p></li>
<li><p>Typically, <span class="math inline">\(k=5\)</span> or <span class="math inline">\(10\)</span> is chosen.</p></li>
</ul>
</div>
<div id="your-turn-5" class="section level2 hasAnchor" number="5.24">
<h2><span class="header-section-number">5.24</span> <span style="color:blue">Your Turn!!!</span><a href="k-nearest-neighbors-classifier.html#your-turn-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Auto dataset</strong></p>
<p>Load the dataset.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="k-nearest-neighbors-classifier.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ISLR2)  <span class="co"># load library</span></span>
<span id="cb99-2"><a href="k-nearest-neighbors-classifier.html#cb99-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-3"><a href="k-nearest-neighbors-classifier.html#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Auto&quot;</span>)   <span class="co"># load dataset</span></span></code></pre></div>
<p>Consider <code>mpg</code> as the response and <code>horsepower</code> as the predictor.</p>
<p><strong>Objective</strong>: Find the optimum choice of <span class="math inline">\(K\)</span> in the KNN approach with 5-fold CV repeated 5 times. You can use the following steps.</p>
<ul>
<li><p>Split the data into training and test data (80-20 split).</p></li>
<li><p>Specify CV specifications using <strong>trainControl</strong>.</p></li>
<li><p>Create an object <strong>k_grid</strong> using the following code.</p></li>
</ul>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="k-nearest-neighbors-classifier.html#cb100-1" aria-hidden="true" tabindex="-1"></a>k_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">100</span>, <span class="at">by =</span> <span class="dv">1</span>))  <span class="co"># creates a grid of k values to be used (1 to 100 in this case)</span></span></code></pre></div>
<ul>
<li><p>Use the <strong>train</strong> function to run CV. Use <strong>method = knn</strong>, <strong>tuneGrid = k_grid</strong>, and <strong>metric = RMSE</strong>.</p></li>
<li><p>Obtain the results and plot them. What is the optimum <span class="math inline">\(k\)</span> chosen?</p></li>
<li><p>Create the final model using the optimum <span class="math inline">\(k\)</span> and estimate its prediction error from the test data.</p></li>
</ul>
</div>
<div id="your-turn-split-data" class="section level2 hasAnchor" number="5.25">
<h2><span class="header-section-number">5.25</span> <span style="color:blue">Your Turn!!!</span>: Split Data<a href="k-nearest-neighbors-classifier.html#your-turn-split-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Auto dataset</strong></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="k-nearest-neighbors-classifier.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">012423</span>)  <span class="co"># fix the random number generator for reproducibility</span></span>
<span id="cb101-2"><a href="k-nearest-neighbors-classifier.html#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="k-nearest-neighbors-classifier.html#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)  <span class="co"># load library</span></span>
<span id="cb101-4"><a href="k-nearest-neighbors-classifier.html#cb101-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-5"><a href="k-nearest-neighbors-classifier.html#cb101-5" aria-hidden="true" tabindex="-1"></a>train_index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(<span class="at">y =</span> Auto<span class="sc">$</span>mpg, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>) <span class="co"># split available data into 80% training and 20% test datasets</span></span>
<span id="cb101-6"><a href="k-nearest-neighbors-classifier.html#cb101-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-7"><a href="k-nearest-neighbors-classifier.html#cb101-7" aria-hidden="true" tabindex="-1"></a>Auto_train <span class="ot">&lt;-</span> Auto[train_index,]   <span class="co"># training data, use this dataset to build model</span></span>
<span id="cb101-8"><a href="k-nearest-neighbors-classifier.html#cb101-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-9"><a href="k-nearest-neighbors-classifier.html#cb101-9" aria-hidden="true" tabindex="-1"></a>Auto_test <span class="ot">&lt;-</span> Auto[<span class="sc">-</span>train_index,]   <span class="co"># test data, use this dataset to evaluate model&#39;s performance</span></span></code></pre></div>
</div>
<div id="your-turn-perform-cv" class="section level2 hasAnchor" number="5.26">
<h2><span class="header-section-number">5.26</span> <span style="color:blue">Your Turn!!!</span>: Perform CV<a href="k-nearest-neighbors-classifier.html#your-turn-perform-cv" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Auto dataset</strong></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="k-nearest-neighbors-classifier.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">012423</span>)  <span class="co"># fix the random number generator for reproducibility</span></span>
<span id="cb102-2"><a href="k-nearest-neighbors-classifier.html#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="k-nearest-neighbors-classifier.html#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CV specifications</span></span>
<span id="cb102-4"><a href="k-nearest-neighbors-classifier.html#cb102-4" aria-hidden="true" tabindex="-1"></a>cv_specs <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>, <span class="at">repeats =</span> <span class="dv">5</span>)</span>
<span id="cb102-5"><a href="k-nearest-neighbors-classifier.html#cb102-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-6"><a href="k-nearest-neighbors-classifier.html#cb102-6" aria-hidden="true" tabindex="-1"></a><span class="co"># specify grid of &#39;k&#39; values to search over</span></span>
<span id="cb102-7"><a href="k-nearest-neighbors-classifier.html#cb102-7" aria-hidden="true" tabindex="-1"></a>k_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">100</span>, <span class="at">by =</span> <span class="dv">1</span>))</span>
<span id="cb102-8"><a href="k-nearest-neighbors-classifier.html#cb102-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-9"><a href="k-nearest-neighbors-classifier.html#cb102-9" aria-hidden="true" tabindex="-1"></a><span class="co"># train the KNN model to find optimal &#39;k&#39;</span></span>
<span id="cb102-10"><a href="k-nearest-neighbors-classifier.html#cb102-10" aria-hidden="true" tabindex="-1"></a>knn_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">form =</span> mpg <span class="sc">~</span> horsepower, </span>
<span id="cb102-11"><a href="k-nearest-neighbors-classifier.html#cb102-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data =</span> Auto_train, </span>
<span id="cb102-12"><a href="k-nearest-neighbors-classifier.html#cb102-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb102-13"><a href="k-nearest-neighbors-classifier.html#cb102-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trControl =</span> cv_specs, </span>
<span id="cb102-14"><a href="k-nearest-neighbors-classifier.html#cb102-14" aria-hidden="true" tabindex="-1"></a>                 <span class="at">tuneGrid =</span> k_grid,</span>
<span id="cb102-15"><a href="k-nearest-neighbors-classifier.html#cb102-15" aria-hidden="true" tabindex="-1"></a>                 <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</span></code></pre></div>
<!-- ## <span style="color:blue">Your Turn!!!</span>  -->
<!-- ```{r} -->
<!-- knn_fit   # model training results -->
<!-- ``` -->
</div>
<div id="your-turn-observe-cv-results" class="section level2 hasAnchor" number="5.27">
<h2><span class="header-section-number">5.27</span> <span style="color:blue">Your Turn!!!</span>: Observe CV Results<a href="k-nearest-neighbors-classifier.html#your-turn-observe-cv-results" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Auto dataset</strong></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="k-nearest-neighbors-classifier.html#cb103-1" aria-hidden="true" tabindex="-1"></a>knn_cv   <span class="co"># model training results</span></span></code></pre></div>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="k-nearest-neighbors-classifier.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(knn_cv)   <span class="co"># plot the model training results for different &#39;k&#39;</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-75-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="your-turn-final-model" class="section level2 hasAnchor" number="5.28">
<h2><span class="header-section-number">5.28</span> <span style="color:blue">Your Turn!!!</span>: Final Model<a href="k-nearest-neighbors-classifier.html#your-turn-final-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Auto dataset</strong></p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="k-nearest-neighbors-classifier.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># final model with optimal &#39;k&#39; chosen from &#39;knn_fit&#39; results</span></span>
<span id="cb105-2"><a href="k-nearest-neighbors-classifier.html#cb105-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-3"><a href="k-nearest-neighbors-classifier.html#cb105-3" aria-hidden="true" tabindex="-1"></a>knn_cv<span class="sc">$</span>finalModel   <span class="co"># final model</span></span></code></pre></div>
<pre><code>## 16-nearest neighbor regression model</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="k-nearest-neighbors-classifier.html#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain predictions on test data</span></span>
<span id="cb107-2"><a href="k-nearest-neighbors-classifier.html#cb107-2" aria-hidden="true" tabindex="-1"></a>final_model_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(knn_cv, <span class="at">newdata =</span> Auto_test)</span>
<span id="cb107-3"><a href="k-nearest-neighbors-classifier.html#cb107-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-4"><a href="k-nearest-neighbors-classifier.html#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="co"># estimate prediction error using RMSE</span></span>
<span id="cb107-5"><a href="k-nearest-neighbors-classifier.html#cb107-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((Auto_test<span class="sc">$</span>mpg <span class="sc">-</span> final_model_preds)<span class="sc">^</span><span class="dv">2</span>))    <span class="co"># RMSE</span></span></code></pre></div>
<pre><code>## [1] 4.314033</code></pre>
</div>
<div id="mid-term-check" class="section level2 hasAnchor" number="5.29">
<h2><span class="header-section-number">5.29</span> Mid-Term Check<a href="k-nearest-neighbors-classifier.html#mid-term-check" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Are you comfortable with the concepts? Are you comfortable with the coding aspect? Explain.</p></li>
<li><p>What study habits have been working for you with this course? What habits havent worked?</p></li>
<li><p>Please mention any comments about the course in general (classwork, live coding, homework, quizzes, course structure and workflow, grading guidelines, etc.)</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-linear-regression-mlr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="review-of-cv.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/abhicc/208notes/edit/master/04-Slides4.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/abhicc/208notes/blob/master/04-Slides4.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
