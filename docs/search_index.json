[["index.html", "208 Course Notes Chapter 1 Introduction", " 208 Course Notes Abhishek Chakraborty 2023-03-21 Chapter 1 Introduction "],["what-is-machine-learning.html", "Chapter 2 What is Machine Learning? 2.1 What is Machine Learning? 2.2 Question!!! 2.3 Statistical Learning vs Machine Learning vs Data Science 2.4 Notations 2.5 Notations 2.6 Question!!! 2.7 Question!!! 2.8 Supervised vs Unsupervised 2.9 Supervised Learning 2.10 Supervised Learning 2.11 Unsupervised Learning 2.12 Question!!! 2.13 Supervised Learning 2.14 Supervised Learning 2.15 Supervised Learning 2.16 Supervised Learning 2.17 Supervised Learning: Why Estimate \\(f(\\mathbf{X})\\)? 2.18 Supervised Learning: Prediction and Inference 2.19 Supervised Learning: Prediction and Inference 2.20 Supervised Learning: Prediction 2.21 Question!!! 2.22 Supervised Learning: How Do We Estimate \\(f(\\mathbf{X})\\)? 2.23 Supervised Learning: Parametric Methods 2.24 Supervised Learning: Parametric Methods 2.25 Supervised Learning: Non-parametric Methods 2.26 Supervised Learning: Flexibility of Models 2.27 Supervised Learning: Some Trade-offs 2.28 Supervised Learning: Some Trade-offs", " Chapter 2 What is Machine Learning? Machine Learning is the study of tools/techniques for understanding complex datasets. The name machine learning was coined in 1959 by Arthur Samuel. “Field of study that gives computers the ability to learn without being explicitly programmed.” ## ── Attaching packages ────────────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.0 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ───────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 2.1 What is Machine Learning? Tom M. Mitchell (1998) defined algorithms studied in the machine learning field as “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.” 2.2 Question!!! Suppose your email program watches which emails you do or do not mark as spam, and based on that learns how to better filter spam. According to Tom Mitchell’s definition, what is the task T, experience E, and performance measure P in this setting? The number (or fraction) of emails correctly classified as spam/ham. Classifying emails as spam or ham (not spam) Watching you label emails as spam or ham. 2.3 Statistical Learning vs Machine Learning vs Data Science Machine learning arose as a subfield of Artificial Intelligence. Statistical learning arose as a subfield of Statistics. There is much overlap, a great deal of “cross-fertilization”. “Data Science” - Reflects the fact that both statistical and machine learning are about data. “Machine learning” or “Data Science” are “fancier” terms. 2.4 Notations Matrices - Bold, Upper-case \\(\\mathbf{X}\\) Vectors - Bold, Lower-case \\(\\mathbf{x}\\) Scalars - Normal, Lower-case \\(x\\) Random Variables - Normal, Upper-case \\(X\\) No. of data points/observations - \\(n\\) No. of variables - \\(p\\) 2.5 Notations Figure 2.1: A matrix of dimension n x p Figure 2.2: A n-dimensional vector Figure 2.3: Matrix 2.6 Question!!! Suppose you are given the following feature matrix. \\[ \\mathbf{X}=\\begin{pmatrix} 8.5 &amp; 11.2 &amp; 7.0 &amp; 9.3 \\\\ 8.0 &amp; 11.5 &amp; 13.1 &amp; 7.4 \\\\ 6.4 &amp; 9.6 &amp; 7.0 &amp; 6.8 \\\\ 9.5 &amp; -3.2 &amp; 14.4 &amp; 1.6 \\end{pmatrix} \\] What are the corresponding values of \\(n\\) and \\(p\\)? What will be the dimension of the corresponding response vector \\(\\mathbf{y}\\)? What is the value of the 3rd feature for the 2nd observation? 2.7 Question!!! Suppose you have information about 867 cancer patients on their age, tumor size, clump thickness of the tumor, uniformity of cell size, and whether the tumor is malignant or benign. Based on these data, you are interested in building a model to predict the type of tumor (malignant or benign) for future cancer patients. What are the values of \\(n\\) and \\(p\\) in this dataset? What are the inputs/features? 2.8 Supervised vs Unsupervised Figure 2.4: Machine Learning Tasks 2.9 Supervised Learning Labeled training data Inputs/Features/Regressors/Covariates/Independent Variables Response/Target/Dependent Variable 2.10 Supervised Learning The objective is to learn the overall pattern of the relationship between the inputs (\\(\\mathbf{X}\\)) and response (\\(\\mathbf{y}\\)) in order to Investigate the relationship between inputs and response. Predict for potential unseen test cases. Assess the quality of predictions. Supervised Learning problems can be categorized into Regression problems (response is quantitative, continuous) Classification problems (response is qualitative, categorical) 2.11 Unsupervised Learning No outcome variable, just \\(\\mathbf{X}\\). Understand structure within data. find similar groups of observations based on features (clustering) find a smaller subset of features with the most variation (dimensionality reduction) No gold-standard. Easier to collect unlabeled data. Useful pre-processing step for supervised learning. 2.12 Question!!! Some of the problems below are best addressed using a supervised learning algorithm, while others with an unsupervised learning algorithm. In each case, identify whether the problem belongs to the supervised or unsupervised learning paradigm. (Assume some appropriate dataset is available for your algorithm to “learn” from.) Examine the statistics of two football teams, and predict which team will win tomorrow’s match (given historical data of teams’ wins/losses to learn from). Given genetic (DNA) data from a person, predict the odds of the person developing diabetes over the next 10 years. Take a collection of 1000 essays written on the US economy, and find a way to automatically group these essays into a small number of groups of essays that are somehow “similar” or “related”. Examine a large collection of emails that are known to be spam, to discover if there are sub-types of spam email. Suppose you have information about 867 cancer patients on their age, tumor size, clump thickness of the tumor, uniformity of cell size, and whether the tumor is malignant or benign. Based on these data, you are interested in building a model to predict the type of tumor (malignant or benign) for future cancer patients. Examine data on the income and years of education of adults in a neighborhood and build a model to predict the income from years of education. 2.13 Supervised Learning More mathematically, the “true”/population model can be represented by \\[Y=f(\\mathbf{X}) + \\epsilon\\] where \\(\\epsilon\\) is a random error term (includes measurement error, other discrepancies) independent of \\(\\mathbf{X}\\) and has mean zero. 2.14 Supervised Learning The primary objective is to: Regression: response \\(Y\\) is quantitative Build a model \\(\\hat{Y} = \\hat{f}(\\mathbf{X})\\) Classification: response \\(Y\\) is qualitative Build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\) 2.15 Supervised Learning Income dataset 2.16 Supervised Learning Income dataset 2.17 Supervised Learning: Why Estimate \\(f(\\mathbf{X})\\)? We wish to know about \\(f(\\mathbf{X})\\) for two reasons: Prediction at new unseen data points \\(x_0\\) \\[\\hat{y}_0=\\hat{f}(x_0) \\ \\ \\ \\text{or} \\ \\ \\ \\hat{y}_0=\\hat{C}(x_0)\\] Inference: Understand the relationship between \\(\\mathbf{X}\\) and \\(Y\\). An ML algorithm that is developed mainly for predictive purposes is often termed as a Black Box algorithm. 2.18 Supervised Learning: Prediction and Inference Income dataset 2.19 Supervised Learning: Prediction and Inference Income dataset 2.20 Supervised Learning: Prediction When we estimate \\(f(\\mathbf{X})\\) using \\(\\hat{f}(\\mathbf{X})\\), then, \\[E\\left[Y-\\hat{Y}\\right]^2=E\\left[f(\\mathbf{X})+\\epsilon - \\hat{f}(\\mathbf{X})\\right]^2=\\underbrace{\\left[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})\\right]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}\\] \\(E\\left[Y-\\hat{Y}\\right]^2\\): Expected (average) squared difference between predicted and actual (observed) response. We will focus on techniques for estimating \\(f(\\mathbf{X})\\) with the objective of minimizing the reducible error. 2.21 Question!!! Which of the following statements are true for the random error term \\(\\epsilon\\) in the expression \\(Y=f(\\mathbf{X})+\\epsilon\\)? \\(\\epsilon\\) depends on \\(\\mathbf{X}\\) and has mean zero. \\(Var(\\epsilon)\\) is also known as the irreducible error. \\(\\epsilon\\) is independent of \\(\\mathbf{X}\\) and has mean zero. \\(\\epsilon\\) is some fixed but unknown function of \\(\\mathbf{X}\\). 2.22 Supervised Learning: How Do We Estimate \\(f(\\mathbf{X})\\)? Broadly speaking, we have two approaches. Parametric and Structured Methods A functional form of \\(f(\\mathbf{X})\\) is assumed, such as \\[f(\\mathbf{X})=\\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\ldots + \\beta_p \\mathbf{x}_p\\] We estimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) by fitting the model to labeled training data. 2.23 Supervised Learning: Parametric Methods Income dataset 2.24 Supervised Learning: Parametric Methods Income dataset Figure 2.5: Linear Model Fit to Income Data \\[\\text{Income} \\approx \\beta_0 + \\beta_1 \\times \\text{Years of Education} + \\beta_2 \\times \\text{Seniority}\\] 2.25 Supervised Learning: Non-parametric Methods Non-parametric approaches do not make any explicit assumptions about the functional form of \\(f(\\mathbf{X})\\). A very large number of observations (compared to a parametric approach) is required to fit a model using the non-parametric approach. Income dataset Figure 2.6: Smooth Thin-plate Spline Fit to Income Data 2.26 Supervised Learning: Flexibility of Models Flexibility refers to the smoothness of functions. (More theoretically, flexibility depends on the number of parameters of the function). More flexible \\(\\implies\\) More complex \\(\\implies\\) Less Smooth \\(\\implies\\) Less Restrictive \\(\\implies\\) Less Interpretable 2.27 Supervised Learning: Some Trade-offs Prediction Accuracy versus Interpretability Good Fit versus Over-fit or Under-fit 2.28 Supervised Learning: Some Trade-offs "],["supervised-learning-assessing-model-accuracy.html", "Chapter 3 Supervised Learning: Assessing Model Accuracy 3.1 Supervised Learning: Assessing Model Accuracy 3.2 Supervised Learning: Assessing Model Accuracy 3.3 Supervised Learning: Assessing Model Accuracy 3.4 Supervised Learning: Assessing Model Accuracy 3.5 Supervised Learning: Assessing Model Accuracy 3.6 Supervised Learning: Assessing Model Accuracy 3.7 Supervised Learning: Bias-Variance Trade-off 3.8 Supervised Learning: Bias-Variance Trade-off 3.9 Supervised Learning: Bias-Variance Trade-off 3.10 Question!!! 3.11 Simple Linear Regression (SLR) 3.12 Question!!! 3.13 SLR: Estimating Parameters 3.14 SLR: Estimating Parameters 3.15 SLR: Estimating Parameters 3.16 Ames Housing Dataset 3.17 Ames Housing dataset 3.18 SLR: Estimating Parameters 3.19 SLR: Model 3.20 SLR: Model 3.21 SLR: Prediction 3.22 SLR: Interpreting Parameters 3.23 SLR: Assessing Accuracy of Model 3.24 SLR: Assessing Accuracy of Model 3.25 Your Turn!!! 3.26 Question!!! 3.27 Question!!! 3.28 Question!!! 3.29 Regression: Conditional Averaging 3.30 Regression: Conditional Averaging 3.31 K-Nearest Neighbors Regression 3.32 K-Nearest Neighbors Regression: Fit 3.33 K-Nearest Neighbors Regression: Prediction 3.34 Regression Methods: Comparison 3.35 Your Turn!!! 3.36 Question!!!", " Chapter 3 Supervised Learning: Assessing Model Accuracy Why are we going to study so many different ML techniques? There is no free lunch in statistics: No one method dominates all others over all possible datasets. 3.1 Supervised Learning: Assessing Model Accuracy Suppose we have labeled training data \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\), i.e, \\(n\\) training data points/observations. We fit/train a model \\(\\hat{y}=\\hat{f}(x)\\) (or, a classifier \\(\\hat{y}=\\hat{C}(x)\\)) on the training data and obtain estimates \\(\\hat{f}(x_1), \\hat{f}(x_2), \\ldots, \\hat{f}(x_n)\\) (or, \\(\\hat{C}(x_1), \\hat{C}(x_2), \\ldots, \\hat{C}(x_n)\\)). We could then compute the Regression \\[\\text{Training MSE}=\\text{Average}_{Training} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\left(y_i-\\hat{f}(x_i)\\right)^2\\] Classification \\[\\text{Training Error Rate}=\\text{Average}_{Training} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]= \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\ I\\left(y_i \\ne \\hat{C}(x_i)\\right)\\] 3.2 Supervised Learning: Assessing Model Accuracy But in general, we are not interested in how the method works on the training data. We want to measure the accuracy of the method on previously unseen test data. Suppose, if possible, we have fresh test data, \\((x_1^{test},y_1^{test}), (x_2^{test},y_2^{test}), \\ldots, (x_m^{test},y_m^{test})\\). Then we can compute, Regression \\[\\text{Test MSE}=\\text{Average}_{Test} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\left(y_i^{test}-\\hat{f}(x_i^{test})\\right)^2\\] Classification \\[\\text{Test Error Rate}=\\text{Average}_{Test} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]= \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\ I\\left(y_i^{test} \\ne \\hat{C}(x_i^{test})\\right)\\] 3.3 Supervised Learning: Assessing Model Accuracy In the following slides, we look at three different examples with simulated toy datasets. We work within the regression setting (but the ideas also extend to the classification setting) and three different \\(\\hat{f}(.)\\)’s. Linear Regression (Orange) Smoothing Spline 1 (Blue) More flexible Smoothing Spline 2 (Green) The “true” function (simulated) is \\(f(.)\\) (black). 3.4 Supervised Learning: Assessing Model Accuracy Simulated toy dataset 3.5 Supervised Learning: Assessing Model Accuracy Simulated toy dataset 3.6 Supervised Learning: Assessing Model Accuracy Simulated toy dataset 3.7 Supervised Learning: Bias-Variance Trade-off Why is the Test MSE U-shaped? Suppose we have fit a model \\(\\hat{f}(x)\\) to some training data. Let the “true” model be \\(Y=f(x)+\\epsilon\\). Let \\((x_0, y_0)\\) be a test observation. We have, \\[\\underbrace{E\\left(y_0-\\hat{f}(x_0)\\right)^2}_{total \\ error}=\\underbrace{Var\\left(\\hat{f}(x_0)\\right)}_{source \\ 3} + \\underbrace{\\left[Bias\\left(\\hat{f}(x_0)\\right)\\right]^2}_{source \\ 2}+\\underbrace{Var(\\epsilon)}_{source \\ 1}\\] where \\(Bias\\left(\\hat{f}(x_0)\\right)=E\\left(\\hat{f}(x_0)\\right)-f(x_0)\\) 3.8 Supervised Learning: Bias-Variance Trade-off source 1: how \\(y\\) differs from “true” \\(f(x)\\) source 2: how \\(\\hat{f}(x)\\) (when fitted to the test data) differs from \\(f(x)\\) source 3: how \\(\\hat{f}(x)\\) varies among different randomly selected possible training data 3.9 Supervised Learning: Bias-Variance Trade-off 3.10 Question!!! As the flexibility of a model \\(\\hat{f}(\\mathbf{X})\\) increases, its variance \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) its bias \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) its training MSE \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) its test MSE \\(\\underline{\\hspace{5cm}}\\) (increases/decreases/U-shaped) 3.11 Simple Linear Regression (SLR) Response \\(Y\\) and a single predictor variable \\(X\\). We assume \\[Y=f(\\mathbf{X}) + \\epsilon=\\beta_0 + \\beta_1 X+ \\epsilon\\] Parameters/Coefficients: \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope) Training data: \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\) We use training data to find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) such that \\[\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x\\] 3.12 Question!!! Linear regression is \\(\\underline{\\hspace{5cm}}\\) (supervised/unsupervised) \\(\\underline{\\hspace{5cm}}\\) (regression/classification) \\(\\underline{\\hspace{5cm}}\\) (parametric/non-parametric) 3.13 SLR: Estimating Parameters Training data: \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\) Observed response: \\(y_i\\) for \\(i=1,\\ldots,n\\) Predicted response: \\(\\hat{y}_i\\) for \\(i=1, \\ldots, n\\) Residual: \\(e_i=y_i - \\hat{y}_i\\) for \\(i=1, \\ldots, n\\) Residual Sum of Squares (RSS): \\(RSS =e^2_1+e^2_2+\\ldots+e^2_n\\) Problem: Find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimizes \\(RSS\\) 3.14 SLR: Estimating Parameters Figure 3.1: Three-dimensional plot of RSS 3.15 SLR: Estimating Parameters The least squares regression coefficient estimates are \\[\\hat{\\beta}_1=\\dfrac{\\displaystyle\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\displaystyle\\sum_{i=1}^n (x_i-\\bar{x})^2}\\] \\[\\hat{\\beta}_0=\\bar{y}- \\hat{\\beta}_1 \\ \\bar{x}\\] where \\(\\bar{y}=\\dfrac{1}{n} \\displaystyle\\sum_{i=1}^n y_i\\) and \\(\\bar{x}=\\dfrac{1}{n} \\displaystyle\\sum_{i=1}^n x_i\\). 3.16 Ames Housing Dataset Contains data on 881 properties in Ames, IA. ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # read in the dataset after specifying directory 3.17 Ames Housing dataset Variable descriptions: Sale_Price: Property sale price in USD Gr_Liv_Area: Above grade (ground) living area square feet Garage_Type: Garage location Garage_Cars: Size of garage in car capacity Garage_Area: Size of garage in square feet Street: Type of road access to property Utilities: Type of utilities available Pool_Area: Pool area in square feet Neighborhood: Physical locations within Ames city limits Screen_Porch: Screen porch area in square feet Overall_Qual: Rates the overall material and finish of the house Lot_Area: Lot size in square feet Lot_Frontage: Linear feet of street connected to property MS_SubClass: Identifies the type of dwelling involved in the sale. Misc_Val: Dollar value of miscellaneous feature Open_Porch_SF: Open porch area in square feet TotRms_AbvGrd: Total rooms above grade (does not include bathrooms) First_Flr_SF: First Floor square feet Second_Flr_SF: Second floor square feet Year_Built: Original construction date 3.18 SLR: Estimating Parameters Ames Housing dataset slrfit &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames) # fit the SLR model summary(slrfit) # produce result summaries of the SLR model ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Residuals: ## Min 1Q Median 3Q Max ## -496577 -33108 -3216 22644 321629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10546.305 6678.534 1.579 0.115 ## Gr_Liv_Area 114.504 4.221 27.127 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60480 on 766 degrees of freedom ## (113 observations deleted due to missingness) ## Multiple R-squared: 0.49, Adjusted R-squared: 0.4893 ## F-statistic: 735.9 on 1 and 766 DF, p-value: &lt; 2.2e-16 3.19 SLR: Model Ames Housing dataset We have, \\(\\hat{\\beta}_0=10546.305\\) and \\(\\hat{\\beta}_1=114.504\\). The least squares regression model is \\[\\widehat{\\text{Sale_Price}} = 10546.305 + 114.504 \\times \\text{Gr_Liv_Area}\\] 3.20 SLR: Model Ames Housing dataset ggplot(data = ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point() + # create scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE) # add the SLR line ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 113 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 113 rows containing missing values (`geom_point()`). 3.21 SLR: Prediction Ames Housing dataset For a house with Gr_Liv_Area equaling 1000 square feet, we have \\[\\widehat{\\text{Sale_Price}} = 10546.305 + 114.504 \\times \\text{Gr_Liv_Area} = 10546.305 + 114.504 \\times 1003 \\approx 125393.8 \\ \\text{USD}\\] predict(slrfit, newdata = data.frame(Gr_Liv_Area = 1003)) # predict response for a given value of x ## 1 ## 125393.6 The observed sale price of a property with Gr_Liv_Area equaling 1000 square feet is 142500 USD. Then, \\(\\text{residual} = \\text{observed}-\\text{predicted} \\approx 142500 - 125393.6 \\approx 17106.4\\) Note: We should not attempt to predict the response for a value of the predictor that lies outside the range of our data. This is called extrapolation, and the predictions tend to be unreliable. 3.22 SLR: Interpreting Parameters Ames Housing dataset \\(\\hat{\\beta}_0=10546.305\\): When Gr_Liv_Area is 0 square feet, the predicted sale price is approximately 10546.305 USD. \\(\\hat{\\beta}_1=114.504\\): For every 1 square foot increase in Gr_Liv_Area, Sale_Price is expected to increase by approximately 114.504 USD. 3.23 SLR: Assessing Accuracy of Model Residual Standard Error (RSE) \\[RSE=\\sqrt{\\dfrac{RSS}{n-2}}\\] RSE is considered as a measure of the lack of fit of the linear model to the data. It is the average amount that the response will deviate from the true regression line. It is measured in the units of the response variable. \\(R^2\\) statistic \\[R^2=\\dfrac{TSS-RSS}{TSS}\\] where \\(TSS=\\sum_{i=1}^n \\left(y_i-\\bar{y}\\right)^2\\) \\(R^2\\) measures the proportion of variability in the response that is explained by the linear regression model using the predictor variable. \\(R^2\\) is unitless, \\(0 &lt; R^2 &lt; 1\\), and is generally expressed as a percentage. 3.24 SLR: Assessing Accuracy of Model Ames Housing dataset summary(slrfit) # produce result summaries of the SLR model ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Residuals: ## Min 1Q Median 3Q Max ## -496577 -33108 -3216 22644 321629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10546.305 6678.534 1.579 0.115 ## Gr_Liv_Area 114.504 4.221 27.127 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60480 on 766 degrees of freedom ## (113 observations deleted due to missingness) ## Multiple R-squared: 0.49, Adjusted R-squared: 0.4893 ## F-statistic: 735.9 on 1 and 766 DF, p-value: &lt; 2.2e-16 3.25 Your Turn!!! The Advertising.csv dataset contains data on the sales (in 1000 units) of a product in 200 different markets, along with advertising budgets (in $1000) for the product for three different media: TV, radio, and newspaper. Create an SLR model slrfit1 with sales as response and TV as predictor. Display the least squares regression line on a scatterplot. Create another SLR model slrfit2 with sales as response and radio as predictor. Predict the sales when the radio advertising budgets are $20,000 and $40,000. Between slrfit1 and slrfit2, which model is better in terms of the variability explained within sales? Between TV and radio advertising budgets, which would result in a higher increase in sales for an additional $1000 investment? 3.26 Question!!! Consider the population model \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\). The estimated model is \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\). Reflecting on the concepts from week 1, fill in the blanks below. If \\(\\beta_0\\) and \\(\\beta_1\\) were known (“truth” known), the discrepancy between response \\(Y\\) and \\(\\beta_0 + \\beta_1 X\\) is related to the \\(\\underline{\\hspace{5cm}}\\) (irreducible/reducible) error. The inaccuracy of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) as estimates of \\(\\beta_0\\) and \\(\\beta_1\\) is related to the \\(\\underline{\\hspace{5cm}}\\) (irreducible/reducible) error. 3.27 Question!!! The least squares approach minimizes the sum of squared predictor values. minimizes the sum of squared response values. minimizes the sum of squared residuals. maximizes the sum of squared residuals. 3.28 Question!!! Geometrically, the residual for the \\(i^{th}\\) observation in a regression model is the horizontal distance between the observed response and the vertical axis. distance of the predicted response from the horizontal axis. distance of the predicted response from the vertical axis. vertical distance between the observed response and predicted response. 3.29 Regression: Conditional Averaging Ames Housing dataset ggplot(data = ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point() + # create scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE) # add the SLR line ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 113 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 113 rows containing missing values (`geom_point()`). What is a good value of \\(\\hat{f}(x)\\), say at \\(x=1008\\)? 3.30 Regression: Conditional Averaging What is a good value of \\(\\hat{f}(x)\\), say at \\(x=1008\\)? A possible value is \\[\\hat{f}(x)=E(Y|x=1008)\\] \\(E(Y|x=1008)\\) means expected value, or, the average of the observed responses at \\(x=1008\\). But we may not observe responses for certain \\(x\\) values. 3.31 K-Nearest Neighbors Regression Non-parametric approach Given a value for \\(K\\) and a test data point \\(x_0\\), \\[\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) \\] where \\(\\mathcal{N}_0\\) is known as the neighborhood of \\(x_0\\). The method is based on the concept of closeness of \\(x_i\\)’s from \\(x_0\\) for inclusion in the neighborhood \\(\\mathcal{N}_0\\). Usually, the Euclidean distance is used as a measure of closeness. The Euclidean distance between two \\(p\\)-dimensional vectors \\(\\mathbf{a}=(a_1, a_2, \\ldots, a_p)\\) and \\(\\mathbf{b}=(b_1, b_2, \\ldots, b_p)\\) is \\[||\\mathbf{a}-\\mathbf{b}||_2 = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \\ldots + (a_p-b_p)^2}\\] 3.32 K-Nearest Neighbors Regression: Fit Ames Housing dataset library(caret) # load the caret package knnfit1 &lt;- knnreg(Sale_Price ~ Gr_Liv_Area, data = ames, k = 1) # 1-nn regression knnfit5 &lt;- knnreg(Sale_Price ~ Gr_Liv_Area, data = ames, k = 5) # 5-nn regression 3.33 K-Nearest Neighbors Regression: Prediction Ames Housing dataset nearest_neighbors &lt;- ames %&gt;% select(Sale_Price, Gr_Liv_Area) %&gt;% mutate(distance = sqrt((1008-Gr_Liv_Area)^2)) %&gt;% # calculate distance arrange(distance) # sort by increasing distance predict(knnfit1, newdata = data.frame(Gr_Liv_Area = 1008)) # 1-nn prediction ## [1] 135166.7 predict(knnfit5, newdata = data.frame(Gr_Liv_Area = 1008)) # 5-nn prediction ## [1] 118280 3.34 Regression Methods: Comparison Ames Housing dataset ## Warning: Removed 113 rows containing missing values (`geom_point()`). Figure 3.2: dashed cyan: 1-nn fit, dotted red: 5-nn fit, blue: linear regression fit 3.35 Your Turn!!! For the Advertising.csv dataset, create a 10-nearest neighbors fit knnfit10 with sales as response and TV as predictor. Obtain the predicted sales for \\(\\text{TV} = 225,000\\). 3.36 Question!!! As \\(k\\) in KNN regression increases, the flexibility of the fit \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) the bias of the fit \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) the variance of the fit \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) "],["multiple-linear-regression-mlr.html", "Chapter 4 Multiple Linear Regression (MLR) 4.1 MLR: Estimating Parameters 4.2 MLR: Estimating Parameters 4.3 MLR: Estimating Parameters 4.4 MLR: Interpreting Parameters 4.5 MLR: Prediction 4.6 MLR: Assessing Accuracy of Model 4.7 Your Turn!!! 4.8 MLR: Assessing Accuracy of Model 4.9 Question!!! 4.10 K-Nearest Neighbors Regression (multiple predictors) 4.11 K-Nearest Neighbors Regression (multiple predictors) 4.12 Linear Regression vs K-Nearest Neighbors 4.13 Classification Problems 4.14 Classification Problems: Example 4.15 Classification Problems: Example 4.16 Why Not Linear Regression? 4.17 Why Not Linear Regression? 4.18 Logistic Regression 4.19 Logistic Regression 4.20 Your Turn!!! 4.21 Logistic Regression: Example 4.22 Logistic Regression: Estimating Parameters 4.23 Logistic Regression: Individual Predictions 4.24 Logistic Regression: Test Set Predictions 4.25 Logistic Regression: Test Set Predictions 4.26 Logistic Regression: Performance 4.27 Confusion Matrix Terms", " Chapter 4 Multiple Linear Regression (MLR) Response \\(Y\\) and more than one predictor variable. We assume \\[Y=f(\\mathbf{X}) + \\epsilon=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\\] \\(\\beta_j\\) quantifies the association between the \\(j^{th}\\) predictor and the response. 4.1 MLR: Estimating Parameters We use training data to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) such that \\[\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_1 + \\ldots + \\hat{\\beta}_p \\ x_p\\] Observed response: \\(y_i\\) for \\(i=1,\\ldots,n\\) Predicted response: \\(\\hat{y}_i\\) for \\(i=1, \\ldots, n\\) Residual: \\(e_i=y_i - \\hat{y}_i\\) for \\(i=1, \\ldots, n\\) Residual Sum of Squares (RSS): \\(RSS =e^2_1+e^2_2+\\ldots+e^2_n\\) Problem: Find \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) which minimizes \\(RSS\\) 4.2 MLR: Estimating Parameters 4.3 MLR: Estimating Parameters Ames Housing dataset ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # read in the dataset after specifying directory mlrfit &lt;- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames) # fit the MLR model summary(mlrfit) # produce result summaries of the MLR model ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames) ## ## Residuals: ## Min 1Q Median 3Q Max ## -469355 -29029 -590 18691 301689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.242e+06 1.234e+05 -18.17 &lt;2e-16 *** ## Gr_Liv_Area 9.781e+01 3.641e+00 26.87 &lt;2e-16 *** ## Year_Built 1.155e+03 6.322e+01 18.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 50500 on 765 degrees of freedom ## (113 observations deleted due to missingness) ## Multiple R-squared: 0.6449, Adjusted R-squared: 0.644 ## F-statistic: 694.7 on 2 and 765 DF, p-value: &lt; 2.2e-16 4.4 MLR: Interpreting Parameters Ames Housing dataset \\(\\hat{\\beta}_0=-2.242e+06\\): With Gr_Liv_Area equaling 0 square feet, and Year_Built equaling 0, the predicted Sale_Price is approximately -2.242e+06 USD. The interpretation is not meaningful in this context. \\(\\hat{\\beta}_1=9.781e+01\\): With Year_Built remaining fixed, an additional 1 square foot of Gr_Liv_Area leads to an increase in Sale_Price by approximately 98 USD. \\(\\hat{\\beta}_2=1.155e+03\\): With Gr_Liv_Area remaining fixed, an additional 1 year on Year_Built leads to an increase in Sale_Price by approximately 1155 USD. 4.5 MLR: Prediction Ames Housing dataset Prediction of Sale_Price when Gr_Liv_Area is 1000 SF for a house built in 1990. predict(mlrfit, newdata = data.frame(Gr_Liv_Area = 1000, Year_Built = 1990)) # obtain prediction ## 1 ## 154506.3 4.6 MLR: Assessing Accuracy of Model Residual Standard Error \\[RSE=\\sqrt{\\dfrac{RSS}{n-p-1}}\\] \\(R^2\\) statistic \\[R^2=\\dfrac{TSS-RSS}{TSS} = 1 - \\dfrac{RSS}{TSS}\\] Adjusted \\(R^2\\) statistic \\[\\text{Adjusted} \\ R^2 = 1 - \\dfrac{RSS/(n-p-1)}{TSS/(n-1)}\\] 4.7 Your Turn!!! With the Advertising dataset, create two additional models with sales as response: mlrfit1: MLR model with TV and radio as predictors mlrfit2: MLR model with TV, radio, and newspaper as predictors For each model, note \\(p\\) (the number of predictors), \\(R^2\\), \\(\\text{Adjusted} \\ R^2\\), \\(RSS\\), and \\(RSE\\). 4.8 MLR: Assessing Accuracy of Model cor(advertising) # obtain correlation matrix ## TV radio newspaper sales ## TV 1.00000000 0.05480866 0.05664787 0.7822244 ## radio 0.05480866 1.00000000 0.35410375 0.5762226 ## newspaper 0.05664787 0.35410375 1.00000000 0.2282990 ## sales 0.78222442 0.57622257 0.22829903 1.0000000 4.9 Question!!! As we add variables to the linear regression model, (Select all that apply) the RSE always decreases. the RSS always decreases. the \\(R^2\\) always increases. the \\(\\text{Adjusted} \\ R^2\\) always increases. the number of parameters always increases. 4.10 K-Nearest Neighbors Regression (multiple predictors) It is important to scale (subtract mean and divide by standard deviation) the predictors when considering KNN regression so that the Euclidean distance is not dominated by a few of them with large values. Ames Housing dataset ames_scaled &lt;- ames %&gt;% dplyr::select(Sale_Price, Gr_Liv_Area, Year_Built) %&gt;% # select required variables mutate(Gr_Liv_Area_scaled = scale(Gr_Liv_Area), Year_Built_scaled = scale(Year_Built)) # scale predictors head(ames_scaled) ## # A tibble: 6 × 5 ## Sale_Price Gr_Liv_Area Year_Built Gr_Liv_Area_scaled[,1] Year_Built_scaled[,1] ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 244000 2110 1968 1.19 -0.102 ## 2 213500 1338 2001 -0.304 0.985 ## 3 185000 1187 1992 -0.596 0.688 ## 4 394432 1856 2010 0.697 1.28 ## 5 190000 1844 1977 0.674 0.194 ## 6 149000 NA 1970 NA -0.0359 4.11 K-Nearest Neighbors Regression (multiple predictors) Ames Housing dataset library(caret) # load library knnfit10 &lt;- knnreg(Sale_Price ~ Gr_Liv_Area_scaled + Year_Built_scaled, data = ames_scaled, k = 10) # 10-nn regression It is also important to apply scaling to test data points before prediction. Suppose, you want predictions for Gr_Liv_Area = 1000 SF, and Year_Built = 1990, then # obtain 10-nn prediction predict(knnfit10, newdata = data.frame(Gr_Liv_Area_scaled = (1000 - mean(ames$Gr_Liv_Area, na.rm = TRUE))/sd(ames$Gr_Liv_Area, na.rm = TRUE), Year_Built_scaled = (1990 - mean(ames$Year_Built))/sd(ames$Year_Built))) ## [1] 148850 4.12 Linear Regression vs K-Nearest Neighbors Linear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric. Linear regression works for regression problems (\\(Y\\) numerical), KNN can be used for both regression and classification (\\(Y\\) qualitative). Linear regression is interpretable, KNN is not. Linear regression can accommodate qualitative predictors and can be extended to include interaction terms as well. Using Euclidean distance with KNN does not allow for qualitative predictors. In terms of prediction, KNN can be pretty good for small \\(p\\), that is, \\(p \\le 4\\) and large \\(n\\). Performance of KNN deteriorates as \\(p\\) increases - curse of dimensionality. 4.13 Classification Problems Response \\(Y\\) is qualitative (categorical). The objective is to build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\) that assigns a class label to a future unlabeled (unseen) observation and understand the relationship between the predictors and response. There can be two types of predictions based on the research problem. Class probabilities Class labels 4.14 Classification Problems: Example Default dataset ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 table(Default$default) ## ## No Yes ## 9667 333 4.15 Classification Problems: Example For some algorithms, we might need to convert the categorical response to numeric values. Default dataset Default$default_id &lt;- ifelse(Default$default == &quot;Yes&quot;, 1, 0) # create 0/1 variable head(Default, 10) # print first ten observations ## default student balance income default_id ## 1 No No 729.5265 44361.625 0 ## 2 No Yes 817.1804 12106.135 0 ## 3 No No 1073.5492 31767.139 0 ## 4 No No 529.2506 35704.494 0 ## 5 No No 785.6559 38463.496 0 ## 6 No Yes 919.5885 7491.559 0 ## 7 No No 825.5133 24905.227 0 ## 8 No Yes 808.6675 17600.451 0 ## 9 No No 1161.0579 37468.529 0 ## 10 No No 0.0000 29275.268 0 4.16 Why Not Linear Regression? Default dataset slrfit &lt;- lm(default_id ~ balance, data = Default) # fit SLR summary(slrfit$fitted.values) # summary of y_hats ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.07519 -0.01263 0.03178 0.03330 0.07628 0.26953 ## `geom_smooth()` using formula = &#39;y ~ x&#39; Linear regression does not model probabilities well. Linear regression might produce probabilities less than zero or bigger than one. 4.17 Why Not Linear Regression? Suppose we have a response \\(Y\\), \\(Y=\\begin{cases} 1; &amp; \\text{if stroke} \\\\ 2; &amp; \\text{if drug overdose} \\\\ 3; &amp; \\text{if epileptic seizure} \\end{cases}\\) Linear regression suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure. 4.18 Logistic Regression Consider a one-dimensional two-class problem. Transform the linear model \\(\\beta_0 + \\beta_1 \\ X\\) so that the output is a probability. Use logistic or sigmoid function \\[g(t)=\\dfrac{e^t}{1+e^t} \\ \\ \\ \\text{for} \\ t \\in \\mathcal{R}\\] Suppose \\(p(X)=P(Y=1|X)\\). Then, \\[p(X)=g\\left(\\beta_0 + \\beta_1 \\ X\\right)=\\dfrac{e^{\\beta_0 + \\beta_1 \\ X}}{1+e^{\\beta_0 + \\beta_1 \\ X}}\\] \\(e \\approx 2.71828\\) is a mathematical constant (Euler’s number). 4.19 Logistic Regression Default dataset 4.20 Your Turn!!! Consider \\(p(X)=P(Y=1|X) = \\dfrac{e^{\\beta_0 + \\beta_1 \\ X}}{1+e^{\\beta_0 + \\beta_1 \\ X}}\\). Find \\(1-p(X)\\) \\(\\ln \\left(\\dfrac{p(X)}{1-p(X)}\\right)\\) 4.21 Logistic Regression: Example Attrition dataset library(modeldata) # load library data(&quot;attrition&quot;) # load dataset We will consider Attrition as the response variable. To mimic real-world ML practices, we will split the dataset into a tranining and test set. We will build our model on the training set and evaluate its performance on the test set. set.seed(011723) # fix the random number generator for reproducibility library(caret) # load library train_index &lt;- createDataPartition(y = attrition$Attrition, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets attrition_train &lt;- attrition[train_index,] # training data, use this dataset to build model attrition_test &lt;- attrition[-train_index,] # test data, use this dataset to evaluate model&#39;s performance 4.22 Logistic Regression: Estimating Parameters Attrition dataset Let’s build a logistic regression model with MonthlyIncome as the predictor. logregfit &lt;- glm(Attrition ~ MonthlyIncome, data = attrition_train, family = binomial) # fit logistic regression model summary(logregfit) # obtain results ## ## Call: ## glm(formula = Attrition ~ MonthlyIncome, family = binomial, data = attrition_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.7776 -0.6676 -0.5782 -0.3121 2.6570 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.040e-01 1.441e-01 -6.272 3.56e-10 *** ## MonthlyIncome -1.307e-04 2.413e-05 -5.418 6.04e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1040.5 on 1176 degrees of freedom ## Residual deviance: 1001.5 on 1175 degrees of freedom ## AIC: 1005.5 ## ## Number of Fisher Scoring iterations: 5 4.23 Logistic Regression: Individual Predictions Attrition dataset For MonthlyIncome=$5000, \\[\\hat{p}(X)=\\dfrac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}=\\dfrac{e^{-0.904 + (-0.0001307 \\times 5000)}}{1+e^{-0.904 + (-0.0001307 \\times 5000)}}=0.174\\] predict(logregfit, newdata = data.frame(MonthlyIncome = 5000)) # obtain log-odds predictions ## 1 ## -1.557622 predict(logregfit, newdata = data.frame(MonthlyIncome = 5000), type = &quot;response&quot;) # obtain probability predictions ## 1 ## 0.1739881 4.24 Logistic Regression: Test Set Predictions Attrition dataset To predict probabilities for observations in the test set, we use logreg_prob_preds &lt;- predict(logregfit, newdata = attrition_test, type = &quot;response&quot;) # obtain probability predictions head(logreg_prob_preds) # predicted probabilities for first six observations in test set ## 1 13 15 18 19 39 ## 0.1561133 0.1695803 0.1896750 0.2223808 0.2370187 0.2261332 4.25 Logistic Regression: Test Set Predictions Attrition dataset Set a threshold to obtain predicted class labels. The following uses a threshold of 0.5. threshold &lt;- 0.5 # set threshold logreg_class_preds &lt;- factor(ifelse(logreg_prob_preds &gt; threshold, &quot;Yes&quot;, &quot;No&quot;)) # obtain class predictions 4.26 Logistic Regression: Performance Attrition dataset library(caret) # load package &#39;caret&#39; # create confusion matrix levels(logreg_class_preds) = c(&quot;No&quot;, &quot;Yes&quot;) # create &#39;Yes&#39; factor level (not always required) confusionMatrix(data = relevel(logreg_class_preds, ref = &quot;Yes&quot;), reference = relevel(attrition_test$Attrition, ref = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 0 0 ## No 47 246 ## ## Accuracy : 0.8396 ## 95% CI : (0.7925, 0.8797) ## No Information Rate : 0.8396 ## P-Value [Acc &gt; NIR] : 0.5388 ## ## Kappa : 0 ## ## Mcnemar&#39;s Test P-Value : 1.949e-11 ## ## Sensitivity : 0.0000 ## Specificity : 1.0000 ## Pos Pred Value : NaN ## Neg Pred Value : 0.8396 ## Prevalence : 0.1604 ## Detection Rate : 0.0000 ## Detection Prevalence : 0.0000 ## Balanced Accuracy : 0.5000 ## ## &#39;Positive&#39; Class : Yes ## 4.27 Confusion Matrix Terms Reference class labels 1 0 ———– ———- ———– 1 0 "],["k-nearest-neighbors-classifier.html", "Chapter 5 K-Nearest Neighbors Classifier 5.1 Your Turn!!! 5.2 K-Nearest Neighbors Classifier: Split Data 5.3 K-Nearest Neighbors Classifier: Build Model 5.4 K-Nearest Neighbors Classifier: Predictions 5.5 K-Nearest Neighbors Classifier: Performance 5.6 ROC Curve and AUC 5.7 ROC Curve and AUC 5.8 Data Splitting 5.9 Resampling Methods 5.10 Cross-Validation (CV) 5.11 Leave-One-Out Cross-Validation (LOOCV) 5.12 Leave-One-Out Cross-Validation (LOOCV) 5.13 \\(k\\)-Fold Cross-Validation 5.14 \\(k\\)-Fold Cross-Validation: Implementation 5.15 \\(k\\)-Fold Cross-Validation: Implementation 5.16 \\(k\\)-Fold Cross-Validation: Implementation 5.17 \\(k\\)-Fold Cross-Validation: Implementation 5.18 \\(k\\)-Fold Cross-Validation: Implementation 5.19 \\(k\\)-Fold Cross-Validation: Implementation 5.20 \\(k\\)-Fold Cross-Validation: Results 5.21 Final Model and Prediction Error Estimate 5.22 Variable Importance 5.23 Bias-Variance Trade-off for LOOCV and \\(k\\)-fold CV 5.24 Your Turn!!! 5.25 Your Turn!!!: Split Data 5.26 Your Turn!!!: Perform CV 5.27 Your Turn!!!: Observe CV Results 5.28 Your Turn!!!: Final Model 5.29 Mid-Term Check", " Chapter 5 K-Nearest Neighbors Classifier Given a value for \\(K\\) and a test data point \\(x_0\\), \\[P(Y=j | X=x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} I(y_i = j)\\] where \\(\\mathcal{N}_0\\) is known as the neighborhood of \\(x_0\\). 5.1 Your Turn!!! Build a 10-NN classifier with the Default dataset. Consider default as the response variable and balance as the predictor. Follow the steps below. Load the dataset into R. Observe the dataset, specifically the response variable. Perform a 70-30 split of the original dataset. With the training data, construct a 10-NN classifier. See help page of function knn3. Predict class probabilities on the test data points. See help page of function predict.knn3. Look carefully at the object that is created. Obtain predicted class labels for a threshold of 0.3. Create the confusion matrix between the observed and predicted class labels. 5.2 K-Nearest Neighbors Classifier: Split Data Default dataset library(ISLR2) # load library data(&quot;Default&quot;) # load dataset set.seed(012423) # fix the random number generator for reproducibility library(caret) # load library train_index &lt;- createDataPartition(y = Default$default, p = 0.7, list = FALSE) # split available data into 70% training and 30% test datasets Default_train &lt;- Default[train_index,] # training data, use this dataset to build model Default_test &lt;- Default[-train_index,] # test data, use this dataset to evaluate model&#39;s performance 5.3 K-Nearest Neighbors Classifier: Build Model Default dataset library(caret) # load package &#39;caret&#39; knnfit &lt;- knn3(default ~ balance, data = Default_train, k = 10) # fit 10-nn model 5.4 K-Nearest Neighbors Classifier: Predictions Default dataset knn_prob_preds &lt;- predict(knnfit, newdata = Default_test, type = &quot;prob&quot;) # obtain predictions as probabilities threshold &lt;- 0.3 # set threshold knn_class_preds &lt;- factor(ifelse(knn_prob_preds[,2] &gt; threshold, &quot;Yes&quot;, &quot;No&quot;)) # obtain predictions as class labels 5.5 K-Nearest Neighbors Classifier: Performance Default dataset # create confusion matrix confusionMatrix(data = relevel(knn_class_preds, ref = &quot;Yes&quot;), reference = relevel(Default_test$default, ref = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 39 34 ## No 60 2866 ## ## Accuracy : 0.9687 ## 95% CI : (0.9618, 0.9746) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 0.327332 ## ## Kappa : 0.4377 ## ## Mcnemar&#39;s Test P-Value : 0.009922 ## ## Sensitivity : 0.39394 ## Specificity : 0.98828 ## Pos Pred Value : 0.53425 ## Neg Pred Value : 0.97949 ## Prevalence : 0.03301 ## Detection Rate : 0.01300 ## Detection Prevalence : 0.02434 ## Balanced Accuracy : 0.69111 ## ## &#39;Positive&#39; Class : Yes ## 5.6 ROC Curve and AUC The ROC (Receiver Operating Characteristics) curve is a popular graphic for comparing different classifiers across all possible thresholds. The ROC curve plots the Specificity (1-false positive rate) along the x-axis and the Sensitivity (true positive rate) along the y-axis. Another popular metric for comparing classifiers is the AUC (Area Under the ROC Curve). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. 5.7 ROC Curve and AUC Default dataset library(pROC) # load library ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var # create object for ROC curve for KNN fit roc_object_knn &lt;- roc(response = Default_test$default, predictor = knn_prob_preds[,2]) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases # plot ROC curve plot(roc_object_knn, col = &quot;red&quot;) # obtain AUC&#39;s auc(roc_object_knn) ## Area under the curve: 0.8564 5.8 Data Splitting Available data split into training and test datasets. Training set: these data are used to develop feature sets, train our algorithms, tune parameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production). Test set: having chosen a final model, these data are used to obtain an unbiased estimate of the model’s performance. It is critical that the test set not be used prior to selecting your final model. Assessing results on the test set prior to final model selection biases the model selection process since the testing data will have become part of the model development process. 5.9 Resampling Methods Idea: Repeatedly draw samples from the training data and refit a model on each sample, and evaluate its performance on the other parts. Objective: To obtain additional information about the fitted model. Cross-Validation (CV) is probably the most widely used resampling method. It is a general approach that can be applied to almost any statistical learning method. 5.10 Cross-Validation (CV) Used for model selection: select the optimum level of flexibility (tune hyperparameters) or compare different models to choose the best one model assessment: evaluate the performance of a model (estimate its test error) We will talk about Leave-One-Out Cross-Validation (LOOCV) \\(k\\)-Fold Cross-Validation 5.11 Leave-One-Out Cross-Validation (LOOCV) 5.12 Leave-One-Out Cross-Validation (LOOCV) Advantages LOOCV will give approximately unbiased estimates of the test error, since each training set contains \\(n−1\\) observations, which is almost as many as the number of observations in the full training dataset. Performing LOOCV multiple times will always yield the same results. Disadvantages Can be potentially expensive to implement, specially for large \\(n\\). LOOCV error estimate can have high variance. 5.13 \\(k\\)-Fold Cross-Validation Randomly divide the training data into \\(k\\) groups or folds (approximately equal size). Consider one of these folds as the validation set. Fit the model on the remaining \\(k-1\\) folds combined, and obtain predictions for the \\(k^{th}\\) fold. Repeat for all \\(k\\) folds. 5.14 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # load dataset Consider Sale_Price as the response variable. We will compare the following three linear regression models: with Garage_Area as the only predictor; with Overall_Qual as the only predictor; with Garage_Area, Year_Built, and Overall_Qual as predictors. 5.15 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Split the data into training and test data. set.seed(012423) # fix the random number generator for reproducibility library(caret) # load library train_index &lt;- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets ames_train &lt;- ames[train_index,] # training data, use this dataset to build model ames_test &lt;- ames[-train_index,] # test data, use this dataset to evaluate model&#39;s performance 5.16 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Define CV specifications. cv_specs_kcv &lt;- trainControl(method = &quot;repeatedcv&quot;, # CV method number = 10, # number of folds repeats = 5) # each repeated 5 times 5.17 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Implement \\(k\\)-fold CV with the first model. m1 &lt;- train(form = Sale_Price ~ Garage_Area, # specify model data = ames_train, # specify dataset method = &quot;lm&quot;, # specify type of model trControl = cv_specs_kcv, # CV specifications metric = &quot;RMSE&quot;) # metric to evaluate model m1 # summary of LOOCV ## Linear Regression ## ## 706 samples ## 1 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 636, 635, 637, 636, 635, 634, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 62931.53 0.4437306 43590.34 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE m1$results # estimate and variability of metrics ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 62931.53 0.4437306 43590.34 12411.41 0.1024043 5478.664 5.18 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Implement \\(k\\)-fold CV with the second model. m2 &lt;- train(form = Sale_Price ~ Overall_Qual, data = ames_train, method = &quot;lm&quot;, trControl = cv_specs_kcv, metric = &quot;RMSE&quot;) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading m2 ## Linear Regression ## ## 706 samples ## 1 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 635, 636, 635, 636, 636, 636, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 46268.31 0.6968243 31912.75 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE m2$results ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 46268.31 0.6968243 31912.75 10192.63 0.09720067 4282.579 5.19 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Implement \\(k\\)-fold CV with the third model. m3 &lt;- train(form = Sale_Price ~ Garage_Area + Year_Built + Overall_Qual, data = ames_train, method = &quot;lm&quot;, trControl = cv_specs_kcv, metric = &quot;RMSE&quot;) m3 ## Linear Regression ## ## 706 samples ## 3 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 636, 634, 635, 636, 636, 636, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 42626.49 0.745911 28441.93 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE m3$results ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 42626.49 0.745911 28441.93 10024.47 0.1022967 3484.786 5.20 \\(k\\)-Fold Cross-Validation: Results Ames Housing dataset Compare \\(k\\)-fold CV results for different models. # create data frame to plot results df &lt;- data.frame(model_number = 1:3, RMSE = c(m1$results$RMSE, m2$results$RMSE, m3$results$RMSE)) # plot results from LOOCV ggplot(data = df, aes(x = model_number, y = RMSE)) + geom_point() + geom_line() 5.21 Final Model and Prediction Error Estimate Ames Housing dataset # after choosing final (optimal) model, refit final model using ALL training data, and obtain estimate of prediction error from test data m3$finalModel # final model ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Coefficients: ## (Intercept) Garage_Area ## -504482.40 78.99 ## Year_Built Overall_QualAverage ## 320.09 -18028.34 ## Overall_QualBelow_Average Overall_QualExcellent ## -34620.70 163226.47 ## Overall_QualFair Overall_QualGood ## -63928.81 30780.92 ## Overall_QualPoor Overall_QualVery_Excellent ## -76426.87 261135.65 ## Overall_QualVery_Good Overall_QualVery_Poor ## 81711.39 -69470.99 final_model_preds &lt;- predict(m3, newdata = ames_test) # obtain predictions on test data pred_error_est &lt;- sqrt(mean((ames_test$Sale_Price - final_model_preds)^2)) # calculate RMSE (estimate of prediction error) from test data pred_error_est ## [1] 34006.68 5.22 Variable Importance Ames Housing dataset # variable importance library(vip) ## ## Attaching package: &#39;vip&#39; ## The following object is masked from &#39;package:utils&#39;: ## ## vi vip(object = m3, # CV object num_features = 20, # maximum number of predictors to show importance for method = &quot;model&quot;) # model-specific VI scores 5.23 Bias-Variance Trade-off for LOOCV and \\(k\\)-fold CV LOOCV has very less bias. Using \\(k=5\\) or \\(10\\) yields more bias than LOOCV. For LOOCV, the error estimates for each fold are highly (positively) correlated. \\(k\\)-fold CV error estimates are somewhat less correlated. LOOCV error estimate has higher variance than \\(k\\)-fold CV error estimate. Typically, \\(k=5\\) or \\(10\\) is chosen. 5.24 Your Turn!!! Auto dataset Load the dataset. library(ISLR2) # load library data(&quot;Auto&quot;) # load dataset Consider mpg as the response and horsepower as the predictor. Objective: Find the optimum choice of \\(K\\) in the KNN approach with 5-fold CV repeated 5 times. You can use the following steps. Split the data into training and test data (80-20 split). Specify CV specifications using trainControl. Create an object k_grid using the following code. k_grid &lt;- expand.grid(k = seq(1, 100, by = 1)) # creates a grid of k values to be used (1 to 100 in this case) Use the train function to run CV. Use method = “knn”, tuneGrid = k_grid, and metric = “RMSE”. Obtain the results and plot them. What is the optimum \\(k\\) chosen? Create the final model using the optimum \\(k\\) and estimate its prediction error from the test data. 5.25 Your Turn!!!: Split Data Auto dataset set.seed(012423) # fix the random number generator for reproducibility library(caret) # load library train_index &lt;- createDataPartition(y = Auto$mpg, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets Auto_train &lt;- Auto[train_index,] # training data, use this dataset to build model Auto_test &lt;- Auto[-train_index,] # test data, use this dataset to evaluate model&#39;s performance 5.26 Your Turn!!!: Perform CV Auto dataset set.seed(012423) # fix the random number generator for reproducibility # CV specifications cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # specify grid of &#39;k&#39; values to search over k_grid &lt;- expand.grid(k = seq(1, 100, by = 1)) # train the KNN model to find optimal &#39;k&#39; knn_cv &lt;- train(form = mpg ~ horsepower, data = Auto_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;RMSE&quot;) 5.27 Your Turn!!!: Observe CV Results Auto dataset knn_cv # model training results ggplot(knn_cv) # plot the model training results for different &#39;k&#39; 5.28 Your Turn!!!: Final Model Auto dataset # final model with optimal &#39;k&#39; chosen from &#39;knn_fit&#39; results knn_cv$finalModel # final model ## 16-nearest neighbor regression model # obtain predictions on test data final_model_preds &lt;- predict(knn_cv, newdata = Auto_test) # estimate prediction error using RMSE sqrt(mean((Auto_test$mpg - final_model_preds)^2)) # RMSE ## [1] 4.314033 5.29 Mid-Term Check Are you comfortable with the concepts? Are you comfortable with the coding aspect? Explain. What study habits have been working for you with this course? What habits haven’t worked? Please mention any comments about the course in general (classwork, live coding, homework, quizzes, course structure and workflow, grading guidelines, etc.) "],["review-of-cv.html", "Chapter 6 Review of CV 6.1 Data Leakage (A Serious, Common Problem) 6.2 Data Preprocessing and Feature Enginnering 6.3 Ames Housing Dataset 6.4 Ames Housing Dataset 6.5 Ames Housing Dataset 6.6 Ames Housing Dataset 6.7 Ames Housing Dataset 6.8 Ames Housing Dataset 6.9 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables 6.10 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables 6.11 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables 6.12 Imputing Missing Entries 6.13 Imputing Missing Entries 6.14 Imputing Missing Entries 6.15 Label Encoding Ordinal Categorical Variables 6.16 Label Encoding Ordinal Categorical Variables 6.17 Label Encoding Ordinal Categorical Variables 6.18 Standardizing (centering and scaling) Numeric Predictors 6.19 Lumping Predictors 6.20 One-hot/dummy Encoding Categorical Predictors 6.21 One-hot/dummy Encoding Categorical Predictors 6.22 Preprocessing Steps 6.23 Preprocessing With recipes Package 6.24 Preprocessing With recipes Package 6.25 Training Model 6.26 Training Model 6.27 Training Model 6.28 Final Model and Test Set Error 6.29 Variable Importance 6.30 Your Turn!!! 6.31 Your Turn!!! Step 1 6.32 Your Turn!!! Step 1 6.33 Your Turn!!! Step 1 6.34 Your Turn!!! Step 1 6.35 Your Turn!!! Step 1 6.36 Your Turn!!! Step 2 6.37 Your Turn!!! Step 3 6.38 Your Turn!!! Step 4 6.39 Your Turn!!! Step 4 6.40 Your Turn!!! Step 4 6.41 Your Turn!!! Step 5 6.42 Your Turn!!! Step 5 6.43 Your Turn!!! Step 5", " Chapter 6 Review of CV KNN Classification: Toy Example Obs. \\(X_1\\) \\(X_2\\) Y 1 1033 1.7 Red 2 1112 1.5 Red 3 1500 1 Red 4 999 1 Green 5 1012 1.5 Green 6 1013 1 Red 7 1233 1 Green 8 1332 1 Red Suppose you implement 4-fold CV. What is the size of each training and validation set? Let’s say the folds are randomly chosen to be observation pairs (2, 3), (4, 7), (1, 8), and (5, 6). 6.1 Data Leakage (A Serious, Common Problem) Data leakage is when information from outside the training data set is used to create the model. Data leakage often occurs when the data preprocessing task is implemented with CV. To minimize this, feature engineering should be done in isolation of each resampling iteration. 6.2 Data Preprocessing and Feature Enginnering Data preprocessing and engineering techniques generally refer to the addition, deletion, or transformation of data. We will cover several fundamental and common preprocessing tasks that can potentially significantly improve modeling performance. Dealing with zero-variance (zv) and/or near-zero variance (nzv) variables Imputing missing entries Label encoding ordinal categorical variables Standardizing (centering and scaling) numeric predictors Lumping predictors One-hot/dummy encoding categorical predictors 6.3 Ames Housing Dataset ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # load dataset # ames &lt;- ames %&gt;% mutate_if(is.character, as.factor) # convert all character variables to factor variables sum(is.na(ames)) # check missing values ## [1] 113 6.4 Ames Housing Dataset glimpse(ames) # check type of variables, missing entries? ## Rows: 881 ## Columns: 20 ## $ Sale_Price &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, … ## $ Gr_Liv_Area &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, … ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, … ## $ Garage_Cars &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,… ## $ Garage_Area &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa… ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, … ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Neighborhood &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest… ## $ Screen_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Overall_Qual &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average… ## $ Lot_Area &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101… ## $ Lot_Frontage &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6… ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_… ## $ Misc_Val &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,… ## $ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7… ## $ First_Flr_SF &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15… ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11… ## $ Year_Built &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20… 6.5 Ames Housing Dataset summary(ames) # check type of variables, missing entries? ## Sale_Price Gr_Liv_Area Garage_Type Garage_Cars ## Min. : 34900 Min. : 334 Attchd :514 Min. :0.000 ## 1st Qu.:129500 1st Qu.:1118 Basment : 10 1st Qu.:1.000 ## Median :160000 Median :1442 BuiltIn : 55 Median :2.000 ## Mean :181115 Mean :1495 CarPort : 5 Mean :1.762 ## 3rd Qu.:213500 3rd Qu.:1728 Detchd :234 3rd Qu.:2.000 ## Max. :755000 Max. :5642 More_Than_Two_Types: 9 Max. :4.000 ## NA&#39;s :113 No_Garage : 54 ## Garage_Area Street Utilities Pool_Area ## Min. : 0.0 Grvl: 4 AllPub:880 Min. : 0.00 ## 1st Qu.: 324.0 Pave:877 NoSeWa: 0 1st Qu.: 0.00 ## Median : 480.0 NoSewr: 1 Median : 0.00 ## Mean : 476.5 Mean : 2.41 ## 3rd Qu.: 592.0 3rd Qu.: 0.00 ## Max. :1418.0 Max. :576.00 ## ## Neighborhood Screen_Porch Overall_Qual Lot_Area ## North_Ames :127 Min. : 0.00 Average :243 Min. : 1300 ## College_Creek : 86 1st Qu.: 0.00 Above_Average:217 1st Qu.: 7449 ## Old_Town : 83 Median : 0.00 Good :177 Median : 9512 ## Northridge_Heights: 52 Mean : 18.11 Very_Good : 99 Mean : 10105 ## Somerset : 50 3rd Qu.: 0.00 Below_Average: 83 3rd Qu.: 11526 ## Edwards : 49 Max. :490.00 Excellent : 38 Max. :159000 ## (Other) :434 (Other) : 24 ## Lot_Frontage MS_SubClass Misc_Val ## Min. : 0.00 One_Story_1946_and_Newer_All_Styles :335 Min. : 0.00 ## 1st Qu.: 43.00 Two_Story_1946_and_Newer :171 1st Qu.: 0.00 ## Median : 63.00 One_and_Half_Story_Finished_All_Ages: 92 Median : 0.00 ## Mean : 57.78 One_Story_PUD_1946_and_Newer : 53 Mean : 37.97 ## 3rd Qu.: 78.00 Duplex_All_Styles_and_Ages : 40 3rd Qu.: 0.00 ## Max. :313.00 One_Story_1945_and_Older : 36 Max. :8300.00 ## (Other) :154 ## Open_Porch_SF TotRms_AbvGrd First_Flr_SF Second_Flr_SF ## Min. : 0.00 Min. : 2.000 Min. : 334 Min. : 0.0 ## 1st Qu.: 0.00 1st Qu.: 5.000 1st Qu.: 877 1st Qu.: 0.0 ## Median : 27.00 Median : 6.000 Median :1092 Median : 0.0 ## Mean : 49.93 Mean : 6.413 Mean :1171 Mean : 319.6 ## 3rd Qu.: 72.00 3rd Qu.: 7.000 3rd Qu.:1426 3rd Qu.: 682.0 ## Max. :742.00 Max. :12.000 Max. :4692 Max. :2065.0 ## ## Year_Built ## Min. :1875 ## 1st Qu.:1954 ## Median :1972 ## Mean :1971 ## 3rd Qu.:2000 ## Max. :2010 ## 6.6 Ames Housing Dataset levels(ames$Overall_Qual) # the levels are NOT properly ordered ## [1] &quot;Above_Average&quot; &quot;Average&quot; &quot;Below_Average&quot; &quot;Excellent&quot; ## [5] &quot;Fair&quot; &quot;Good&quot; &quot;Poor&quot; &quot;Very_Excellent&quot; ## [9] &quot;Very_Good&quot; &quot;Very_Poor&quot; # relevel the levels ames$Overall_Qual &lt;- factor(ames$Overall_Qual, levels = c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;)) levels(ames$Overall_Qual) # the levels are properly ordered ## [1] &quot;Very_Poor&quot; &quot;Poor&quot; &quot;Fair&quot; &quot;Below_Average&quot; ## [5] &quot;Average&quot; &quot;Above_Average&quot; &quot;Good&quot; &quot;Very_Good&quot; ## [9] &quot;Excellent&quot; &quot;Very_Excellent&quot; 6.7 Ames Housing Dataset # split the dataset into training and test sets set.seed(013123) # set seed index &lt;- createDataPartition(ames$Sale_Price, p = 0.7, list = FALSE) # &#39;Sale_Price&#39; is the response ames_train &lt;- ames[index,] # training data ames_test &lt;- ames[-index,] # test data 6.8 Ames Housing Dataset # set up the recipe library(recipes) ## ## Attaching package: &#39;recipes&#39; ## The following object is masked from &#39;package:stringr&#39;: ## ## fixed ## The following object is masked from &#39;package:stats&#39;: ## ## step ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) # sets up the type and role of variables ames_recipe$var_info ## # A tibble: 20 × 4 ## variable type role source ## &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Gr_Liv_Area &lt;chr [2]&gt; predictor original ## 2 Garage_Type &lt;chr [3]&gt; predictor original ## 3 Garage_Cars &lt;chr [2]&gt; predictor original ## 4 Garage_Area &lt;chr [2]&gt; predictor original ## 5 Street &lt;chr [3]&gt; predictor original ## 6 Utilities &lt;chr [3]&gt; predictor original ## 7 Pool_Area &lt;chr [2]&gt; predictor original ## 8 Neighborhood &lt;chr [3]&gt; predictor original ## 9 Screen_Porch &lt;chr [2]&gt; predictor original ## 10 Overall_Qual &lt;chr [3]&gt; predictor original ## 11 Lot_Area &lt;chr [2]&gt; predictor original ## 12 Lot_Frontage &lt;chr [2]&gt; predictor original ## 13 MS_SubClass &lt;chr [3]&gt; predictor original ## 14 Misc_Val &lt;chr [2]&gt; predictor original ## 15 Open_Porch_SF &lt;chr [2]&gt; predictor original ## 16 TotRms_AbvGrd &lt;chr [2]&gt; predictor original ## 17 First_Flr_SF &lt;chr [2]&gt; predictor original ## 18 Second_Flr_SF &lt;chr [2]&gt; predictor original ## 19 Year_Built &lt;chr [2]&gt; predictor original ## 20 Sale_Price &lt;chr [2]&gt; outcome original 6.9 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables A rule of thumb for detecting near-zero variance features is: The fraction of unique values over the sample size is low (say \\(\\le 10\\%\\)). The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say \\(\\ge 20\\%\\)). 6.10 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables # investigate zv/nzv predictors nearZeroVar(ames_train, saveMetrics = TRUE) # check which predictors are zv/nzv ## freqRatio percentUnique zeroVar nzv ## Sale_Price 1.250000 61.8122977 FALSE FALSE ## Gr_Liv_Area 1.666667 68.7702265 FALSE FALSE ## Garage_Type 2.312102 1.1326861 FALSE FALSE ## Garage_Cars 1.922619 0.8090615 FALSE FALSE ## Garage_Area 2.105263 44.0129450 FALSE FALSE ## Street 153.500000 0.3236246 FALSE TRUE ## Utilities 617.000000 0.3236246 FALSE TRUE ## Pool_Area 614.000000 0.8090615 FALSE TRUE ## Neighborhood 1.655172 4.2071197 FALSE FALSE ## Screen_Porch 140.250000 6.9579288 FALSE TRUE ## Overall_Qual 1.176871 1.6181230 FALSE FALSE ## Lot_Area 1.100000 82.0388350 FALSE FALSE ## Lot_Frontage 1.485294 15.0485437 FALSE FALSE ## MS_SubClass 1.892562 2.4271845 FALSE FALSE ## Misc_Val 149.000000 2.2653722 FALSE TRUE ## Open_Porch_SF 20.142857 22.6537217 FALSE FALSE ## TotRms_AbvGrd 1.212329 1.7799353 FALSE FALSE ## First_Flr_SF 2.333333 70.8737864 FALSE FALSE ## Second_Flr_SF 90.750000 34.4660194 FALSE FALSE ## Year_Built 1.214286 15.8576052 FALSE FALSE 6.11 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) # filter out zv/nzv predictors 6.12 Imputing Missing Entries Possible imputation techniques: step_impute_median step_impute_mean step_impute_knn step_impute_mode 6.13 Imputing Missing Entries summary(ames_train) # check which predictors have missing entries ## Sale_Price Gr_Liv_Area Garage_Type Garage_Cars ## Min. : 39300 Min. : 334 Attchd :363 Min. :0.000 ## 1st Qu.:129100 1st Qu.:1114 Basment : 6 1st Qu.:1.000 ## Median :160000 Median :1446 BuiltIn : 41 Median :2.000 ## Mean :180748 Mean :1489 CarPort : 5 Mean :1.746 ## 3rd Qu.:213408 3rd Qu.:1742 Detchd :157 3rd Qu.:2.000 ## Max. :745000 Max. :4476 More_Than_Two_Types: 6 Max. :4.000 ## NA&#39;s :77 No_Garage : 40 ## Garage_Area Street Utilities Pool_Area ## Min. : 0.0 Grvl: 4 AllPub:617 Min. : 0.000 ## 1st Qu.: 320.5 Pave:614 NoSeWa: 0 1st Qu.: 0.000 ## Median : 482.0 NoSewr: 1 Median : 0.000 ## Mean : 473.4 Mean : 2.659 ## 3rd Qu.: 587.5 3rd Qu.: 0.000 ## Max. :1348.0 Max. :576.000 ## ## Neighborhood Screen_Porch Overall_Qual Lot_Area ## North_Ames : 96 Min. : 0.00 Average :173 Min. : 1491 ## College_Creek : 58 1st Qu.: 0.00 Above_Average:147 1st Qu.: 7572 ## Old_Town : 54 Median : 0.00 Good :121 Median : 9582 ## Sawyer : 39 Mean : 17.42 Very_Good : 72 Mean : 9830 ## Northridge_Heights: 38 3rd Qu.: 0.00 Below_Average: 61 3rd Qu.:11500 ## Edwards : 36 Max. :410.00 Excellent : 25 Max. :50102 ## (Other) :297 (Other) : 19 ## Lot_Frontage MS_SubClass Misc_Val ## Min. : 0.0 One_Story_1946_and_Newer_All_Styles :229 Min. : 0.00 ## 1st Qu.: 44.0 Two_Story_1946_and_Newer :121 1st Qu.: 0.00 ## Median : 64.0 One_and_Half_Story_Finished_All_Ages: 59 Median : 0.00 ## Mean : 58.4 One_Story_PUD_1946_and_Newer : 37 Mean : 43.73 ## 3rd Qu.: 79.0 Duplex_All_Styles_and_Ages : 33 3rd Qu.: 0.00 ## Max. :182.0 Two_Story_1945_and_Older : 27 Max. :8300.00 ## (Other) :112 ## Open_Porch_SF TotRms_AbvGrd First_Flr_SF Second_Flr_SF ## Min. : 0.00 Min. : 2.000 Min. : 334 Min. : 0.0 ## 1st Qu.: 0.00 1st Qu.: 5.000 1st Qu.: 875 1st Qu.: 0.0 ## Median : 24.50 Median : 6.000 Median :1072 Median : 0.0 ## Mean : 47.29 Mean : 6.422 Mean :1159 Mean : 320.1 ## 3rd Qu.: 69.00 3rd Qu.: 7.000 3rd Qu.:1395 3rd Qu.: 683.8 ## Max. :742.00 Max. :12.000 Max. :2674 Max. :2065.0 ## ## Year_Built ## Min. :1875 ## 1st Qu.:1954 ## Median :1972 ## Mean :1971 ## 3rd Qu.:2000 ## Max. :2009 ## 6.14 Imputing Missing Entries blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) # impute missing entries 6.15 Label Encoding Ordinal Categorical Variables Label encoding is a pure numeric conversion of the levels of a categorical variable. If a categorical variable is a factor and it has pre-specified levels then the numeric conversion will be in level order. If no levels are specified, the encoding will be based on alphabetical order. We should be careful with label encoding unordered categorical features because most models will treat them as ordered numeric features 6.16 Label Encoding Ordinal Categorical Variables # investigate predictors with possible ordering (label encoding) ames_train %&gt;% count(Overall_Qual) ## # A tibble: 10 × 2 ## Overall_Qual n ## &lt;fct&gt; &lt;int&gt; ## 1 Very_Poor 1 ## 2 Poor 2 ## 3 Fair 7 ## 4 Below_Average 61 ## 5 Average 173 ## 6 Above_Average 147 ## 7 Good 121 ## 8 Very_Good 72 ## 9 Excellent 25 ## 10 Very_Excellent 9 6.17 Label Encoding Ordinal Categorical Variables blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) # numeric conversion of levels of the predictors 6.18 Standardizing (centering and scaling) Numeric Predictors Standardizing features includes centering and scaling so that numeric variables have zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Before centering and scaling, it is better to remove zv/nzv variables, and perform necessary imputation and label encoding. blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors step_scale(all_numeric(), -all_outcomes()) # scale (divide by standard deviation) all numeric predictors 6.19 Lumping Predictors Sometimes features (numerical or categorical) will contain levels that have very few observations (decided by a threshold). It can be beneficial to collapse, or “lump” these into a lesser number of categories. # lumping categorical predictors if need be ames_train %&gt;% count(Neighborhood) %&gt;% arrange(n) # check frequency of categories blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors step_scale(all_numeric(), -all_outcomes()) %&gt;% # scale (divide by standard deviation) all numeric predictors step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) # lumping required predictors 6.20 One-hot/dummy Encoding Categorical Predictors Figure 6.1: Adapted from Hands-on Machine Learning with R, Bradley Boehmke &amp; Brandon Greenwell 6.21 One-hot/dummy Encoding Categorical Predictors blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors step_scale(all_numeric(), -all_outcomes()) %&gt;% # scale (divide by standard deviation) all numeric predictors step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% # lumping required predictors step_dummy(all_nominal(), one_hot = TRUE) # one-hot/dummy encode nominal categorical predictors 6.22 Preprocessing Steps A suggested order of potential steps that should work for most problems: Filter out zero or near-zero variance features. Perform imputation if required. Label encode ordinal categorical features. Normalize/Standardize (center and scale) numeric features. Lump certain features if required. One-hot or dummy encode categorical features. 6.23 Preprocessing With recipes Package There are three main steps in creating and applying feature engineering with recipes: recipe: where you define your feature engineering steps to create your blueprint. prepare: estimate feature engineering parameters based on training data. bake: apply the blueprint to new data. 6.24 Preprocessing With recipes Package # finally, after all preprocessing steps have been decided set up the overall blueprint ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% step_impute_mean(Gr_Liv_Area) %&gt;% step_integer(Overall_Qual) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% step_scale(all_numeric(), -all_outcomes()) %&gt;% step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% step_dummy(all_nominal(), one_hot = TRUE) prepare &lt;- prep(blueprint, data = ames_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = ames_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = ames_test) # apply the blueprint to test data for future use 6.25 Training Model A KNN model # perform CV to tune K set.seed(013123) cv_specs &lt;- trainControl(method = &quot;cv&quot;, number = 5) # 5-fold CV (no repeats) k_grid &lt;- expand.grid(k = seq(1, 10, by = 2)) knn_fit &lt;- train(blueprint, data = ames_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;RMSE&quot;) knn_fit ## k-Nearest Neighbors ## ## 618 samples ## 19 predictor ## ## Recipe steps: nzv, impute_mean, integer, center, scale, other, dummy ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 494, 494, 495, 494, 495 ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 1 42573.14 0.7499158 27889.19 ## 3 35686.37 0.8233707 23448.95 ## 5 33973.09 0.8432532 22186.60 ## 7 34439.17 0.8446435 22479.38 ## 9 35202.64 0.8399091 22419.31 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 5. 6.26 Training Model A KNN model ggplot(knn_fit) 6.27 Training Model A linear regression model lm_fit &lt;- train(blueprint, data = ames_train, method = &quot;lm&quot;, trControl = cv_specs, metric = &quot;RMSE&quot;) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading lm_fit ## Linear Regression ## ## 618 samples ## 19 predictor ## ## Recipe steps: nzv, impute_mean, integer, center, scale, other, dummy ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 495, 494, 495, 494, 494 ## Resampling results: ## ## RMSE Rsquared MAE ## 32755.85 0.8502331 22609.88 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE 6.28 Final Model and Test Set Error # refit the final/optimal model using ALL modified training data, and obtain estimate of prediction error from modified test data final_model &lt;- lm(Sale_Price ~ ., data = baked_train) # build final model final_preds &lt;- predict(final_model, newdata = baked_test) # obtain predictions on test data ## Warning in predict.lm(final_model, newdata = baked_test): prediction from a ## rank-deficient fit may be misleading sqrt(mean((baked_test$Sale_Price - final_preds)^2)) # calculate test set RMSE ## [1] 47839.78 6.29 Variable Importance # variable importance library(vip) vip(object = lm_fit, # CV object num_features = 20, # maximum number of predictors to show importance for method = &quot;model&quot;) # model-specific VI scores 6.30 Your Turn!!! You will work with the attrition data from the modeldata package. The task is to predict Attrition (Yes/No) using the rest of the variables in the data (predictors/features). library(modeldata) # load package data(attrition) # load dataset Step 1: Investigate the dataset What are the types of features? - categorical or numeric If categorical, are they ordinal or nominal? If ordinal, are their levels in appropriate order? You can use the levels function to check the ordering. Are there any features with missing entries? Are there any zv/nzv features? Step 2: Split the data into training and test sets (70-30 split) Step 3: Perform required data preprocessing and create the blueprint. If using step_dummy(), set one_hot = FALSE. Step 4: Implement 5-fold CV (no repeats) to compare the performance of the following models. Use metric = \"Accuracy\". a logistic regression model (method = \"glm\" and family = \"binomial\") a KNN classifier with the optimal K chosen by CV (method = \"knn\"). Use a grid of K values \\(1, 2, \\ldots, 10\\). What is the optimal K chosen? How do the models compare in terms of the CV accuracies? Step 5: Build your final optimal model. Obtain probability and class label predictions for the test set (use threshold of 0.5). Create the corresponding confusion matrix and report the test set accuracy. Also, create the ROC curve for the optimal model and report the AUC. 6.31 Your Turn!!! Step 1 glimpse(attrition) # types of variables ## Rows: 1,470 ## Columns: 31 ## $ Age &lt;int&gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36, 35, 2… ## $ Attrition &lt;fct&gt; Yes, No, Yes, No, No, No, No, No, No, No, No,… ## $ BusinessTravel &lt;fct&gt; Travel_Rarely, Travel_Frequently, Travel_Rare… ## $ DailyRate &lt;int&gt; 1102, 279, 1373, 1392, 591, 1005, 1324, 1358,… ## $ Department &lt;fct&gt; Sales, Research_Development, Research_Develop… ## $ DistanceFromHome &lt;int&gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15, 26, … ## $ Education &lt;ord&gt; College, Below_College, College, Master, Belo… ## $ EducationField &lt;fct&gt; Life_Sciences, Life_Sciences, Other, Life_Sci… ## $ EnvironmentSatisfaction &lt;ord&gt; Medium, High, Very_High, Very_High, Low, Very… ## $ Gender &lt;fct&gt; Female, Male, Male, Female, Male, Male, Femal… ## $ HourlyRate &lt;int&gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94, 84, 4… ## $ JobInvolvement &lt;ord&gt; High, Medium, Medium, High, High, High, Very_… ## $ JobLevel &lt;int&gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, … ## $ JobRole &lt;fct&gt; Sales_Executive, Research_Scientist, Laborato… ## $ JobSatisfaction &lt;ord&gt; Very_High, Medium, High, High, Medium, Very_H… ## $ MaritalStatus &lt;fct&gt; Single, Married, Single, Married, Married, Si… ## $ MonthlyIncome &lt;int&gt; 5993, 5130, 2090, 2909, 3468, 3068, 2670, 269… ## $ MonthlyRate &lt;int&gt; 19479, 24907, 2396, 23159, 16632, 11864, 9964… ## $ NumCompaniesWorked &lt;int&gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, 0, 5, … ## $ OverTime &lt;fct&gt; Yes, No, Yes, Yes, No, No, Yes, No, No, No, N… ## $ PercentSalaryHike &lt;int&gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13, 13, 1… ## $ PerformanceRating &lt;ord&gt; Excellent, Outstanding, Excellent, Excellent,… ## $ RelationshipSatisfaction &lt;ord&gt; Low, Very_High, Medium, High, Very_High, High… ## $ StockOptionLevel &lt;int&gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, 1, 0, … ## $ TotalWorkingYears &lt;int&gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10, 5, 3… ## $ TrainingTimesLastYear &lt;int&gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, 2, 4, … ## $ WorkLifeBalance &lt;ord&gt; Bad, Better, Better, Better, Better, Good, Go… ## $ YearsAtCompany &lt;int&gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5, 2, 4,… ## $ YearsInCurrentRole &lt;int&gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, 2, 2, … ## $ YearsSinceLastPromotion &lt;int&gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, 1, 0, … ## $ YearsWithCurrManager &lt;int&gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, 2, 3, … Numerical variables are represented as &lt;int&gt;, categorical variables are represented as either &lt;ord&gt; or &lt;fct&gt;. 6.32 Your Turn!!! Step 1 # checking the levels of ordinal variables levels(attrition$BusinessTravel) ## [1] &quot;Non-Travel&quot; &quot;Travel_Frequently&quot; &quot;Travel_Rarely&quot; levels(attrition$Education) ## [1] &quot;Below_College&quot; &quot;College&quot; &quot;Bachelor&quot; &quot;Master&quot; ## [5] &quot;Doctor&quot; levels(attrition$EnvironmentSatisfaction) ## [1] &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; &quot;Very_High&quot; levels(attrition$JobInvolvement) ## [1] &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; &quot;Very_High&quot; levels(attrition$JobSatisfaction) ## [1] &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; &quot;Very_High&quot; levels(attrition$PerformanceRating) ## [1] &quot;Low&quot; &quot;Good&quot; &quot;Excellent&quot; &quot;Outstanding&quot; levels(attrition$RelationshipSatisfaction) ## [1] &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; &quot;Very_High&quot; levels(attrition$WorkLifeBalance) ## [1] &quot;Bad&quot; &quot;Good&quot; &quot;Better&quot; &quot;Best&quot; 6.33 Your Turn!!! Step 1 # reorder levels of &#39;BusinessTravel&#39; attrition$BusinessTravel &lt;- factor(attrition$BusinessTravel, levels = c(&quot;Non-Travel&quot;, &quot;Travel_Rarely&quot;, &quot;Travel_Frequently&quot;)) levels(attrition$BusinessTravel) ## [1] &quot;Non-Travel&quot; &quot;Travel_Rarely&quot; &quot;Travel_Frequently&quot; 6.34 Your Turn!!! Step 1 sum(is.na(attrition)) # check for missing entries ## [1] 0 6.35 Your Turn!!! Step 1 nearZeroVar(attrition, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## Age 1.012987 2.9251701 FALSE FALSE ## Attrition 5.202532 0.1360544 FALSE FALSE ## BusinessTravel 3.765343 0.2040816 FALSE FALSE ## DailyRate 1.200000 60.2721088 FALSE FALSE ## Department 2.154709 0.2040816 FALSE FALSE ## DistanceFromHome 1.014423 1.9727891 FALSE FALSE ## Education 1.437186 0.3401361 FALSE FALSE ## EducationField 1.306034 0.4081633 FALSE FALSE ## EnvironmentSatisfaction 1.015695 0.2721088 FALSE FALSE ## Gender 1.500000 0.1360544 FALSE FALSE ## HourlyRate 1.035714 4.8299320 FALSE FALSE ## JobInvolvement 2.314667 0.2721088 FALSE FALSE ## JobLevel 1.016854 0.3401361 FALSE FALSE ## JobRole 1.116438 0.6122449 FALSE FALSE ## JobSatisfaction 1.038462 0.2721088 FALSE FALSE ## MaritalStatus 1.431915 0.2040816 FALSE FALSE ## MonthlyIncome 1.333333 91.7687075 FALSE FALSE ## MonthlyRate 1.000000 97.0748299 FALSE FALSE ## NumCompaniesWorked 2.644670 0.6802721 FALSE FALSE ## OverTime 2.533654 0.1360544 FALSE FALSE ## PercentSalaryHike 1.004785 1.0204082 FALSE FALSE ## PerformanceRating 5.504425 0.1360544 FALSE FALSE ## RelationshipSatisfaction 1.062500 0.2721088 FALSE FALSE ## StockOptionLevel 1.058725 0.2721088 FALSE FALSE ## TotalWorkingYears 1.616000 2.7210884 FALSE FALSE ## TrainingTimesLastYear 1.114053 0.4761905 FALSE FALSE ## WorkLifeBalance 2.595930 0.2721088 FALSE FALSE ## YearsAtCompany 1.146199 2.5170068 FALSE FALSE ## YearsInCurrentRole 1.524590 1.2925170 FALSE FALSE ## YearsSinceLastPromotion 1.627451 1.0884354 FALSE FALSE ## YearsWithCurrManager 1.307985 1.2244898 FALSE FALSE 6.36 Your Turn!!! Step 2 # split data set.seed(013123) train_index &lt;- createDataPartition(attrition$Attrition, p = 0.7, list = FALSE) attrition_train &lt;- attrition[train_index, ] attrition_test &lt;- attrition[-train_index, ] 6.37 Your Turn!!! Step 3 # create recipe, blueprint, prepare, and bake attrition_recipe &lt;- recipe(formula = Attrition ~ ., data = attrition_train) # sets up the type and role of variables blueprint &lt;- attrition_recipe %&gt;% step_integer(BusinessTravel, Education, EnvironmentSatisfaction, JobInvolvement, JobSatisfaction, PerformanceRating, RelationshipSatisfaction, WorkLifeBalance) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) prepare &lt;- prep(blueprint, data = attrition_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = attrition_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = attrition_test) # apply the blueprint to test data for future use 6.38 Your Turn!!! Step 4 # perform CV set.seed(013123) cv_specs &lt;- trainControl(method = &quot;cv&quot;, number = 5) # 5-fold CV (no repeats) # CV with logistic regression logistic_fit &lt;- train(blueprint, data = attrition_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = cv_specs, metric = &quot;Accuracy&quot;) logistic_fit ## Generalized Linear Model ## ## 1030 samples ## 30 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Recipe steps: integer, center, scale, dummy ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 824, 823, 825, 824, 824 ## Resampling results: ## ## Accuracy Kappa ## 0.870915 0.4528554 6.39 Your Turn!!! Step 4 # CV with KNN set.seed(013123) k_grid &lt;- expand.grid(k = seq(1, 10, by = 1)) knn_fit &lt;- train(blueprint, data = attrition_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;Accuracy&quot;) knn_fit ## k-Nearest Neighbors ## ## 1030 samples ## 30 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Recipe steps: integer, center, scale, dummy ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 824, 823, 825, 824, 824 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.7835261 0.13378621 ## 2 0.7873814 0.14344232 ## 3 0.8398231 0.20441187 ## 4 0.8310899 0.17242779 ## 5 0.8504841 0.17107000 ## 6 0.8446258 0.15021424 ## 7 0.8465958 0.13835761 ## 8 0.8485517 0.13591269 ## 9 0.8427169 0.07579273 ## 10 0.8456343 0.08956634 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 5. 6.40 Your Turn!!! Step 4 ggplot(knn_fit) 6.41 Your Turn!!! Step 5 # build final optimal model and obtain predictions on test set final_model &lt;- glm(Attrition ~ ., data = baked_train, family = binomial) # build final model final_model_prob_preds &lt;- predict(object = final_model, newdata = baked_test, type = &quot;response&quot;) # obtain probability predictions on test data threshold &lt;- 0.5 final_model_class_preds &lt;- factor(ifelse(final_model_prob_preds &gt; threshold, &quot;Yes&quot;, &quot;No&quot;)) 6.42 Your Turn!!! Step 5 # create confusion matrix confusionMatrix(data = relevel(final_model_class_preds, ref = &quot;Yes&quot;), reference = relevel(baked_test$Attrition, ref = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 28 14 ## No 43 355 ## ## Accuracy : 0.8705 ## 95% CI : (0.8354, 0.9004) ## No Information Rate : 0.8386 ## P-Value [Acc &gt; NIR] : 0.0372769 ## ## Kappa : 0.4268 ## ## Mcnemar&#39;s Test P-Value : 0.0002083 ## ## Sensitivity : 0.39437 ## Specificity : 0.96206 ## Pos Pred Value : 0.66667 ## Neg Pred Value : 0.89196 ## Prevalence : 0.16136 ## Detection Rate : 0.06364 ## Detection Prevalence : 0.09545 ## Balanced Accuracy : 0.67821 ## ## &#39;Positive&#39; Class : Yes ## 6.43 Your Turn!!! Step 5 # create ROC cuvre and compute AUC roc_object &lt;- roc(response = baked_test$Attrition, predictor = final_model_prob_preds) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases plot(roc_object, col = &quot;red&quot;) auc(roc_object) ## Area under the curve: 0.8361 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
