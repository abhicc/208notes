[["index.html", "208 Course Notes Chapter 1 Introduction", " 208 Course Notes Abhishek Chakraborty 2023-03-22 Chapter 1 Introduction Packages that we would need library(tidyverse) More packages to follow as per the requirements in the following chapters. "],["what-is-machine-learning.html", "Chapter 2 What is Machine Learning? 2.1 What is Machine Learning? 2.2 Question!!! 2.3 Statistical Learning vs Machine Learning vs Data Science 2.4 Notations 2.5 Notations 2.6 Question!!! 2.7 Question!!! 2.8 Supervised vs Unsupervised 2.9 Supervised Learning 2.10 Supervised Learning 2.11 Unsupervised Learning 2.12 Question!!! 2.13 Supervised Learning 2.14 Supervised Learning 2.15 Supervised Learning 2.16 Supervised Learning 2.17 Supervised Learning: Why Estimate \\(f(\\mathbf{X})\\)? 2.18 Supervised Learning: Prediction and Inference 2.19 Supervised Learning: Prediction and Inference 2.20 Supervised Learning: Prediction 2.21 Question!!! 2.22 Supervised Learning: How Do We Estimate \\(f(\\mathbf{X})\\)? 2.23 Supervised Learning: Parametric Methods 2.24 Supervised Learning: Parametric Methods 2.25 Supervised Learning: Non-parametric Methods 2.26 Supervised Learning: Flexibility of Models 2.27 Supervised Learning: Some Trade-offs 2.28 Supervised Learning: Some Trade-offs", " Chapter 2 What is Machine Learning? Machine Learning is the study of tools/techniques for understanding complex datasets. The name machine learning was coined in 1959 by Arthur Samuel. “Field of study that gives computers the ability to learn without being explicitly programmed.” 2.1 What is Machine Learning? Tom M. Mitchell (1998) defined algorithms studied in the machine learning field as “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.” 2.2 Question!!! Suppose your email program watches which emails you do or do not mark as spam, and based on that learns how to better filter spam. According to Tom Mitchell’s definition, what is the task T, experience E, and performance measure P in this setting? The number (or fraction) of emails correctly classified as spam/ham. Classifying emails as spam or ham (not spam) Watching you label emails as spam or ham. 2.3 Statistical Learning vs Machine Learning vs Data Science Machine learning arose as a subfield of Artificial Intelligence. Statistical learning arose as a subfield of Statistics. There is much overlap, a great deal of “cross-fertilization”. “Data Science” - Reflects the fact that both statistical and machine learning are about data. “Machine learning” or “Data Science” are “fancier” terms. 2.4 Notations Matrices - Bold, Upper-case \\(\\mathbf{X}\\) Vectors - Bold, Lower-case \\(\\mathbf{x}\\) Scalars - Normal, Lower-case \\(x\\) Random Variables - Normal, Upper-case \\(X\\) No. of data points/observations - \\(n\\) No. of variables - \\(p\\) 2.5 Notations Figure 2.1: A matrix of dimension n x p Figure 2.2: A n-dimensional vector Figure 2.3: Matrix 2.6 Question!!! Suppose you are given the following feature matrix. \\[ \\mathbf{X}=\\begin{pmatrix} 8.5 &amp; 11.2 &amp; 7.0 &amp; 9.3 \\\\ 8.0 &amp; 11.5 &amp; 13.1 &amp; 7.4 \\\\ 6.4 &amp; 9.6 &amp; 7.0 &amp; 6.8 \\\\ 9.5 &amp; -3.2 &amp; 14.4 &amp; 1.6 \\end{pmatrix} \\] What are the corresponding values of \\(n\\) and \\(p\\)? What will be the dimension of the corresponding response vector \\(\\mathbf{y}\\)? What is the value of the 3rd feature for the 2nd observation? 2.7 Question!!! Suppose you have information about 867 cancer patients on their age, tumor size, clump thickness of the tumor, uniformity of cell size, and whether the tumor is malignant or benign. Based on these data, you are interested in building a model to predict the type of tumor (malignant or benign) for future cancer patients. What are the values of \\(n\\) and \\(p\\) in this dataset? What are the inputs/features? 2.8 Supervised vs Unsupervised Figure 2.4: Machine Learning Tasks 2.9 Supervised Learning Labeled training data Inputs/Features/Regressors/Covariates/Independent Variables Response/Target/Dependent Variable 2.10 Supervised Learning The objective is to learn the overall pattern of the relationship between the inputs (\\(\\mathbf{X}\\)) and response (\\(\\mathbf{y}\\)) in order to Investigate the relationship between inputs and response. Predict for potential unseen test cases. Assess the quality of predictions. Supervised Learning problems can be categorized into Regression problems (response is quantitative, continuous) Classification problems (response is qualitative, categorical) 2.11 Unsupervised Learning No outcome variable, just \\(\\mathbf{X}\\). Understand structure within data. find similar groups of observations based on features (clustering) find a smaller subset of features with the most variation (dimensionality reduction) No gold-standard. Easier to collect unlabeled data. Useful pre-processing step for supervised learning. 2.12 Question!!! Some of the problems below are best addressed using a supervised learning algorithm, while others with an unsupervised learning algorithm. In each case, identify whether the problem belongs to the supervised or unsupervised learning paradigm. (Assume some appropriate dataset is available for your algorithm to “learn” from.) Examine the statistics of two football teams, and predict which team will win tomorrow’s match (given historical data of teams’ wins/losses to learn from). Given genetic (DNA) data from a person, predict the odds of the person developing diabetes over the next 10 years. Take a collection of 1000 essays written on the US economy, and find a way to automatically group these essays into a small number of groups of essays that are somehow “similar” or “related”. Examine a large collection of emails that are known to be spam, to discover if there are sub-types of spam email. Suppose you have information about 867 cancer patients on their age, tumor size, clump thickness of the tumor, uniformity of cell size, and whether the tumor is malignant or benign. Based on these data, you are interested in building a model to predict the type of tumor (malignant or benign) for future cancer patients. Examine data on the income and years of education of adults in a neighborhood and build a model to predict the income from years of education. 2.13 Supervised Learning More mathematically, the “true”/population model can be represented by \\[Y=f(\\mathbf{X}) + \\epsilon\\] where \\(\\epsilon\\) is a random error term (includes measurement error, other discrepancies) independent of \\(\\mathbf{X}\\) and has mean zero. 2.14 Supervised Learning The primary objective is to: Regression: response \\(Y\\) is quantitative Build a model \\(\\hat{Y} = \\hat{f}(\\mathbf{X})\\) Classification: response \\(Y\\) is qualitative Build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\) 2.15 Supervised Learning Income dataset 2.16 Supervised Learning Income dataset 2.17 Supervised Learning: Why Estimate \\(f(\\mathbf{X})\\)? We wish to know about \\(f(\\mathbf{X})\\) for two reasons: Prediction at new unseen data points \\(x_0\\) \\[\\hat{y}_0=\\hat{f}(x_0) \\ \\ \\ \\text{or} \\ \\ \\ \\hat{y}_0=\\hat{C}(x_0)\\] Inference: Understand the relationship between \\(\\mathbf{X}\\) and \\(Y\\). An ML algorithm that is developed mainly for predictive purposes is often termed as a Black Box algorithm. 2.18 Supervised Learning: Prediction and Inference Income dataset 2.19 Supervised Learning: Prediction and Inference Income dataset 2.20 Supervised Learning: Prediction When we estimate \\(f(\\mathbf{X})\\) using \\(\\hat{f}(\\mathbf{X})\\), then, \\[E\\left[Y-\\hat{Y}\\right]^2=E\\left[f(\\mathbf{X})+\\epsilon - \\hat{f}(\\mathbf{X})\\right]^2=\\underbrace{\\left[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})\\right]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}\\] \\(E\\left[Y-\\hat{Y}\\right]^2\\): Expected (average) squared difference between predicted and actual (observed) response. We will focus on techniques for estimating \\(f(\\mathbf{X})\\) with the objective of minimizing the reducible error. 2.21 Question!!! Which of the following statements are true for the random error term \\(\\epsilon\\) in the expression \\(Y=f(\\mathbf{X})+\\epsilon\\)? \\(\\epsilon\\) depends on \\(\\mathbf{X}\\) and has mean zero. \\(Var(\\epsilon)\\) is also known as the irreducible error. \\(\\epsilon\\) is independent of \\(\\mathbf{X}\\) and has mean zero. \\(\\epsilon\\) is some fixed but unknown function of \\(\\mathbf{X}\\). 2.22 Supervised Learning: How Do We Estimate \\(f(\\mathbf{X})\\)? Broadly speaking, we have two approaches. Parametric and Structured Methods A functional form of \\(f(\\mathbf{X})\\) is assumed, such as \\[f(\\mathbf{X})=\\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\ldots + \\beta_p \\mathbf{x}_p\\] We estimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) by fitting the model to labeled training data. 2.23 Supervised Learning: Parametric Methods Income dataset 2.24 Supervised Learning: Parametric Methods Income dataset Figure 2.5: Linear Model Fit to Income Data \\[\\text{Income} \\approx \\beta_0 + \\beta_1 \\times \\text{Years of Education} + \\beta_2 \\times \\text{Seniority}\\] 2.25 Supervised Learning: Non-parametric Methods Non-parametric approaches do not make any explicit assumptions about the functional form of \\(f(\\mathbf{X})\\). A very large number of observations (compared to a parametric approach) is required to fit a model using the non-parametric approach. Income dataset Figure 2.6: Smooth Thin-plate Spline Fit to Income Data 2.26 Supervised Learning: Flexibility of Models Flexibility refers to the smoothness of functions. (More theoretically, flexibility depends on the number of parameters of the function). More flexible \\(\\implies\\) More complex \\(\\implies\\) Less Smooth \\(\\implies\\) Less Restrictive \\(\\implies\\) Less Interpretable 2.27 Supervised Learning: Some Trade-offs Prediction Accuracy versus Interpretability Good Fit versus Over-fit or Under-fit 2.28 Supervised Learning: Some Trade-offs "],["supervised-learning-assessing-model-accuracy.html", "Chapter 3 Supervised Learning: Assessing Model Accuracy 3.1 Supervised Learning: Assessing Model Accuracy 3.2 Supervised Learning: Assessing Model Accuracy 3.3 Supervised Learning: Assessing Model Accuracy 3.4 Supervised Learning: Assessing Model Accuracy 3.5 Supervised Learning: Assessing Model Accuracy 3.6 Supervised Learning: Assessing Model Accuracy 3.7 Supervised Learning: Bias-Variance Trade-off 3.8 Supervised Learning: Bias-Variance Trade-off 3.9 Supervised Learning: Bias-Variance Trade-off 3.10 Question!!! 3.11 Simple Linear Regression (SLR) 3.12 Question!!! 3.13 SLR: Estimating Parameters 3.14 SLR: Estimating Parameters 3.15 SLR: Estimating Parameters 3.16 Ames Housing Dataset 3.17 Ames Housing dataset 3.18 SLR: Estimating Parameters 3.19 SLR: Model 3.20 SLR: Model 3.21 SLR: Prediction 3.22 SLR: Interpreting Parameters 3.23 SLR: Assessing Accuracy of Model 3.24 SLR: Assessing Accuracy of Model 3.25 Your Turn!!! 3.26 Question!!! 3.27 Question!!! 3.28 Question!!! 3.29 Regression: Conditional Averaging 3.30 Regression: Conditional Averaging 3.31 K-Nearest Neighbors Regression 3.32 K-Nearest Neighbors Regression: Fit 3.33 K-Nearest Neighbors Regression: Prediction 3.34 Regression Methods: Comparison 3.35 Your Turn!!! 3.36 Question!!!", " Chapter 3 Supervised Learning: Assessing Model Accuracy Why are we going to study so many different ML techniques? There is no free lunch in statistics: No one method dominates all others over all possible datasets. 3.1 Supervised Learning: Assessing Model Accuracy Suppose we have labeled training data \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\), i.e, \\(n\\) training data points/observations. We fit/train a model \\(\\hat{y}=\\hat{f}(x)\\) (or, a classifier \\(\\hat{y}=\\hat{C}(x)\\)) on the training data and obtain estimates \\(\\hat{f}(x_1), \\hat{f}(x_2), \\ldots, \\hat{f}(x_n)\\) (or, \\(\\hat{C}(x_1), \\hat{C}(x_2), \\ldots, \\hat{C}(x_n)\\)). We could then compute the Regression \\[\\text{Training MSE}=\\text{Average}_{Training} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\left(y_i-\\hat{f}(x_i)\\right)^2\\] Classification \\[\\text{Training Error Rate}=\\text{Average}_{Training} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]= \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\ I\\left(y_i \\ne \\hat{C}(x_i)\\right)\\] 3.2 Supervised Learning: Assessing Model Accuracy But in general, we are not interested in how the method works on the training data. We want to measure the accuracy of the method on previously unseen test data. Suppose, if possible, we have fresh test data, \\((x_1^{test},y_1^{test}), (x_2^{test},y_2^{test}), \\ldots, (x_m^{test},y_m^{test})\\). Then we can compute, Regression \\[\\text{Test MSE}=\\text{Average}_{Test} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\left(y_i^{test}-\\hat{f}(x_i^{test})\\right)^2\\] Classification \\[\\text{Test Error Rate}=\\text{Average}_{Test} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]= \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\ I\\left(y_i^{test} \\ne \\hat{C}(x_i^{test})\\right)\\] 3.3 Supervised Learning: Assessing Model Accuracy In the following slides, we look at three different examples with simulated toy datasets. We work within the regression setting (but the ideas also extend to the classification setting) and three different \\(\\hat{f}(.)\\)’s. Linear Regression (Orange) Smoothing Spline 1 (Blue) More flexible Smoothing Spline 2 (Green) The “true” function (simulated) is \\(f(.)\\) (black). 3.4 Supervised Learning: Assessing Model Accuracy Simulated toy dataset 3.5 Supervised Learning: Assessing Model Accuracy Simulated toy dataset 3.6 Supervised Learning: Assessing Model Accuracy Simulated toy dataset 3.7 Supervised Learning: Bias-Variance Trade-off Why is the Test MSE U-shaped? Suppose we have fit a model \\(\\hat{f}(x)\\) to some training data. Let the “true” model be \\(Y=f(x)+\\epsilon\\). Let \\((x_0, y_0)\\) be a test observation. We have, \\[\\underbrace{E\\left(y_0-\\hat{f}(x_0)\\right)^2}_{total \\ error}=\\underbrace{Var\\left(\\hat{f}(x_0)\\right)}_{source \\ 3} + \\underbrace{\\left[Bias\\left(\\hat{f}(x_0)\\right)\\right]^2}_{source \\ 2}+\\underbrace{Var(\\epsilon)}_{source \\ 1}\\] where \\(Bias\\left(\\hat{f}(x_0)\\right)=E\\left(\\hat{f}(x_0)\\right)-f(x_0)\\) 3.8 Supervised Learning: Bias-Variance Trade-off source 1: how \\(y\\) differs from “true” \\(f(x)\\) source 2: how \\(\\hat{f}(x)\\) (when fitted to the test data) differs from \\(f(x)\\) source 3: how \\(\\hat{f}(x)\\) varies among different randomly selected possible training data 3.9 Supervised Learning: Bias-Variance Trade-off 3.10 Question!!! As the flexibility of a model \\(\\hat{f}(\\mathbf{X})\\) increases, its variance \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) its bias \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) its training MSE \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) its test MSE \\(\\underline{\\hspace{5cm}}\\) (increases/decreases/U-shaped) 3.11 Simple Linear Regression (SLR) Response \\(Y\\) and a single predictor variable \\(X\\). We assume \\[Y=f(\\mathbf{X}) + \\epsilon=\\beta_0 + \\beta_1 X+ \\epsilon\\] Parameters/Coefficients: \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope) Training data: \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\) We use training data to find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) such that \\[\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x\\] 3.12 Question!!! Linear regression is \\(\\underline{\\hspace{5cm}}\\) (supervised/unsupervised) \\(\\underline{\\hspace{5cm}}\\) (regression/classification) \\(\\underline{\\hspace{5cm}}\\) (parametric/non-parametric) 3.13 SLR: Estimating Parameters Training data: \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\) Observed response: \\(y_i\\) for \\(i=1,\\ldots,n\\) Predicted response: \\(\\hat{y}_i\\) for \\(i=1, \\ldots, n\\) Residual: \\(e_i=y_i - \\hat{y}_i\\) for \\(i=1, \\ldots, n\\) Residual Sum of Squares (RSS): \\(RSS =e^2_1+e^2_2+\\ldots+e^2_n\\) Problem: Find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimizes \\(RSS\\) 3.14 SLR: Estimating Parameters Figure 3.1: Three-dimensional plot of RSS 3.15 SLR: Estimating Parameters The least squares regression coefficient estimates are \\[\\hat{\\beta}_1=\\dfrac{\\displaystyle\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\displaystyle\\sum_{i=1}^n (x_i-\\bar{x})^2}\\] \\[\\hat{\\beta}_0=\\bar{y}- \\hat{\\beta}_1 \\ \\bar{x}\\] where \\(\\bar{y}=\\dfrac{1}{n} \\displaystyle\\sum_{i=1}^n y_i\\) and \\(\\bar{x}=\\dfrac{1}{n} \\displaystyle\\sum_{i=1}^n x_i\\). 3.16 Ames Housing Dataset Contains data on 881 properties in Ames, IA. ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # read in the dataset after specifying directory 3.17 Ames Housing dataset Variable descriptions: Sale_Price: Property sale price in USD Gr_Liv_Area: Above grade (ground) living area square feet Garage_Type: Garage location Garage_Cars: Size of garage in car capacity Garage_Area: Size of garage in square feet Street: Type of road access to property Utilities: Type of utilities available Pool_Area: Pool area in square feet Neighborhood: Physical locations within Ames city limits Screen_Porch: Screen porch area in square feet Overall_Qual: Rates the overall material and finish of the house Lot_Area: Lot size in square feet Lot_Frontage: Linear feet of street connected to property MS_SubClass: Identifies the type of dwelling involved in the sale. Misc_Val: Dollar value of miscellaneous feature Open_Porch_SF: Open porch area in square feet TotRms_AbvGrd: Total rooms above grade (does not include bathrooms) First_Flr_SF: First Floor square feet Second_Flr_SF: Second floor square feet Year_Built: Original construction date 3.18 SLR: Estimating Parameters Ames Housing dataset slrfit &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames) # fit the SLR model summary(slrfit) # produce result summaries of the SLR model ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Residuals: ## Min 1Q Median 3Q Max ## -496577 -33108 -3216 22644 321629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10546.305 6678.534 1.579 0.115 ## Gr_Liv_Area 114.504 4.221 27.127 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60480 on 766 degrees of freedom ## (113 observations deleted due to missingness) ## Multiple R-squared: 0.49, Adjusted R-squared: 0.4893 ## F-statistic: 735.9 on 1 and 766 DF, p-value: &lt; 2.2e-16 3.19 SLR: Model Ames Housing dataset We have, \\(\\hat{\\beta}_0=10546.305\\) and \\(\\hat{\\beta}_1=114.504\\). The least squares regression model is \\[\\widehat{\\text{Sale_Price}} = 10546.305 + 114.504 \\times \\text{Gr_Liv_Area}\\] 3.20 SLR: Model Ames Housing dataset ggplot(data = ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point() + # create scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE) # add the SLR line ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 113 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 113 rows containing missing values (`geom_point()`). 3.21 SLR: Prediction Ames Housing dataset For a house with Gr_Liv_Area equaling 1000 square feet, we have \\[\\widehat{\\text{Sale_Price}} = 10546.305 + 114.504 \\times \\text{Gr_Liv_Area} = 10546.305 + 114.504 \\times 1003 \\approx 125393.8 \\ \\text{USD}\\] predict(slrfit, newdata = data.frame(Gr_Liv_Area = 1003)) # predict response for a given value of x ## 1 ## 125393.6 The observed sale price of a property with Gr_Liv_Area equaling 1000 square feet is 142500 USD. Then, \\(\\text{residual} = \\text{observed}-\\text{predicted} \\approx 142500 - 125393.6 \\approx 17106.4\\) Note: We should not attempt to predict the response for a value of the predictor that lies outside the range of our data. This is called extrapolation, and the predictions tend to be unreliable. 3.22 SLR: Interpreting Parameters Ames Housing dataset \\(\\hat{\\beta}_0=10546.305\\): When Gr_Liv_Area is 0 square feet, the predicted sale price is approximately 10546.305 USD. \\(\\hat{\\beta}_1=114.504\\): For every 1 square foot increase in Gr_Liv_Area, Sale_Price is expected to increase by approximately 114.504 USD. 3.23 SLR: Assessing Accuracy of Model Residual Standard Error (RSE) \\[RSE=\\sqrt{\\dfrac{RSS}{n-2}}\\] RSE is considered as a measure of the lack of fit of the linear model to the data. It is the average amount that the response will deviate from the true regression line. It is measured in the units of the response variable. \\(R^2\\) statistic \\[R^2=\\dfrac{TSS-RSS}{TSS}\\] where \\(TSS=\\sum_{i=1}^n \\left(y_i-\\bar{y}\\right)^2\\) \\(R^2\\) measures the proportion of variability in the response that is explained by the linear regression model using the predictor variable. \\(R^2\\) is unitless, \\(0 &lt; R^2 &lt; 1\\), and is generally expressed as a percentage. 3.24 SLR: Assessing Accuracy of Model Ames Housing dataset summary(slrfit) # produce result summaries of the SLR model ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames) ## ## Residuals: ## Min 1Q Median 3Q Max ## -496577 -33108 -3216 22644 321629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10546.305 6678.534 1.579 0.115 ## Gr_Liv_Area 114.504 4.221 27.127 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60480 on 766 degrees of freedom ## (113 observations deleted due to missingness) ## Multiple R-squared: 0.49, Adjusted R-squared: 0.4893 ## F-statistic: 735.9 on 1 and 766 DF, p-value: &lt; 2.2e-16 3.25 Your Turn!!! The Advertising.csv dataset contains data on the sales (in 1000 units) of a product in 200 different markets, along with advertising budgets (in $1000) for the product for three different media: TV, radio, and newspaper. Create an SLR model slrfit1 with sales as response and TV as predictor. Display the least squares regression line on a scatterplot. Create another SLR model slrfit2 with sales as response and radio as predictor. Predict the sales when the radio advertising budgets are $20,000 and $40,000. Between slrfit1 and slrfit2, which model is better in terms of the variability explained within sales? Between TV and radio advertising budgets, which would result in a higher increase in sales for an additional $1000 investment? 3.26 Question!!! Consider the population model \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\). The estimated model is \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\). Reflecting on the concepts from week 1, fill in the blanks below. If \\(\\beta_0\\) and \\(\\beta_1\\) were known (“truth” known), the discrepancy between response \\(Y\\) and \\(\\beta_0 + \\beta_1 X\\) is related to the \\(\\underline{\\hspace{5cm}}\\) (irreducible/reducible) error. The inaccuracy of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) as estimates of \\(\\beta_0\\) and \\(\\beta_1\\) is related to the \\(\\underline{\\hspace{5cm}}\\) (irreducible/reducible) error. 3.27 Question!!! The least squares approach minimizes the sum of squared predictor values. minimizes the sum of squared response values. minimizes the sum of squared residuals. maximizes the sum of squared residuals. 3.28 Question!!! Geometrically, the residual for the \\(i^{th}\\) observation in a regression model is the horizontal distance between the observed response and the vertical axis. distance of the predicted response from the horizontal axis. distance of the predicted response from the vertical axis. vertical distance between the observed response and predicted response. 3.29 Regression: Conditional Averaging Ames Housing dataset ggplot(data = ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point() + # create scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE) # add the SLR line ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 113 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 113 rows containing missing values (`geom_point()`). What is a good value of \\(\\hat{f}(x)\\), say at \\(x=1008\\)? 3.30 Regression: Conditional Averaging What is a good value of \\(\\hat{f}(x)\\), say at \\(x=1008\\)? A possible value is \\[\\hat{f}(x)=E(Y|x=1008)\\] \\(E(Y|x=1008)\\) means expected value, or, the average of the observed responses at \\(x=1008\\). But we may not observe responses for certain \\(x\\) values. 3.31 K-Nearest Neighbors Regression Non-parametric approach Given a value for \\(K\\) and a test data point \\(x_0\\), \\[\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) \\] where \\(\\mathcal{N}_0\\) is known as the neighborhood of \\(x_0\\). The method is based on the concept of closeness of \\(x_i\\)’s from \\(x_0\\) for inclusion in the neighborhood \\(\\mathcal{N}_0\\). Usually, the Euclidean distance is used as a measure of closeness. The Euclidean distance between two \\(p\\)-dimensional vectors \\(\\mathbf{a}=(a_1, a_2, \\ldots, a_p)\\) and \\(\\mathbf{b}=(b_1, b_2, \\ldots, b_p)\\) is \\[||\\mathbf{a}-\\mathbf{b}||_2 = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \\ldots + (a_p-b_p)^2}\\] 3.32 K-Nearest Neighbors Regression: Fit Ames Housing dataset library(caret) # load the caret package knnfit1 &lt;- knnreg(Sale_Price ~ Gr_Liv_Area, data = ames, k = 1) # 1-nn regression knnfit5 &lt;- knnreg(Sale_Price ~ Gr_Liv_Area, data = ames, k = 5) # 5-nn regression 3.33 K-Nearest Neighbors Regression: Prediction Ames Housing dataset nearest_neighbors &lt;- ames %&gt;% select(Sale_Price, Gr_Liv_Area) %&gt;% mutate(distance = sqrt((1008-Gr_Liv_Area)^2)) %&gt;% # calculate distance arrange(distance) # sort by increasing distance predict(knnfit1, newdata = data.frame(Gr_Liv_Area = 1008)) # 1-nn prediction ## [1] 135166.7 predict(knnfit5, newdata = data.frame(Gr_Liv_Area = 1008)) # 5-nn prediction ## [1] 118280 3.34 Regression Methods: Comparison Ames Housing dataset ## Warning: Removed 113 rows containing missing values (`geom_point()`). Figure 3.2: dashed cyan: 1-nn fit, dotted red: 5-nn fit, blue: linear regression fit 3.35 Your Turn!!! For the Advertising.csv dataset, create a 10-nearest neighbors fit knnfit10 with sales as response and TV as predictor. Obtain the predicted sales for \\(\\text{TV} = 225,000\\). 3.36 Question!!! As \\(k\\) in KNN regression increases, the flexibility of the fit \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) the bias of the fit \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) the variance of the fit \\(\\underline{\\hspace{5cm}}\\) (increases/decreases) "],["multiple-linear-regression-mlr.html", "Chapter 4 Multiple Linear Regression (MLR) 4.1 MLR: Estimating Parameters 4.2 MLR: Estimating Parameters 4.3 MLR: Estimating Parameters 4.4 MLR: Interpreting Parameters 4.5 MLR: Prediction 4.6 MLR: Assessing Accuracy of Model 4.7 Your Turn!!! 4.8 MLR: Assessing Accuracy of Model 4.9 Question!!! 4.10 K-Nearest Neighbors Regression (multiple predictors) 4.11 K-Nearest Neighbors Regression (multiple predictors) 4.12 Linear Regression vs K-Nearest Neighbors 4.13 Classification Problems 4.14 Classification Problems: Example 4.15 Classification Problems: Example 4.16 Why Not Linear Regression? 4.17 Why Not Linear Regression? 4.18 Logistic Regression 4.19 Logistic Regression 4.20 Your Turn!!! 4.21 Logistic Regression: Example 4.22 Logistic Regression: Estimating Parameters 4.23 Logistic Regression: Individual Predictions 4.24 Logistic Regression: Test Set Predictions 4.25 Logistic Regression: Test Set Predictions 4.26 Logistic Regression: Performance 4.27 Confusion Matrix Terms", " Chapter 4 Multiple Linear Regression (MLR) Response \\(Y\\) and more than one predictor variable. We assume \\[Y=f(\\mathbf{X}) + \\epsilon=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\\] \\(\\beta_j\\) quantifies the association between the \\(j^{th}\\) predictor and the response. 4.1 MLR: Estimating Parameters We use training data to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) such that \\[\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_1 + \\ldots + \\hat{\\beta}_p \\ x_p\\] Observed response: \\(y_i\\) for \\(i=1,\\ldots,n\\) Predicted response: \\(\\hat{y}_i\\) for \\(i=1, \\ldots, n\\) Residual: \\(e_i=y_i - \\hat{y}_i\\) for \\(i=1, \\ldots, n\\) Residual Sum of Squares (RSS): \\(RSS =e^2_1+e^2_2+\\ldots+e^2_n\\) Problem: Find \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) which minimizes \\(RSS\\) 4.2 MLR: Estimating Parameters 4.3 MLR: Estimating Parameters Ames Housing dataset ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # read in the dataset after specifying directory mlrfit &lt;- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames) # fit the MLR model summary(mlrfit) # produce result summaries of the MLR model ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames) ## ## Residuals: ## Min 1Q Median 3Q Max ## -469355 -29029 -590 18691 301689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.242e+06 1.234e+05 -18.17 &lt;2e-16 *** ## Gr_Liv_Area 9.781e+01 3.641e+00 26.87 &lt;2e-16 *** ## Year_Built 1.155e+03 6.322e+01 18.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 50500 on 765 degrees of freedom ## (113 observations deleted due to missingness) ## Multiple R-squared: 0.6449, Adjusted R-squared: 0.644 ## F-statistic: 694.7 on 2 and 765 DF, p-value: &lt; 2.2e-16 4.4 MLR: Interpreting Parameters Ames Housing dataset \\(\\hat{\\beta}_0=-2.242e+06\\): With Gr_Liv_Area equaling 0 square feet, and Year_Built equaling 0, the predicted Sale_Price is approximately -2.242e+06 USD. The interpretation is not meaningful in this context. \\(\\hat{\\beta}_1=9.781e+01\\): With Year_Built remaining fixed, an additional 1 square foot of Gr_Liv_Area leads to an increase in Sale_Price by approximately 98 USD. \\(\\hat{\\beta}_2=1.155e+03\\): With Gr_Liv_Area remaining fixed, an additional 1 year on Year_Built leads to an increase in Sale_Price by approximately 1155 USD. 4.5 MLR: Prediction Ames Housing dataset Prediction of Sale_Price when Gr_Liv_Area is 1000 SF for a house built in 1990. predict(mlrfit, newdata = data.frame(Gr_Liv_Area = 1000, Year_Built = 1990)) # obtain prediction ## 1 ## 154506.3 4.6 MLR: Assessing Accuracy of Model Residual Standard Error \\[RSE=\\sqrt{\\dfrac{RSS}{n-p-1}}\\] \\(R^2\\) statistic \\[R^2=\\dfrac{TSS-RSS}{TSS} = 1 - \\dfrac{RSS}{TSS}\\] Adjusted \\(R^2\\) statistic \\[\\text{Adjusted} \\ R^2 = 1 - \\dfrac{RSS/(n-p-1)}{TSS/(n-1)}\\] 4.7 Your Turn!!! With the Advertising dataset, create two additional models with sales as response: mlrfit1: MLR model with TV and radio as predictors mlrfit2: MLR model with TV, radio, and newspaper as predictors For each model, note \\(p\\) (the number of predictors), \\(R^2\\), \\(\\text{Adjusted} \\ R^2\\), \\(RSS\\), and \\(RSE\\). 4.8 MLR: Assessing Accuracy of Model cor(advertising) # obtain correlation matrix ## TV radio newspaper sales ## TV 1.00000000 0.05480866 0.05664787 0.7822244 ## radio 0.05480866 1.00000000 0.35410375 0.5762226 ## newspaper 0.05664787 0.35410375 1.00000000 0.2282990 ## sales 0.78222442 0.57622257 0.22829903 1.0000000 4.9 Question!!! As we add variables to the linear regression model, (Select all that apply) the RSE always decreases. the RSS always decreases. the \\(R^2\\) always increases. the \\(\\text{Adjusted} \\ R^2\\) always increases. the number of parameters always increases. 4.10 K-Nearest Neighbors Regression (multiple predictors) It is important to scale (subtract mean and divide by standard deviation) the predictors when considering KNN regression so that the Euclidean distance is not dominated by a few of them with large values. Ames Housing dataset ames_scaled &lt;- ames %&gt;% dplyr::select(Sale_Price, Gr_Liv_Area, Year_Built) %&gt;% # select required variables mutate(Gr_Liv_Area_scaled = scale(Gr_Liv_Area), Year_Built_scaled = scale(Year_Built)) # scale predictors head(ames_scaled) ## # A tibble: 6 × 5 ## Sale_Price Gr_Liv_Area Year_Built Gr_Liv_Area_scaled[,1] Year_Built_scaled[,1] ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 244000 2110 1968 1.19 -0.102 ## 2 213500 1338 2001 -0.304 0.985 ## 3 185000 1187 1992 -0.596 0.688 ## 4 394432 1856 2010 0.697 1.28 ## 5 190000 1844 1977 0.674 0.194 ## 6 149000 NA 1970 NA -0.0359 4.11 K-Nearest Neighbors Regression (multiple predictors) Ames Housing dataset library(caret) # load library knnfit10 &lt;- knnreg(Sale_Price ~ Gr_Liv_Area_scaled + Year_Built_scaled, data = ames_scaled, k = 10) # 10-nn regression It is also important to apply scaling to test data points before prediction. Suppose, you want predictions for Gr_Liv_Area = 1000 SF, and Year_Built = 1990, then # obtain 10-nn prediction predict(knnfit10, newdata = data.frame(Gr_Liv_Area_scaled = (1000 - mean(ames$Gr_Liv_Area, na.rm = TRUE))/sd(ames$Gr_Liv_Area, na.rm = TRUE), Year_Built_scaled = (1990 - mean(ames$Year_Built))/sd(ames$Year_Built))) ## [1] 148850 4.12 Linear Regression vs K-Nearest Neighbors Linear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric. Linear regression works for regression problems (\\(Y\\) numerical), KNN can be used for both regression and classification (\\(Y\\) qualitative). Linear regression is interpretable, KNN is not. Linear regression can accommodate qualitative predictors and can be extended to include interaction terms as well. Using Euclidean distance with KNN does not allow for qualitative predictors. In terms of prediction, KNN can be pretty good for small \\(p\\), that is, \\(p \\le 4\\) and large \\(n\\). Performance of KNN deteriorates as \\(p\\) increases - curse of dimensionality. 4.13 Classification Problems Response \\(Y\\) is qualitative (categorical). The objective is to build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\) that assigns a class label to a future unlabeled (unseen) observation and understand the relationship between the predictors and response. There can be two types of predictions based on the research problem. Class probabilities Class labels 4.14 Classification Problems: Example Default dataset ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 table(Default$default) ## ## No Yes ## 9667 333 4.15 Classification Problems: Example For some algorithms, we might need to convert the categorical response to numeric values. Default dataset Default$default_id &lt;- ifelse(Default$default == &quot;Yes&quot;, 1, 0) # create 0/1 variable head(Default, 10) # print first ten observations ## default student balance income default_id ## 1 No No 729.5265 44361.625 0 ## 2 No Yes 817.1804 12106.135 0 ## 3 No No 1073.5492 31767.139 0 ## 4 No No 529.2506 35704.494 0 ## 5 No No 785.6559 38463.496 0 ## 6 No Yes 919.5885 7491.559 0 ## 7 No No 825.5133 24905.227 0 ## 8 No Yes 808.6675 17600.451 0 ## 9 No No 1161.0579 37468.529 0 ## 10 No No 0.0000 29275.268 0 4.16 Why Not Linear Regression? Default dataset slrfit &lt;- lm(default_id ~ balance, data = Default) # fit SLR summary(slrfit$fitted.values) # summary of y_hats ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.07519 -0.01263 0.03178 0.03330 0.07628 0.26953 ## `geom_smooth()` using formula = &#39;y ~ x&#39; Linear regression does not model probabilities well. Linear regression might produce probabilities less than zero or bigger than one. 4.17 Why Not Linear Regression? Suppose we have a response \\(Y\\), \\(Y=\\begin{cases} 1; &amp; \\text{if stroke} \\\\ 2; &amp; \\text{if drug overdose} \\\\ 3; &amp; \\text{if epileptic seizure} \\end{cases}\\) Linear regression suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure. 4.18 Logistic Regression Consider a one-dimensional two-class problem. Transform the linear model \\(\\beta_0 + \\beta_1 \\ X\\) so that the output is a probability. Use logistic or sigmoid function \\[g(t)=\\dfrac{e^t}{1+e^t} \\ \\ \\ \\text{for} \\ t \\in \\mathcal{R}\\] Suppose \\(p(X)=P(Y=1|X)\\). Then, \\[p(X)=g\\left(\\beta_0 + \\beta_1 \\ X\\right)=\\dfrac{e^{\\beta_0 + \\beta_1 \\ X}}{1+e^{\\beta_0 + \\beta_1 \\ X}}\\] \\(e \\approx 2.71828\\) is a mathematical constant (Euler’s number). 4.19 Logistic Regression Default dataset 4.20 Your Turn!!! Consider \\(p(X)=P(Y=1|X) = \\dfrac{e^{\\beta_0 + \\beta_1 \\ X}}{1+e^{\\beta_0 + \\beta_1 \\ X}}\\). Find \\(1-p(X)\\) \\(\\ln \\left(\\dfrac{p(X)}{1-p(X)}\\right)\\) 4.21 Logistic Regression: Example Attrition dataset library(modeldata) # load library data(&quot;attrition&quot;) # load dataset We will consider Attrition as the response variable. To mimic real-world ML practices, we will split the dataset into a tranining and test set. We will build our model on the training set and evaluate its performance on the test set. set.seed(011723) # fix the random number generator for reproducibility library(caret) # load library train_index &lt;- createDataPartition(y = attrition$Attrition, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets attrition_train &lt;- attrition[train_index,] # training data, use this dataset to build model attrition_test &lt;- attrition[-train_index,] # test data, use this dataset to evaluate model&#39;s performance 4.22 Logistic Regression: Estimating Parameters Attrition dataset Let’s build a logistic regression model with MonthlyIncome as the predictor. logregfit &lt;- glm(Attrition ~ MonthlyIncome, data = attrition_train, family = binomial) # fit logistic regression model summary(logregfit) # obtain results ## ## Call: ## glm(formula = Attrition ~ MonthlyIncome, family = binomial, data = attrition_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.7776 -0.6676 -0.5782 -0.3121 2.6570 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.040e-01 1.441e-01 -6.272 3.56e-10 *** ## MonthlyIncome -1.307e-04 2.413e-05 -5.418 6.04e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1040.5 on 1176 degrees of freedom ## Residual deviance: 1001.5 on 1175 degrees of freedom ## AIC: 1005.5 ## ## Number of Fisher Scoring iterations: 5 4.23 Logistic Regression: Individual Predictions Attrition dataset For MonthlyIncome=$5000, \\[\\hat{p}(X)=\\dfrac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}=\\dfrac{e^{-0.904 + (-0.0001307 \\times 5000)}}{1+e^{-0.904 + (-0.0001307 \\times 5000)}}=0.174\\] predict(logregfit, newdata = data.frame(MonthlyIncome = 5000)) # obtain log-odds predictions ## 1 ## -1.557622 predict(logregfit, newdata = data.frame(MonthlyIncome = 5000), type = &quot;response&quot;) # obtain probability predictions ## 1 ## 0.1739881 4.24 Logistic Regression: Test Set Predictions Attrition dataset To predict probabilities for observations in the test set, we use logreg_prob_preds &lt;- predict(logregfit, newdata = attrition_test, type = &quot;response&quot;) # obtain probability predictions head(logreg_prob_preds) # predicted probabilities for first six observations in test set ## 1 13 15 18 19 39 ## 0.1561133 0.1695803 0.1896750 0.2223808 0.2370187 0.2261332 4.25 Logistic Regression: Test Set Predictions Attrition dataset Set a threshold to obtain predicted class labels. The following uses a threshold of 0.5. threshold &lt;- 0.5 # set threshold logreg_class_preds &lt;- factor(ifelse(logreg_prob_preds &gt; threshold, &quot;Yes&quot;, &quot;No&quot;)) # obtain class predictions 4.26 Logistic Regression: Performance Attrition dataset library(caret) # load package &#39;caret&#39; # create confusion matrix levels(logreg_class_preds) = c(&quot;No&quot;, &quot;Yes&quot;) # create &#39;Yes&#39; factor level (not always required) confusionMatrix(data = relevel(logreg_class_preds, ref = &quot;Yes&quot;), reference = relevel(attrition_test$Attrition, ref = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 0 0 ## No 47 246 ## ## Accuracy : 0.8396 ## 95% CI : (0.7925, 0.8797) ## No Information Rate : 0.8396 ## P-Value [Acc &gt; NIR] : 0.5388 ## ## Kappa : 0 ## ## Mcnemar&#39;s Test P-Value : 1.949e-11 ## ## Sensitivity : 0.0000 ## Specificity : 1.0000 ## Pos Pred Value : NaN ## Neg Pred Value : 0.8396 ## Prevalence : 0.1604 ## Detection Rate : 0.0000 ## Detection Prevalence : 0.0000 ## Balanced Accuracy : 0.5000 ## ## &#39;Positive&#39; Class : Yes ## 4.27 Confusion Matrix Terms Reference class labels 1 0 ———– ———- ———– 1 0 "],["k-nearest-neighbors-classifier.html", "Chapter 5 K-Nearest Neighbors Classifier 5.1 Your Turn!!! 5.2 K-Nearest Neighbors Classifier: Split Data 5.3 K-Nearest Neighbors Classifier: Build Model 5.4 K-Nearest Neighbors Classifier: Predictions 5.5 K-Nearest Neighbors Classifier: Performance 5.6 ROC Curve and AUC 5.7 ROC Curve and AUC 5.8 Data Splitting 5.9 Resampling Methods 5.10 Cross-Validation (CV) 5.11 Leave-One-Out Cross-Validation (LOOCV) 5.12 Leave-One-Out Cross-Validation (LOOCV) 5.13 \\(k\\)-Fold Cross-Validation 5.14 \\(k\\)-Fold Cross-Validation: Implementation 5.15 \\(k\\)-Fold Cross-Validation: Implementation 5.16 \\(k\\)-Fold Cross-Validation: Implementation 5.17 \\(k\\)-Fold Cross-Validation: Implementation 5.18 \\(k\\)-Fold Cross-Validation: Implementation 5.19 \\(k\\)-Fold Cross-Validation: Implementation 5.20 \\(k\\)-Fold Cross-Validation: Results 5.21 Final Model and Prediction Error Estimate 5.22 Variable Importance 5.23 Bias-Variance Trade-off for LOOCV and \\(k\\)-fold CV 5.24 Your Turn!!! 5.25 Your Turn!!!: Split Data 5.26 Your Turn!!!: Perform CV 5.27 Your Turn!!!: Observe CV Results 5.28 Your Turn!!!: Final Model 5.29 Mid-Term Check", " Chapter 5 K-Nearest Neighbors Classifier Given a value for \\(K\\) and a test data point \\(x_0\\), \\[P(Y=j | X=x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} I(y_i = j)\\] where \\(\\mathcal{N}_0\\) is known as the neighborhood of \\(x_0\\). 5.1 Your Turn!!! Build a 10-NN classifier with the Default dataset. Consider default as the response variable and balance as the predictor. Follow the steps below. Load the dataset into R. Observe the dataset, specifically the response variable. Perform a 70-30 split of the original dataset. With the training data, construct a 10-NN classifier. See help page of function knn3. Predict class probabilities on the test data points. See help page of function predict.knn3. Look carefully at the object that is created. Obtain predicted class labels for a threshold of 0.3. Create the confusion matrix between the observed and predicted class labels. 5.2 K-Nearest Neighbors Classifier: Split Data Default dataset library(ISLR2) # load library data(&quot;Default&quot;) # load dataset set.seed(012423) # fix the random number generator for reproducibility library(caret) # load library train_index &lt;- createDataPartition(y = Default$default, p = 0.7, list = FALSE) # split available data into 70% training and 30% test datasets Default_train &lt;- Default[train_index,] # training data, use this dataset to build model Default_test &lt;- Default[-train_index,] # test data, use this dataset to evaluate model&#39;s performance 5.3 K-Nearest Neighbors Classifier: Build Model Default dataset library(caret) # load package &#39;caret&#39; knnfit &lt;- knn3(default ~ balance, data = Default_train, k = 10) # fit 10-nn model 5.4 K-Nearest Neighbors Classifier: Predictions Default dataset knn_prob_preds &lt;- predict(knnfit, newdata = Default_test, type = &quot;prob&quot;) # obtain predictions as probabilities threshold &lt;- 0.3 # set threshold knn_class_preds &lt;- factor(ifelse(knn_prob_preds[,2] &gt; threshold, &quot;Yes&quot;, &quot;No&quot;)) # obtain predictions as class labels 5.5 K-Nearest Neighbors Classifier: Performance Default dataset # create confusion matrix confusionMatrix(data = relevel(knn_class_preds, ref = &quot;Yes&quot;), reference = relevel(Default_test$default, ref = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 39 34 ## No 60 2866 ## ## Accuracy : 0.9687 ## 95% CI : (0.9618, 0.9746) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 0.327332 ## ## Kappa : 0.4377 ## ## Mcnemar&#39;s Test P-Value : 0.009922 ## ## Sensitivity : 0.39394 ## Specificity : 0.98828 ## Pos Pred Value : 0.53425 ## Neg Pred Value : 0.97949 ## Prevalence : 0.03301 ## Detection Rate : 0.01300 ## Detection Prevalence : 0.02434 ## Balanced Accuracy : 0.69111 ## ## &#39;Positive&#39; Class : Yes ## 5.6 ROC Curve and AUC The ROC (Receiver Operating Characteristics) curve is a popular graphic for comparing different classifiers across all possible thresholds. The ROC curve plots the Specificity (1-false positive rate) along the x-axis and the Sensitivity (true positive rate) along the y-axis. Another popular metric for comparing classifiers is the AUC (Area Under the ROC Curve). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. 5.7 ROC Curve and AUC Default dataset library(pROC) # load library ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var # create object for ROC curve for KNN fit roc_object_knn &lt;- roc(response = Default_test$default, predictor = knn_prob_preds[,2]) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases # plot ROC curve plot(roc_object_knn, col = &quot;red&quot;) # obtain AUC&#39;s auc(roc_object_knn) ## Area under the curve: 0.8564 5.8 Data Splitting Available data split into training and test datasets. Training set: these data are used to develop feature sets, train our algorithms, tune parameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production). Test set: having chosen a final model, these data are used to obtain an unbiased estimate of the model’s performance. It is critical that the test set not be used prior to selecting your final model. Assessing results on the test set prior to final model selection biases the model selection process since the testing data will have become part of the model development process. 5.9 Resampling Methods Idea: Repeatedly draw samples from the training data and refit a model on each sample, and evaluate its performance on the other parts. Objective: To obtain additional information about the fitted model. Cross-Validation (CV) is probably the most widely used resampling method. It is a general approach that can be applied to almost any statistical learning method. 5.10 Cross-Validation (CV) Used for model selection: select the optimum level of flexibility (tune hyperparameters) or compare different models to choose the best one model assessment: evaluate the performance of a model (estimate its test error) We will talk about Leave-One-Out Cross-Validation (LOOCV) \\(k\\)-Fold Cross-Validation 5.11 Leave-One-Out Cross-Validation (LOOCV) 5.12 Leave-One-Out Cross-Validation (LOOCV) Advantages LOOCV will give approximately unbiased estimates of the test error, since each training set contains \\(n−1\\) observations, which is almost as many as the number of observations in the full training dataset. Performing LOOCV multiple times will always yield the same results. Disadvantages Can be potentially expensive to implement, specially for large \\(n\\). LOOCV error estimate can have high variance. 5.13 \\(k\\)-Fold Cross-Validation Randomly divide the training data into \\(k\\) groups or folds (approximately equal size). Consider one of these folds as the validation set. Fit the model on the remaining \\(k-1\\) folds combined, and obtain predictions for the \\(k^{th}\\) fold. Repeat for all \\(k\\) folds. 5.14 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # load dataset Consider Sale_Price as the response variable. We will compare the following three linear regression models: with Garage_Area as the only predictor; with Overall_Qual as the only predictor; with Garage_Area, Year_Built, and Overall_Qual as predictors. 5.15 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Split the data into training and test data. set.seed(012423) # fix the random number generator for reproducibility library(caret) # load library train_index &lt;- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets ames_train &lt;- ames[train_index,] # training data, use this dataset to build model ames_test &lt;- ames[-train_index,] # test data, use this dataset to evaluate model&#39;s performance 5.16 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Define CV specifications. cv_specs_kcv &lt;- trainControl(method = &quot;repeatedcv&quot;, # CV method number = 10, # number of folds repeats = 5) # each repeated 5 times 5.17 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Implement \\(k\\)-fold CV with the first model. m1 &lt;- train(form = Sale_Price ~ Garage_Area, # specify model data = ames_train, # specify dataset method = &quot;lm&quot;, # specify type of model trControl = cv_specs_kcv, # CV specifications metric = &quot;RMSE&quot;) # metric to evaluate model m1 # summary of LOOCV ## Linear Regression ## ## 706 samples ## 1 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 636, 635, 637, 636, 635, 634, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 62931.53 0.4437306 43590.34 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE m1$results # estimate and variability of metrics ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 62931.53 0.4437306 43590.34 12411.41 0.1024043 5478.664 5.18 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Implement \\(k\\)-fold CV with the second model. m2 &lt;- train(form = Sale_Price ~ Overall_Qual, data = ames_train, method = &quot;lm&quot;, trControl = cv_specs_kcv, metric = &quot;RMSE&quot;) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading m2 ## Linear Regression ## ## 706 samples ## 1 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 635, 636, 635, 636, 636, 636, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 46268.31 0.6968243 31912.75 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE m2$results ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 46268.31 0.6968243 31912.75 10192.63 0.09720067 4282.579 5.19 \\(k\\)-Fold Cross-Validation: Implementation Ames Housing dataset Implement \\(k\\)-fold CV with the third model. m3 &lt;- train(form = Sale_Price ~ Garage_Area + Year_Built + Overall_Qual, data = ames_train, method = &quot;lm&quot;, trControl = cv_specs_kcv, metric = &quot;RMSE&quot;) m3 ## Linear Regression ## ## 706 samples ## 3 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 636, 634, 635, 636, 636, 636, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 42626.49 0.745911 28441.93 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE m3$results ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 42626.49 0.745911 28441.93 10024.47 0.1022967 3484.786 5.20 \\(k\\)-Fold Cross-Validation: Results Ames Housing dataset Compare \\(k\\)-fold CV results for different models. # create data frame to plot results df &lt;- data.frame(model_number = 1:3, RMSE = c(m1$results$RMSE, m2$results$RMSE, m3$results$RMSE)) # plot results from LOOCV ggplot(data = df, aes(x = model_number, y = RMSE)) + geom_point() + geom_line() 5.21 Final Model and Prediction Error Estimate Ames Housing dataset # after choosing final (optimal) model, refit final model using ALL training data, and obtain estimate of prediction error from test data m3$finalModel # final model ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Coefficients: ## (Intercept) Garage_Area ## -504482.40 78.99 ## Year_Built Overall_QualAverage ## 320.09 -18028.34 ## Overall_QualBelow_Average Overall_QualExcellent ## -34620.70 163226.47 ## Overall_QualFair Overall_QualGood ## -63928.81 30780.92 ## Overall_QualPoor Overall_QualVery_Excellent ## -76426.87 261135.65 ## Overall_QualVery_Good Overall_QualVery_Poor ## 81711.39 -69470.99 final_model_preds &lt;- predict(m3, newdata = ames_test) # obtain predictions on test data pred_error_est &lt;- sqrt(mean((ames_test$Sale_Price - final_model_preds)^2)) # calculate RMSE (estimate of prediction error) from test data pred_error_est ## [1] 34006.68 5.22 Variable Importance Ames Housing dataset # variable importance library(vip) ## ## Attaching package: &#39;vip&#39; ## The following object is masked from &#39;package:utils&#39;: ## ## vi vip(object = m3, # CV object num_features = 20, # maximum number of predictors to show importance for method = &quot;model&quot;) # model-specific VI scores 5.23 Bias-Variance Trade-off for LOOCV and \\(k\\)-fold CV LOOCV has very less bias. Using \\(k=5\\) or \\(10\\) yields more bias than LOOCV. For LOOCV, the error estimates for each fold are highly (positively) correlated. \\(k\\)-fold CV error estimates are somewhat less correlated. LOOCV error estimate has higher variance than \\(k\\)-fold CV error estimate. Typically, \\(k=5\\) or \\(10\\) is chosen. 5.24 Your Turn!!! Auto dataset Load the dataset. library(ISLR2) # load library data(&quot;Auto&quot;) # load dataset Consider mpg as the response and horsepower as the predictor. Objective: Find the optimum choice of \\(K\\) in the KNN approach with 5-fold CV repeated 5 times. You can use the following steps. Split the data into training and test data (80-20 split). Specify CV specifications using trainControl. Create an object k_grid using the following code. k_grid &lt;- expand.grid(k = seq(1, 100, by = 1)) # creates a grid of k values to be used (1 to 100 in this case) Use the train function to run CV. Use method = “knn”, tuneGrid = k_grid, and metric = “RMSE”. Obtain the results and plot them. What is the optimum \\(k\\) chosen? Create the final model using the optimum \\(k\\) and estimate its prediction error from the test data. 5.25 Your Turn!!!: Split Data Auto dataset set.seed(012423) # fix the random number generator for reproducibility library(caret) # load library train_index &lt;- createDataPartition(y = Auto$mpg, p = 0.8, list = FALSE) # split available data into 80% training and 20% test datasets Auto_train &lt;- Auto[train_index,] # training data, use this dataset to build model Auto_test &lt;- Auto[-train_index,] # test data, use this dataset to evaluate model&#39;s performance 5.26 Your Turn!!!: Perform CV Auto dataset set.seed(012423) # fix the random number generator for reproducibility # CV specifications cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # specify grid of &#39;k&#39; values to search over k_grid &lt;- expand.grid(k = seq(1, 100, by = 1)) # train the KNN model to find optimal &#39;k&#39; knn_cv &lt;- train(form = mpg ~ horsepower, data = Auto_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;RMSE&quot;) 5.27 Your Turn!!!: Observe CV Results Auto dataset knn_cv # model training results ggplot(knn_cv) # plot the model training results for different &#39;k&#39; 5.28 Your Turn!!!: Final Model Auto dataset # final model with optimal &#39;k&#39; chosen from &#39;knn_fit&#39; results knn_cv$finalModel # final model ## 16-nearest neighbor regression model # obtain predictions on test data final_model_preds &lt;- predict(knn_cv, newdata = Auto_test) # estimate prediction error using RMSE sqrt(mean((Auto_test$mpg - final_model_preds)^2)) # RMSE ## [1] 4.314033 5.29 Mid-Term Check Are you comfortable with the concepts? Are you comfortable with the coding aspect? Explain. What study habits have been working for you with this course? What habits haven’t worked? Please mention any comments about the course in general (classwork, live coding, homework, quizzes, course structure and workflow, grading guidelines, etc.) "],["review-of-cv.html", "Chapter 6 Review of CV 6.1 Data Leakage (A Serious, Common Problem) 6.2 Data Preprocessing and Feature Enginnering 6.3 Ames Housing Dataset 6.4 Ames Housing Dataset 6.5 Ames Housing Dataset 6.6 Ames Housing Dataset 6.7 Ames Housing Dataset 6.8 Ames Housing Dataset 6.9 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables 6.10 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables 6.11 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables 6.12 Imputing Missing Entries 6.13 Imputing Missing Entries 6.14 Imputing Missing Entries 6.15 Label Encoding Ordinal Categorical Variables 6.16 Label Encoding Ordinal Categorical Variables 6.17 Label Encoding Ordinal Categorical Variables 6.18 Standardizing (centering and scaling) Numeric Predictors 6.19 Lumping Predictors 6.20 One-hot/dummy Encoding Categorical Predictors 6.21 One-hot/dummy Encoding Categorical Predictors 6.22 Preprocessing Steps 6.23 Preprocessing With recipes Package 6.24 Preprocessing With recipes Package 6.25 Training Model 6.26 Training Model 6.27 Training Model 6.28 Final Model and Test Set Error 6.29 Variable Importance 6.30 Your Turn!!! 6.31 Your Turn!!! Step 1 6.32 Your Turn!!! Step 1 6.33 Your Turn!!! Step 1 6.34 Your Turn!!! Step 1 6.35 Your Turn!!! Step 1 6.36 Your Turn!!! Step 2 6.37 Your Turn!!! Step 3 6.38 Your Turn!!! Step 4 6.39 Your Turn!!! Step 4 6.40 Your Turn!!! Step 4 6.41 Your Turn!!! Step 5 6.42 Your Turn!!! Step 5 6.43 Your Turn!!! Step 5", " Chapter 6 Review of CV KNN Classification: Toy Example Obs. \\(X_1\\) \\(X_2\\) Y 1 1033 1.7 Red 2 1112 1.5 Red 3 1500 1 Red 4 999 1 Green 5 1012 1.5 Green 6 1013 1 Red 7 1233 1 Green 8 1332 1 Red Suppose you implement 4-fold CV. What is the size of each training and validation set? Let’s say the folds are randomly chosen to be observation pairs (2, 3), (4, 7), (1, 8), and (5, 6). 6.1 Data Leakage (A Serious, Common Problem) Data leakage is when information from outside the training data set is used to create the model. Data leakage often occurs when the data preprocessing task is implemented with CV. To minimize this, feature engineering should be done in isolation of each resampling iteration. 6.2 Data Preprocessing and Feature Enginnering Data preprocessing and engineering techniques generally refer to the addition, deletion, or transformation of data. We will cover several fundamental and common preprocessing tasks that can potentially significantly improve modeling performance. Dealing with zero-variance (zv) and/or near-zero variance (nzv) variables Imputing missing entries Label encoding ordinal categorical variables Standardizing (centering and scaling) numeric predictors Lumping predictors One-hot/dummy encoding categorical predictors 6.3 Ames Housing Dataset ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # load dataset # ames &lt;- ames %&gt;% mutate_if(is.character, as.factor) # convert all character variables to factor variables sum(is.na(ames)) # check missing values ## [1] 113 6.4 Ames Housing Dataset glimpse(ames) # check type of variables, missing entries? ## Rows: 881 ## Columns: 20 ## $ Sale_Price &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, … ## $ Gr_Liv_Area &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, … ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, … ## $ Garage_Cars &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,… ## $ Garage_Area &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa… ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, … ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Neighborhood &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest… ## $ Screen_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Overall_Qual &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average… ## $ Lot_Area &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101… ## $ Lot_Frontage &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6… ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_… ## $ Misc_Val &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,… ## $ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7… ## $ First_Flr_SF &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15… ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11… ## $ Year_Built &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20… 6.5 Ames Housing Dataset summary(ames) # check type of variables, missing entries? ## Sale_Price Gr_Liv_Area Garage_Type Garage_Cars ## Min. : 34900 Min. : 334 Attchd :514 Min. :0.000 ## 1st Qu.:129500 1st Qu.:1118 Basment : 10 1st Qu.:1.000 ## Median :160000 Median :1442 BuiltIn : 55 Median :2.000 ## Mean :181115 Mean :1495 CarPort : 5 Mean :1.762 ## 3rd Qu.:213500 3rd Qu.:1728 Detchd :234 3rd Qu.:2.000 ## Max. :755000 Max. :5642 More_Than_Two_Types: 9 Max. :4.000 ## NA&#39;s :113 No_Garage : 54 ## Garage_Area Street Utilities Pool_Area ## Min. : 0.0 Grvl: 4 AllPub:880 Min. : 0.00 ## 1st Qu.: 324.0 Pave:877 NoSeWa: 0 1st Qu.: 0.00 ## Median : 480.0 NoSewr: 1 Median : 0.00 ## Mean : 476.5 Mean : 2.41 ## 3rd Qu.: 592.0 3rd Qu.: 0.00 ## Max. :1418.0 Max. :576.00 ## ## Neighborhood Screen_Porch Overall_Qual Lot_Area ## North_Ames :127 Min. : 0.00 Average :243 Min. : 1300 ## College_Creek : 86 1st Qu.: 0.00 Above_Average:217 1st Qu.: 7449 ## Old_Town : 83 Median : 0.00 Good :177 Median : 9512 ## Northridge_Heights: 52 Mean : 18.11 Very_Good : 99 Mean : 10105 ## Somerset : 50 3rd Qu.: 0.00 Below_Average: 83 3rd Qu.: 11526 ## Edwards : 49 Max. :490.00 Excellent : 38 Max. :159000 ## (Other) :434 (Other) : 24 ## Lot_Frontage MS_SubClass Misc_Val ## Min. : 0.00 One_Story_1946_and_Newer_All_Styles :335 Min. : 0.00 ## 1st Qu.: 43.00 Two_Story_1946_and_Newer :171 1st Qu.: 0.00 ## Median : 63.00 One_and_Half_Story_Finished_All_Ages: 92 Median : 0.00 ## Mean : 57.78 One_Story_PUD_1946_and_Newer : 53 Mean : 37.97 ## 3rd Qu.: 78.00 Duplex_All_Styles_and_Ages : 40 3rd Qu.: 0.00 ## Max. :313.00 One_Story_1945_and_Older : 36 Max. :8300.00 ## (Other) :154 ## Open_Porch_SF TotRms_AbvGrd First_Flr_SF Second_Flr_SF ## Min. : 0.00 Min. : 2.000 Min. : 334 Min. : 0.0 ## 1st Qu.: 0.00 1st Qu.: 5.000 1st Qu.: 877 1st Qu.: 0.0 ## Median : 27.00 Median : 6.000 Median :1092 Median : 0.0 ## Mean : 49.93 Mean : 6.413 Mean :1171 Mean : 319.6 ## 3rd Qu.: 72.00 3rd Qu.: 7.000 3rd Qu.:1426 3rd Qu.: 682.0 ## Max. :742.00 Max. :12.000 Max. :4692 Max. :2065.0 ## ## Year_Built ## Min. :1875 ## 1st Qu.:1954 ## Median :1972 ## Mean :1971 ## 3rd Qu.:2000 ## Max. :2010 ## 6.6 Ames Housing Dataset levels(ames$Overall_Qual) # the levels are NOT properly ordered ## [1] &quot;Above_Average&quot; &quot;Average&quot; &quot;Below_Average&quot; &quot;Excellent&quot; ## [5] &quot;Fair&quot; &quot;Good&quot; &quot;Poor&quot; &quot;Very_Excellent&quot; ## [9] &quot;Very_Good&quot; &quot;Very_Poor&quot; # relevel the levels ames$Overall_Qual &lt;- factor(ames$Overall_Qual, levels = c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;)) levels(ames$Overall_Qual) # the levels are properly ordered ## [1] &quot;Very_Poor&quot; &quot;Poor&quot; &quot;Fair&quot; &quot;Below_Average&quot; ## [5] &quot;Average&quot; &quot;Above_Average&quot; &quot;Good&quot; &quot;Very_Good&quot; ## [9] &quot;Excellent&quot; &quot;Very_Excellent&quot; 6.7 Ames Housing Dataset # split the dataset into training and test sets set.seed(013123) # set seed index &lt;- createDataPartition(ames$Sale_Price, p = 0.7, list = FALSE) # &#39;Sale_Price&#39; is the response ames_train &lt;- ames[index,] # training data ames_test &lt;- ames[-index,] # test data 6.8 Ames Housing Dataset # set up the recipe library(recipes) ## ## Attaching package: &#39;recipes&#39; ## The following object is masked from &#39;package:stringr&#39;: ## ## fixed ## The following object is masked from &#39;package:stats&#39;: ## ## step ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) # sets up the type and role of variables ames_recipe$var_info ## # A tibble: 20 × 4 ## variable type role source ## &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Gr_Liv_Area &lt;chr [2]&gt; predictor original ## 2 Garage_Type &lt;chr [3]&gt; predictor original ## 3 Garage_Cars &lt;chr [2]&gt; predictor original ## 4 Garage_Area &lt;chr [2]&gt; predictor original ## 5 Street &lt;chr [3]&gt; predictor original ## 6 Utilities &lt;chr [3]&gt; predictor original ## 7 Pool_Area &lt;chr [2]&gt; predictor original ## 8 Neighborhood &lt;chr [3]&gt; predictor original ## 9 Screen_Porch &lt;chr [2]&gt; predictor original ## 10 Overall_Qual &lt;chr [3]&gt; predictor original ## 11 Lot_Area &lt;chr [2]&gt; predictor original ## 12 Lot_Frontage &lt;chr [2]&gt; predictor original ## 13 MS_SubClass &lt;chr [3]&gt; predictor original ## 14 Misc_Val &lt;chr [2]&gt; predictor original ## 15 Open_Porch_SF &lt;chr [2]&gt; predictor original ## 16 TotRms_AbvGrd &lt;chr [2]&gt; predictor original ## 17 First_Flr_SF &lt;chr [2]&gt; predictor original ## 18 Second_Flr_SF &lt;chr [2]&gt; predictor original ## 19 Year_Built &lt;chr [2]&gt; predictor original ## 20 Sale_Price &lt;chr [2]&gt; outcome original 6.9 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables A rule of thumb for detecting near-zero variance features is: The fraction of unique values over the sample size is low (say \\(\\le 10\\%\\)). The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say \\(\\ge 20\\%\\)). 6.10 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables # investigate zv/nzv predictors nearZeroVar(ames_train, saveMetrics = TRUE) # check which predictors are zv/nzv ## freqRatio percentUnique zeroVar nzv ## Sale_Price 1.250000 61.8122977 FALSE FALSE ## Gr_Liv_Area 1.666667 68.7702265 FALSE FALSE ## Garage_Type 2.312102 1.1326861 FALSE FALSE ## Garage_Cars 1.922619 0.8090615 FALSE FALSE ## Garage_Area 2.105263 44.0129450 FALSE FALSE ## Street 153.500000 0.3236246 FALSE TRUE ## Utilities 617.000000 0.3236246 FALSE TRUE ## Pool_Area 614.000000 0.8090615 FALSE TRUE ## Neighborhood 1.655172 4.2071197 FALSE FALSE ## Screen_Porch 140.250000 6.9579288 FALSE TRUE ## Overall_Qual 1.176871 1.6181230 FALSE FALSE ## Lot_Area 1.100000 82.0388350 FALSE FALSE ## Lot_Frontage 1.485294 15.0485437 FALSE FALSE ## MS_SubClass 1.892562 2.4271845 FALSE FALSE ## Misc_Val 149.000000 2.2653722 FALSE TRUE ## Open_Porch_SF 20.142857 22.6537217 FALSE FALSE ## TotRms_AbvGrd 1.212329 1.7799353 FALSE FALSE ## First_Flr_SF 2.333333 70.8737864 FALSE FALSE ## Second_Flr_SF 90.750000 34.4660194 FALSE FALSE ## Year_Built 1.214286 15.8576052 FALSE FALSE 6.11 Zero-Variance (zv) and/or Near-Zero Variance (nzv) Variables blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) # filter out zv/nzv predictors 6.12 Imputing Missing Entries Possible imputation techniques: step_impute_median step_impute_mean step_impute_knn step_impute_mode 6.13 Imputing Missing Entries summary(ames_train) # check which predictors have missing entries ## Sale_Price Gr_Liv_Area Garage_Type Garage_Cars ## Min. : 39300 Min. : 334 Attchd :363 Min. :0.000 ## 1st Qu.:129100 1st Qu.:1114 Basment : 6 1st Qu.:1.000 ## Median :160000 Median :1446 BuiltIn : 41 Median :2.000 ## Mean :180748 Mean :1489 CarPort : 5 Mean :1.746 ## 3rd Qu.:213408 3rd Qu.:1742 Detchd :157 3rd Qu.:2.000 ## Max. :745000 Max. :4476 More_Than_Two_Types: 6 Max. :4.000 ## NA&#39;s :77 No_Garage : 40 ## Garage_Area Street Utilities Pool_Area ## Min. : 0.0 Grvl: 4 AllPub:617 Min. : 0.000 ## 1st Qu.: 320.5 Pave:614 NoSeWa: 0 1st Qu.: 0.000 ## Median : 482.0 NoSewr: 1 Median : 0.000 ## Mean : 473.4 Mean : 2.659 ## 3rd Qu.: 587.5 3rd Qu.: 0.000 ## Max. :1348.0 Max. :576.000 ## ## Neighborhood Screen_Porch Overall_Qual Lot_Area ## North_Ames : 96 Min. : 0.00 Average :173 Min. : 1491 ## College_Creek : 58 1st Qu.: 0.00 Above_Average:147 1st Qu.: 7572 ## Old_Town : 54 Median : 0.00 Good :121 Median : 9582 ## Sawyer : 39 Mean : 17.42 Very_Good : 72 Mean : 9830 ## Northridge_Heights: 38 3rd Qu.: 0.00 Below_Average: 61 3rd Qu.:11500 ## Edwards : 36 Max. :410.00 Excellent : 25 Max. :50102 ## (Other) :297 (Other) : 19 ## Lot_Frontage MS_SubClass Misc_Val ## Min. : 0.0 One_Story_1946_and_Newer_All_Styles :229 Min. : 0.00 ## 1st Qu.: 44.0 Two_Story_1946_and_Newer :121 1st Qu.: 0.00 ## Median : 64.0 One_and_Half_Story_Finished_All_Ages: 59 Median : 0.00 ## Mean : 58.4 One_Story_PUD_1946_and_Newer : 37 Mean : 43.73 ## 3rd Qu.: 79.0 Duplex_All_Styles_and_Ages : 33 3rd Qu.: 0.00 ## Max. :182.0 Two_Story_1945_and_Older : 27 Max. :8300.00 ## (Other) :112 ## Open_Porch_SF TotRms_AbvGrd First_Flr_SF Second_Flr_SF ## Min. : 0.00 Min. : 2.000 Min. : 334 Min. : 0.0 ## 1st Qu.: 0.00 1st Qu.: 5.000 1st Qu.: 875 1st Qu.: 0.0 ## Median : 24.50 Median : 6.000 Median :1072 Median : 0.0 ## Mean : 47.29 Mean : 6.422 Mean :1159 Mean : 320.1 ## 3rd Qu.: 69.00 3rd Qu.: 7.000 3rd Qu.:1395 3rd Qu.: 683.8 ## Max. :742.00 Max. :12.000 Max. :2674 Max. :2065.0 ## ## Year_Built ## Min. :1875 ## 1st Qu.:1954 ## Median :1972 ## Mean :1971 ## 3rd Qu.:2000 ## Max. :2009 ## 6.14 Imputing Missing Entries blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) # impute missing entries 6.15 Label Encoding Ordinal Categorical Variables Label encoding is a pure numeric conversion of the levels of a categorical variable. If a categorical variable is a factor and it has pre-specified levels then the numeric conversion will be in level order. If no levels are specified, the encoding will be based on alphabetical order. We should be careful with label encoding unordered categorical features because most models will treat them as ordered numeric features 6.16 Label Encoding Ordinal Categorical Variables # investigate predictors with possible ordering (label encoding) ames_train %&gt;% count(Overall_Qual) ## # A tibble: 10 × 2 ## Overall_Qual n ## &lt;fct&gt; &lt;int&gt; ## 1 Very_Poor 1 ## 2 Poor 2 ## 3 Fair 7 ## 4 Below_Average 61 ## 5 Average 173 ## 6 Above_Average 147 ## 7 Good 121 ## 8 Very_Good 72 ## 9 Excellent 25 ## 10 Very_Excellent 9 6.17 Label Encoding Ordinal Categorical Variables blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) # numeric conversion of levels of the predictors 6.18 Standardizing (centering and scaling) Numeric Predictors Standardizing features includes centering and scaling so that numeric variables have zero mean and unit variance, which provides a common comparable unit of measure across all the variables. Before centering and scaling, it is better to remove zv/nzv variables, and perform necessary imputation and label encoding. blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors step_scale(all_numeric(), -all_outcomes()) # scale (divide by standard deviation) all numeric predictors 6.19 Lumping Predictors Sometimes features (numerical or categorical) will contain levels that have very few observations (decided by a threshold). It can be beneficial to collapse, or “lump” these into a lesser number of categories. # lumping categorical predictors if need be ames_train %&gt;% count(Neighborhood) %&gt;% arrange(n) # check frequency of categories blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors step_scale(all_numeric(), -all_outcomes()) %&gt;% # scale (divide by standard deviation) all numeric predictors step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) # lumping required predictors 6.20 One-hot/dummy Encoding Categorical Predictors Figure 6.1: Adapted from Hands-on Machine Learning with R, Bradley Boehmke &amp; Brandon Greenwell 6.21 One-hot/dummy Encoding Categorical Predictors blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors step_scale(all_numeric(), -all_outcomes()) %&gt;% # scale (divide by standard deviation) all numeric predictors step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% # lumping required predictors step_dummy(all_nominal(), one_hot = TRUE) # one-hot/dummy encode nominal categorical predictors 6.22 Preprocessing Steps A suggested order of potential steps that should work for most problems: Filter out zero or near-zero variance features. Perform imputation if required. Label encode ordinal categorical features. Normalize/Standardize (center and scale) numeric features. Lump certain features if required. One-hot or dummy encode categorical features. 6.23 Preprocessing With recipes Package There are three main steps in creating and applying feature engineering with recipes: recipe: where you define your feature engineering steps to create your blueprint. prepare: estimate feature engineering parameters based on training data. bake: apply the blueprint to new data. 6.24 Preprocessing With recipes Package # finally, after all preprocessing steps have been decided set up the overall blueprint ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% step_impute_mean(Gr_Liv_Area) %&gt;% step_integer(Overall_Qual) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% step_scale(all_numeric(), -all_outcomes()) %&gt;% step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% step_dummy(all_nominal(), one_hot = TRUE) prepare &lt;- prep(blueprint, data = ames_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = ames_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = ames_test) # apply the blueprint to test data for future use 6.25 Training Model A KNN model # perform CV to tune K set.seed(013123) cv_specs &lt;- trainControl(method = &quot;cv&quot;, number = 5) # 5-fold CV (no repeats) k_grid &lt;- expand.grid(k = seq(1, 10, by = 2)) knn_fit &lt;- train(blueprint, data = ames_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;RMSE&quot;) knn_fit ## k-Nearest Neighbors ## ## 618 samples ## 19 predictor ## ## Recipe steps: nzv, impute_mean, integer, center, scale, other, dummy ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 494, 494, 495, 494, 495 ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 1 42573.14 0.7499158 27889.19 ## 3 35686.37 0.8233707 23448.95 ## 5 33973.09 0.8432532 22186.60 ## 7 34439.17 0.8446435 22479.38 ## 9 35202.64 0.8399091 22419.31 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 5. 6.26 Training Model A KNN model ggplot(knn_fit) 6.27 Training Model A linear regression model lm_fit &lt;- train(blueprint, data = ames_train, method = &quot;lm&quot;, trControl = cv_specs, metric = &quot;RMSE&quot;) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading lm_fit ## Linear Regression ## ## 618 samples ## 19 predictor ## ## Recipe steps: nzv, impute_mean, integer, center, scale, other, dummy ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 495, 494, 495, 494, 494 ## Resampling results: ## ## RMSE Rsquared MAE ## 32755.85 0.8502331 22609.88 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE 6.28 Final Model and Test Set Error # refit the final/optimal model using ALL modified training data, and obtain estimate of prediction error from modified test data final_model &lt;- lm(Sale_Price ~ ., data = baked_train) # build final model final_preds &lt;- predict(final_model, newdata = baked_test) # obtain predictions on test data ## Warning in predict.lm(final_model, newdata = baked_test): prediction from a ## rank-deficient fit may be misleading sqrt(mean((baked_test$Sale_Price - final_preds)^2)) # calculate test set RMSE ## [1] 47839.78 6.29 Variable Importance # variable importance library(vip) vip(object = lm_fit, # CV object num_features = 20, # maximum number of predictors to show importance for method = &quot;model&quot;) # model-specific VI scores 6.30 Your Turn!!! You will work with the attrition data from the modeldata package. The task is to predict Attrition (Yes/No) using the rest of the variables in the data (predictors/features). library(modeldata) # load package data(attrition) # load dataset Step 1: Investigate the dataset What are the types of features? - categorical or numeric If categorical, are they ordinal or nominal? If ordinal, are their levels in appropriate order? You can use the levels function to check the ordering. Are there any features with missing entries? Are there any zv/nzv features? Step 2: Split the data into training and test sets (70-30 split) Step 3: Perform required data preprocessing and create the blueprint. If using step_dummy(), set one_hot = FALSE. Step 4: Implement 5-fold CV (no repeats) to compare the performance of the following models. Use metric = \"Accuracy\". a logistic regression model (method = \"glm\" and family = \"binomial\") a KNN classifier with the optimal K chosen by CV (method = \"knn\"). Use a grid of K values \\(1, 2, \\ldots, 10\\). What is the optimal K chosen? How do the models compare in terms of the CV accuracies? Step 5: Build your final optimal model. Obtain probability and class label predictions for the test set (use threshold of 0.5). Create the corresponding confusion matrix and report the test set accuracy. Also, create the ROC curve for the optimal model and report the AUC. 6.31 Your Turn!!! Step 1 glimpse(attrition) # types of variables ## Rows: 1,470 ## Columns: 31 ## $ Age &lt;int&gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36, 35, 2… ## $ Attrition &lt;fct&gt; Yes, No, Yes, No, No, No, No, No, No, No, No,… ## $ BusinessTravel &lt;fct&gt; Travel_Rarely, Travel_Frequently, Travel_Rare… ## $ DailyRate &lt;int&gt; 1102, 279, 1373, 1392, 591, 1005, 1324, 1358,… ## $ Department &lt;fct&gt; Sales, Research_Development, Research_Develop… ## $ DistanceFromHome &lt;int&gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15, 26, … ## $ Education &lt;ord&gt; College, Below_College, College, Master, Belo… ## $ EducationField &lt;fct&gt; Life_Sciences, Life_Sciences, Other, Life_Sci… ## $ EnvironmentSatisfaction &lt;ord&gt; Medium, High, Very_High, Very_High, Low, Very… ## $ Gender &lt;fct&gt; Female, Male, Male, Female, Male, Male, Femal… ## $ HourlyRate &lt;int&gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94, 84, 4… ## $ JobInvolvement &lt;ord&gt; High, Medium, Medium, High, High, High, Very_… ## $ JobLevel &lt;int&gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, … ## $ JobRole &lt;fct&gt; Sales_Executive, Research_Scientist, Laborato… ## $ JobSatisfaction &lt;ord&gt; Very_High, Medium, High, High, Medium, Very_H… ## $ MaritalStatus &lt;fct&gt; Single, Married, Single, Married, Married, Si… ## $ MonthlyIncome &lt;int&gt; 5993, 5130, 2090, 2909, 3468, 3068, 2670, 269… ## $ MonthlyRate &lt;int&gt; 19479, 24907, 2396, 23159, 16632, 11864, 9964… ## $ NumCompaniesWorked &lt;int&gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, 0, 5, … ## $ OverTime &lt;fct&gt; Yes, No, Yes, Yes, No, No, Yes, No, No, No, N… ## $ PercentSalaryHike &lt;int&gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13, 13, 1… ## $ PerformanceRating &lt;ord&gt; Excellent, Outstanding, Excellent, Excellent,… ## $ RelationshipSatisfaction &lt;ord&gt; Low, Very_High, Medium, High, Very_High, High… ## $ StockOptionLevel &lt;int&gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, 1, 0, … ## $ TotalWorkingYears &lt;int&gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10, 5, 3… ## $ TrainingTimesLastYear &lt;int&gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, 2, 4, … ## $ WorkLifeBalance &lt;ord&gt; Bad, Better, Better, Better, Better, Good, Go… ## $ YearsAtCompany &lt;int&gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5, 2, 4,… ## $ YearsInCurrentRole &lt;int&gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, 2, 2, … ## $ YearsSinceLastPromotion &lt;int&gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, 1, 0, … ## $ YearsWithCurrManager &lt;int&gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, 2, 3, … Numerical variables are represented as &lt;int&gt;, categorical variables are represented as either &lt;ord&gt; or &lt;fct&gt;. 6.32 Your Turn!!! Step 1 # checking the levels of ordinal variables levels(attrition$BusinessTravel) ## [1] &quot;Non-Travel&quot; &quot;Travel_Frequently&quot; &quot;Travel_Rarely&quot; levels(attrition$Education) ## [1] &quot;Below_College&quot; &quot;College&quot; &quot;Bachelor&quot; &quot;Master&quot; ## [5] &quot;Doctor&quot; levels(attrition$EnvironmentSatisfaction) ## [1] &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; &quot;Very_High&quot; levels(attrition$JobInvolvement) ## [1] &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; &quot;Very_High&quot; levels(attrition$JobSatisfaction) ## [1] &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; &quot;Very_High&quot; levels(attrition$PerformanceRating) ## [1] &quot;Low&quot; &quot;Good&quot; &quot;Excellent&quot; &quot;Outstanding&quot; levels(attrition$RelationshipSatisfaction) ## [1] &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; &quot;Very_High&quot; levels(attrition$WorkLifeBalance) ## [1] &quot;Bad&quot; &quot;Good&quot; &quot;Better&quot; &quot;Best&quot; 6.33 Your Turn!!! Step 1 # reorder levels of &#39;BusinessTravel&#39; attrition$BusinessTravel &lt;- factor(attrition$BusinessTravel, levels = c(&quot;Non-Travel&quot;, &quot;Travel_Rarely&quot;, &quot;Travel_Frequently&quot;)) levels(attrition$BusinessTravel) ## [1] &quot;Non-Travel&quot; &quot;Travel_Rarely&quot; &quot;Travel_Frequently&quot; 6.34 Your Turn!!! Step 1 sum(is.na(attrition)) # check for missing entries ## [1] 0 6.35 Your Turn!!! Step 1 nearZeroVar(attrition, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## Age 1.012987 2.9251701 FALSE FALSE ## Attrition 5.202532 0.1360544 FALSE FALSE ## BusinessTravel 3.765343 0.2040816 FALSE FALSE ## DailyRate 1.200000 60.2721088 FALSE FALSE ## Department 2.154709 0.2040816 FALSE FALSE ## DistanceFromHome 1.014423 1.9727891 FALSE FALSE ## Education 1.437186 0.3401361 FALSE FALSE ## EducationField 1.306034 0.4081633 FALSE FALSE ## EnvironmentSatisfaction 1.015695 0.2721088 FALSE FALSE ## Gender 1.500000 0.1360544 FALSE FALSE ## HourlyRate 1.035714 4.8299320 FALSE FALSE ## JobInvolvement 2.314667 0.2721088 FALSE FALSE ## JobLevel 1.016854 0.3401361 FALSE FALSE ## JobRole 1.116438 0.6122449 FALSE FALSE ## JobSatisfaction 1.038462 0.2721088 FALSE FALSE ## MaritalStatus 1.431915 0.2040816 FALSE FALSE ## MonthlyIncome 1.333333 91.7687075 FALSE FALSE ## MonthlyRate 1.000000 97.0748299 FALSE FALSE ## NumCompaniesWorked 2.644670 0.6802721 FALSE FALSE ## OverTime 2.533654 0.1360544 FALSE FALSE ## PercentSalaryHike 1.004785 1.0204082 FALSE FALSE ## PerformanceRating 5.504425 0.1360544 FALSE FALSE ## RelationshipSatisfaction 1.062500 0.2721088 FALSE FALSE ## StockOptionLevel 1.058725 0.2721088 FALSE FALSE ## TotalWorkingYears 1.616000 2.7210884 FALSE FALSE ## TrainingTimesLastYear 1.114053 0.4761905 FALSE FALSE ## WorkLifeBalance 2.595930 0.2721088 FALSE FALSE ## YearsAtCompany 1.146199 2.5170068 FALSE FALSE ## YearsInCurrentRole 1.524590 1.2925170 FALSE FALSE ## YearsSinceLastPromotion 1.627451 1.0884354 FALSE FALSE ## YearsWithCurrManager 1.307985 1.2244898 FALSE FALSE 6.36 Your Turn!!! Step 2 # split data set.seed(013123) train_index &lt;- createDataPartition(attrition$Attrition, p = 0.7, list = FALSE) attrition_train &lt;- attrition[train_index, ] attrition_test &lt;- attrition[-train_index, ] 6.37 Your Turn!!! Step 3 # create recipe, blueprint, prepare, and bake attrition_recipe &lt;- recipe(formula = Attrition ~ ., data = attrition_train) # sets up the type and role of variables blueprint &lt;- attrition_recipe %&gt;% step_integer(BusinessTravel, Education, EnvironmentSatisfaction, JobInvolvement, JobSatisfaction, PerformanceRating, RelationshipSatisfaction, WorkLifeBalance) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) prepare &lt;- prep(blueprint, data = attrition_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = attrition_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = attrition_test) # apply the blueprint to test data for future use 6.38 Your Turn!!! Step 4 # perform CV set.seed(013123) cv_specs &lt;- trainControl(method = &quot;cv&quot;, number = 5) # 5-fold CV (no repeats) # CV with logistic regression logistic_fit &lt;- train(blueprint, data = attrition_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = cv_specs, metric = &quot;Accuracy&quot;) logistic_fit ## Generalized Linear Model ## ## 1030 samples ## 30 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Recipe steps: integer, center, scale, dummy ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 824, 823, 825, 824, 824 ## Resampling results: ## ## Accuracy Kappa ## 0.870915 0.4528554 6.39 Your Turn!!! Step 4 # CV with KNN set.seed(013123) k_grid &lt;- expand.grid(k = seq(1, 10, by = 1)) knn_fit &lt;- train(blueprint, data = attrition_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;Accuracy&quot;) knn_fit ## k-Nearest Neighbors ## ## 1030 samples ## 30 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Recipe steps: integer, center, scale, dummy ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 824, 823, 825, 824, 824 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.7835261 0.13378621 ## 2 0.7873814 0.14344232 ## 3 0.8398231 0.20441187 ## 4 0.8310899 0.17242779 ## 5 0.8504841 0.17107000 ## 6 0.8446258 0.15021424 ## 7 0.8465958 0.13835761 ## 8 0.8485517 0.13591269 ## 9 0.8427169 0.07579273 ## 10 0.8456343 0.08956634 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 5. 6.40 Your Turn!!! Step 4 ggplot(knn_fit) 6.41 Your Turn!!! Step 5 # build final optimal model and obtain predictions on test set final_model &lt;- glm(Attrition ~ ., data = baked_train, family = binomial) # build final model final_model_prob_preds &lt;- predict(object = final_model, newdata = baked_test, type = &quot;response&quot;) # obtain probability predictions on test data threshold &lt;- 0.5 final_model_class_preds &lt;- factor(ifelse(final_model_prob_preds &gt; threshold, &quot;Yes&quot;, &quot;No&quot;)) 6.42 Your Turn!!! Step 5 # create confusion matrix confusionMatrix(data = relevel(final_model_class_preds, ref = &quot;Yes&quot;), reference = relevel(baked_test$Attrition, ref = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 28 14 ## No 43 355 ## ## Accuracy : 0.8705 ## 95% CI : (0.8354, 0.9004) ## No Information Rate : 0.8386 ## P-Value [Acc &gt; NIR] : 0.0372769 ## ## Kappa : 0.4268 ## ## Mcnemar&#39;s Test P-Value : 0.0002083 ## ## Sensitivity : 0.39437 ## Specificity : 0.96206 ## Pos Pred Value : 0.66667 ## Neg Pred Value : 0.89196 ## Prevalence : 0.16136 ## Detection Rate : 0.06364 ## Detection Prevalence : 0.09545 ## Balanced Accuracy : 0.67821 ## ## &#39;Positive&#39; Class : Yes ## 6.43 Your Turn!!! Step 5 # create ROC cuvre and compute AUC roc_object &lt;- roc(response = baked_test$Attrition, predictor = final_model_prob_preds) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases plot(roc_object, col = &quot;red&quot;) auc(roc_object) ## Area under the curve: 0.8361 "],["linear-model-selection-and-regularization.html", "Chapter 7 Linear Model Selection and Regularization 7.1 Alternatives to Least Squares 7.2 Shrinkage/Regularization Methods 7.3 The Lasso 7.4 The Lasso 7.5 The Lasso: Scaling of Predictors 7.6 The Lasso: Implementation 7.7 The Lasso: Implementation 7.8 The Lasso: Implementation 7.9 The Lasso: Implementation 7.10 The Lasso: Implementation 7.11 The Lasso: Implementation 7.12 The Lasso: Implementation 7.13 Your Turn!!! 7.14 Your Turn!!! 7.15 Your Turn!!! 7.16 Your Turn!!! 7.17 Your Turn!!! 7.18 Your Turn!!! 7.19 Your Turn!!! 7.20 Your Turn!!! 7.21 Your Turn!!! 7.22 Your Turn!!! 7.23 Your Turn!!! 7.24 Your Turn!!! 7.25 Multivariate Adaptive Regression Splines (MARS) 7.26 MARS: Geometry 7.27 MARS: Implementation 7.28 MARS: Implementation 7.29 MARS: Implementation 7.30 MARS: Implementation 7.31 MARS: Implementation 7.32 MARS 7.33 MARS Without Feature Engineering: Implementation 7.34 MARS Without Feature Engineering: Implementation 7.35 MARS Without Feature Engineering: Implementation 7.36 MARS Without Feature Engineering: Implementation 7.37 MARS Without Feature Engineering: Implementation 7.38 MARS Without Feature Engineering: Implementation 7.39 Your Turn!!! 7.40 Your Turn!!! 7.41 Your Turn!!! 7.42 Your Turn!!! 7.43 Your Turn!!! 7.44 Your Turn!!! 7.45 Your Turn!!! 7.46 Your Turn!!! 7.47 Your Turn!!! 7.48 Your Turn!!! 7.49 Your Turn!!!", " Chapter 7 Linear Model Selection and Regularization The standard linear model, Benefits: Simple, interpretable, often shows good predictive performance. Limitations: Prediction Accuracy: If \\(n\\) is not big (also when \\(p&gt;n\\)), variance can be high resulting in overfitting and poor predictive performance. Model Interpretability: Irrelevant features lead to unnecessary model complexity. 7.1 Alternatives to Least Squares Extensions/Modifications/Improvements to Least Squares: Subset Selection: Identify a subset of \\(p\\) predictors and then fit a linear model using least squares. Shrinkage/Regularization: Fit a model using \\(p\\) predictors, but shrink some of the estimated coefficients towards zero. Reduces variance and can also perform variable selection. Dimension Reduction: Project \\(p\\) predictors onto a \\(M\\)-dimensional subspace, where \\(M&lt;p\\). Achieved by computing \\(M\\) different linear combinations or projections of the \\(p\\) predictors. Fit a linear model using these \\(M\\) predictors by least squares. 7.2 Shrinkage/Regularization Methods Fit a model containing all \\(p\\) predictors using a technique that shrinks the coefficient estimates towards zero. Ridge Regression Lasso Shrinking the coefficient estimates significantly reduces their variance. 7.3 The Lasso Acronym for Least Absolute Shrinkage and Selection Operator. Standard Linear Model Given a training dataset, for \\(i=1,\\ldots,n\\) \\[y_i=\\beta_0+\\beta_1 x_{i1}+ \\ldots + \\beta_p x_{ip} + \\epsilon_i = \\beta_0 + \\displaystyle \\sum_{j=1}^p \\beta_j x_{ij} + \\epsilon_i\\] Lasso \\(\\lambda \\displaystyle \\sum_{j=1}^{p} |\\beta_j|\\): Shrinkage Penalty \\(\\lambda \\ge 0\\): Tuning/Regularization Parameter 7.4 The Lasso This method not only shrinks the coefficient estimates towards zero, but also makes some of the coefficient estimates exactly equal to zero (when the tuning parameter \\(\\lambda\\) is sufficiently large). Hence, the lasso performs variable selection. We say that the lasso yields sparse models, that is, models that involve only a subset of the variables. 7.5 The Lasso: Scaling of Predictors Standard least squares (regression) coefficient estimates are scale equivariant: multiplying \\(X_j\\) by a constant \\(c\\) simply leads to a scaling of the least squares coefficient estimates by a factor of \\(1/c\\). In other words, regardless of how the \\(j^{th}\\) predictor is scaled, \\(X_j \\hat{\\beta}_j\\) will remain the same. In contrast, the lasso coefficient estimates can change substantially when multiplying a given predictor by a constant. Apply lasso after standardizing the predictors. 7.6 The Lasso: Implementation Ames Housing Dataset ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # load dataset ames$Overall_Qual &lt;- factor(ames$Overall_Qual, levels = c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;)) # split data set.seed(021423) # set seed index &lt;- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE) # consider 70-30 split ames_train &lt;- ames[index,] # training data ames_test &lt;- ames[-index,] # test data 7.7 The Lasso: Implementation Ames Housing Dataset # create recipe and blueprint, prepare and apply blueprint set.seed(021423) # set seed ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) # set up recipe blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors step_scale(all_numeric(), -all_outcomes()) %&gt;% # scale (divide by standard deviation) all numeric predictors step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% # lumping required predictors step_dummy(all_nominal(), one_hot = TRUE) # one-hot/dummy encode nominal categorical predictors prepare &lt;- prep(blueprint, data = ames_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = ames_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = ames_test) # apply the blueprint to test data for future use 7.8 The Lasso: Implementation Ames Housing Dataset Implement CV to tune the hyperparameter \\(\\lambda\\). set.seed(021423) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications lambda_grid &lt;- 10^seq(-3, 3, length = 100) # grid of lambda values to search over library(glmnet) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Loaded glmnet 4.1-6 lasso_cv &lt;- train(blueprint, data = ames_train, method = &quot;glmnet&quot;, # for lasso trControl = cv_specs, tuneGrid = expand.grid(alpha = 1, lambda = lambda_grid), # alpha = 1 implements lasso metric = &quot;RMSE&quot;) # results from the CV procedure lasso_cv$bestTune # optimal lambda ## alpha lambda ## 100 1 1000 min(lasso_cv$results$RMSE) # RMSE for optimal lambda ## [1] 42263.95 7.9 The Lasso: Implementation Ames Housing Dataset Results from the CV procedure. ggplot(lasso_cv) # lambda vs. RMSE plot 7.10 The Lasso: Implementation Ames Housing Dataset We will now build the optimal lasso model on the modified training data using the optimal \\(\\lambda\\). # create datasets required for &#39;glmnet&#39; function X_train &lt;- model.matrix(Sale_Price ~ ., data = baked_train)[, -1] # training features without intercept Y_train &lt;- baked_train$Sale_Price # training response X_test &lt;- model.matrix(Sale_Price ~ ., data = baked_test)[, -1] # test features without intercept # build optimal lasso model final_model &lt;- glmnet(x = X_train, y = Y_train, alpha = 1, # alpha = 1 builds lasso model lambda = lasso_cv$bestTune$lambda, # using optimal lambda from CV standardize = FALSE) # we have already standardized during data preprocessing # obtain predictions and test set RMSE final_model_preds &lt;- predict(final_model, newx = X_test) # obtain predictions sqrt(mean((final_model_preds - baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 30840.5 7.11 The Lasso: Implementation Ames Housing Dataset The coefficients for the optimal lasso model can be obtained from coef(final_model) # estimated coefficients from final lasso model ## 58 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) 176064.4047 ## Gr_Liv_Area 4818.3193 ## Garage_Cars 10866.6279 ## Garage_Area . ## Overall_Qual 34950.7772 ## Lot_Area 2450.1548 ## Lot_Frontage 196.4699 ## Open_Porch_SF 416.8881 ## TotRms_AbvGrd -2617.0296 ## First_Flr_SF 25444.3738 ## Second_Flr_SF 17393.1449 ## Year_Built 7404.4553 ## Garage_Type_Attchd . ## Garage_Type_Basment . ## Garage_Type_BuiltIn . ## Garage_Type_CarPort . ## Garage_Type_Detchd . ## Garage_Type_More_Than_Two_Types . ## Garage_Type_No_Garage . ## Neighborhood_North_Ames . ## Neighborhood_College_Creek . ## Neighborhood_Old_Town . ## Neighborhood_Edwards . ## Neighborhood_Somerset . ## Neighborhood_Northridge_Heights 19267.6111 ## Neighborhood_Gilbert . ## Neighborhood_Sawyer . ## Neighborhood_Northwest_Ames . ## Neighborhood_Sawyer_West . ## Neighborhood_Mitchell . ## Neighborhood_Brookside . ## Neighborhood_Crawford . ## Neighborhood_Iowa_DOT_and_Rail_Road . ## Neighborhood_Timberland . ## Neighborhood_Northridge 13301.0524 ## Neighborhood_Stone_Brook . ## Neighborhood_South_and_West_of_Iowa_State_University . ## Neighborhood_Clear_Creek . ## Neighborhood_Meadow_Village . ## Neighborhood_Bloomington_Heights . ## Neighborhood_Veenker . ## Neighborhood_other . ## MS_SubClass_One_Story_1946_and_Newer_All_Styles 12281.5841 ## MS_SubClass_One_Story_1945_and_Older . ## MS_SubClass_One_Story_with_Finished_Attic_All_Ages . ## MS_SubClass_One_and_Half_Story_Unfinished_All_Ages . ## MS_SubClass_One_and_Half_Story_Finished_All_Ages . ## MS_SubClass_Two_Story_1946_and_Newer . ## MS_SubClass_Two_Story_1945_and_Older . ## MS_SubClass_Two_and_Half_Story_All_Ages . ## MS_SubClass_Split_or_Multilevel . ## MS_SubClass_Split_Foyer . ## MS_SubClass_Duplex_All_Styles_and_Ages . ## MS_SubClass_One_Story_PUD_1946_and_Newer . ## MS_SubClass_One_and_Half_Story_PUD_All_Ages . ## MS_SubClass_Two_Story_PUD_1946_and_Newer . ## MS_SubClass_PUD_Multilevel_Split_Level_Foyer . ## MS_SubClass_Two_Family_conversion_All_Styles_and_Ages . 7.12 The Lasso: Implementation Ames Housing Dataset # variable importance vip(object = lasso_cv, num_features = 20, method = &quot;model&quot;) 7.13 Your Turn!!! You will work with the Hitters.rds dataset. Please download the dataset from Canvas, upload it to Posit Cloud, and load it using the following code. Hitters &lt;- readRDS(&quot;Hitters.rds&quot;) # load dataset The dataset contains baseball statistics from the 1986 and 1987 seasons. The task is to predict Salary using the rest of the variables in the dataset. Compare the performance (in terms of RMSE) of the following two models: A linear regression model; A LASSO model chosen by CV. Consider the grid of possible \\(\\lambda\\) values as lambda_grid &lt;- 10^seq(-2, 2, length = 100). Perform the following tasks. Investigate the dataset and complete any necessary tasks. Split the data into training and test sets (80-20). Perform required data preprocessing and create the blueprint. If using step_dummy(), set one_hot = FALSE. Prepare the blueprint on the training data. Obtain the modified training and test datasets. Implement 5-fold CV repeated 5 times for each of the models above. Report the optimal CV RMSE of each model. Report the optimal value of \\(\\lambda\\) for the LASSO model. Which model performs better in this situation? Using the optimal model, obtain predictions on the test set. Calculate and report the test set RMSE. Using the optimal model, obtain variable importance measures for the features. 7.14 Your Turn!!! glimpse(Hitters) ## Rows: 263 ## Columns: 20 ## $ AtBat &lt;int&gt; 315, 479, 496, 321, 594, 185, 298, 323, 401, 574, 202, 418, … ## $ Hits &lt;int&gt; 81, 130, 141, 87, 169, 37, 73, 81, 92, 159, 53, 113, 60, 43,… ## $ HmRun &lt;int&gt; 7, 18, 20, 10, 4, 1, 0, 6, 17, 21, 4, 13, 0, 7, 20, 2, 8, 16… ## $ Runs &lt;int&gt; 24, 66, 65, 39, 74, 23, 24, 26, 49, 107, 31, 48, 30, 29, 89,… ## $ RBI &lt;int&gt; 38, 72, 78, 42, 51, 8, 24, 32, 66, 75, 26, 61, 11, 27, 75, 8… ## $ Walks &lt;int&gt; 39, 76, 37, 30, 35, 21, 7, 8, 65, 59, 27, 47, 22, 30, 73, 15… ## $ Years &lt;int&gt; 14, 3, 11, 2, 11, 2, 3, 2, 13, 10, 9, 4, 6, 13, 15, 5, 8, 1,… ## $ CAtBat &lt;int&gt; 3449, 1624, 5628, 396, 4408, 214, 509, 341, 5206, 4631, 1876… ## $ CHits &lt;int&gt; 835, 457, 1575, 101, 1133, 42, 108, 86, 1332, 1300, 467, 392… ## $ CHmRun &lt;int&gt; 69, 63, 225, 12, 19, 1, 0, 6, 253, 90, 15, 41, 4, 36, 177, 5… ## $ CRuns &lt;int&gt; 321, 224, 828, 48, 501, 30, 41, 32, 784, 702, 192, 205, 309,… ## $ CRBI &lt;int&gt; 414, 266, 838, 46, 336, 9, 37, 34, 890, 504, 186, 204, 103, … ## $ CWalks &lt;int&gt; 375, 263, 354, 33, 194, 24, 12, 8, 866, 488, 161, 203, 207, … ## $ League &lt;fct&gt; N, A, N, N, A, N, A, N, A, A, N, N, A, N, N, A, N, N, A, N, … ## $ Division &lt;fct&gt; W, W, E, E, W, E, W, W, E, E, W, E, E, E, W, W, W, E, W, W, … ## $ PutOuts &lt;int&gt; 632, 880, 200, 805, 282, 76, 121, 143, 0, 238, 304, 211, 121… ## $ Assists &lt;int&gt; 43, 82, 11, 40, 421, 127, 283, 290, 0, 445, 45, 11, 151, 45,… ## $ Errors &lt;int&gt; 10, 14, 3, 4, 25, 7, 9, 19, 0, 22, 11, 7, 6, 8, 10, 16, 2, 5… ## $ Salary &lt;dbl&gt; 475.000, 480.000, 500.000, 91.500, 750.000, 70.000, 100.000,… ## $ NewLeague &lt;fct&gt; N, A, N, N, A, A, A, N, A, A, N, N, A, N, N, A, N, N, N, N, … # categorical feature: League, Division, NewLeague (all nominal) # Rest of the features are numerical 7.15 Your Turn!!! sum(is.na(Hitters)) ## [1] 0 # no missing entries 7.16 Your Turn!!! summary(Hitters) ## AtBat Hits HmRun Runs ## Min. : 19.0 Min. : 1.0 Min. : 0.00 Min. : 0.00 ## 1st Qu.:282.5 1st Qu.: 71.5 1st Qu.: 5.00 1st Qu.: 33.50 ## Median :413.0 Median :103.0 Median : 9.00 Median : 52.00 ## Mean :403.6 Mean :107.8 Mean :11.62 Mean : 54.75 ## 3rd Qu.:526.0 3rd Qu.:141.5 3rd Qu.:18.00 3rd Qu.: 73.00 ## Max. :687.0 Max. :238.0 Max. :40.00 Max. :130.00 ## RBI Walks Years CAtBat ## Min. : 0.00 Min. : 0.00 Min. : 1.000 Min. : 19.0 ## 1st Qu.: 30.00 1st Qu.: 23.00 1st Qu.: 4.000 1st Qu.: 842.5 ## Median : 47.00 Median : 37.00 Median : 6.000 Median : 1931.0 ## Mean : 51.49 Mean : 41.11 Mean : 7.312 Mean : 2657.5 ## 3rd Qu.: 71.00 3rd Qu.: 57.00 3rd Qu.:10.000 3rd Qu.: 3890.5 ## Max. :121.00 Max. :105.00 Max. :24.000 Max. :14053.0 ## CHits CHmRun CRuns CRBI ## Min. : 4.0 Min. : 0.00 Min. : 2.0 Min. : 3.0 ## 1st Qu.: 212.0 1st Qu.: 15.00 1st Qu.: 105.5 1st Qu.: 95.0 ## Median : 516.0 Median : 40.00 Median : 250.0 Median : 230.0 ## Mean : 722.2 Mean : 69.24 Mean : 361.2 Mean : 330.4 ## 3rd Qu.:1054.0 3rd Qu.: 92.50 3rd Qu.: 497.5 3rd Qu.: 424.5 ## Max. :4256.0 Max. :548.00 Max. :2165.0 Max. :1659.0 ## CWalks League Division PutOuts Assists ## Min. : 1.0 A:139 E:129 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 71.0 N:124 W:134 1st Qu.: 113.5 1st Qu.: 8.0 ## Median : 174.0 Median : 224.0 Median : 45.0 ## Mean : 260.3 Mean : 290.7 Mean :118.8 ## 3rd Qu.: 328.5 3rd Qu.: 322.5 3rd Qu.:192.0 ## Max. :1566.0 Max. :1377.0 Max. :492.0 ## Errors Salary NewLeague ## Min. : 0.000 Min. : 67.5 A:141 ## 1st Qu.: 3.000 1st Qu.: 190.0 N:122 ## Median : 7.000 Median : 425.0 ## Mean : 8.593 Mean : 535.9 ## 3rd Qu.:13.000 3rd Qu.: 750.0 ## Max. :32.000 Max. :2460.0 7.17 Your Turn!!! nearZeroVar(Hitters, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## AtBat 1.000000 79.4676806 FALSE FALSE ## Hits 1.200000 49.4296578 FALSE FALSE ## HmRun 1.058824 13.3079848 FALSE FALSE ## Runs 1.428571 34.9809886 FALSE FALSE ## RBI 1.125000 35.7414449 FALSE FALSE ## Walks 1.222222 33.0798479 FALSE FALSE ## Years 1.000000 7.9847909 FALSE FALSE ## CAtBat 1.000000 97.7186312 FALSE FALSE ## CHits 1.500000 91.6349810 FALSE FALSE ## CHmRun 1.000000 49.0494297 FALSE FALSE ## CRuns 1.000000 85.9315589 FALSE FALSE ## CRBI 1.333333 85.9315589 FALSE FALSE ## CWalks 1.000000 78.7072243 FALSE FALSE ## League 1.120968 0.7604563 FALSE FALSE ## Division 1.038760 0.7604563 FALSE FALSE ## PutOuts 2.750000 75.6653992 FALSE FALSE ## Assists 1.000000 55.1330798 FALSE FALSE ## Errors 1.500000 11.0266160 FALSE FALSE ## Salary 1.333333 57.0342205 FALSE FALSE ## NewLeague 1.155738 0.7604563 FALSE FALSE # no zv/nzv features in the original data 7.18 Your Turn!!! # split data set.seed(021423) index &lt;- createDataPartition(Hitters$Salary, p = 0.8, list = FALSE) Hitters_train &lt;- Hitters[index,] # training set Hitters_test &lt;- Hitters[-index,] # test set 7.19 Your Turn!!! nearZeroVar(Hitters_train, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## AtBat 1.000000 82.5471698 FALSE FALSE ## Hits 1.250000 57.0754717 FALSE FALSE ## HmRun 1.000000 16.0377358 FALSE FALSE ## Runs 1.142857 39.1509434 FALSE FALSE ## RBI 1.142857 40.0943396 FALSE FALSE ## Walks 1.000000 37.7358491 FALSE FALSE ## Years 1.136364 9.4339623 FALSE FALSE ## CAtBat 1.000000 97.6415094 FALSE FALSE ## CHits 1.500000 91.9811321 FALSE FALSE ## CHmRun 1.000000 54.2452830 FALSE FALSE ## CRuns 1.000000 90.0943396 FALSE FALSE ## CRBI 1.333333 88.2075472 FALSE FALSE ## CWalks 1.000000 78.3018868 FALSE FALSE ## League 1.163265 0.9433962 FALSE FALSE ## Division 1.078431 0.9433962 FALSE FALSE ## PutOuts 2.000000 80.6603774 FALSE FALSE ## Assists 1.090909 59.9056604 FALSE FALSE ## Errors 1.388889 13.2075472 FALSE FALSE ## Salary 1.166667 60.3773585 FALSE FALSE ## NewLeague 1.231579 0.9433962 FALSE FALSE # no zv/nzv features in the training data 7.20 Your Turn!!! set.seed(021423) Hitters_recipe &lt;- recipe(Salary ~. , data = Hitters_train) # create recipe set.seed(021423) # create blueprint with feature engineering steps blueprint &lt;- Hitters_recipe %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% # center numerical features step_scale(all_numeric(), -all_outcomes()) %&gt;% # scale numerical features step_dummy(all_nominal(), one_hot = FALSE) # dummy encode nominal features set.seed(021423) prepare &lt;- prep(blueprint, data = Hitters_train) # estimate feature engineering parameters based on training data set.seed(021423) baked_train &lt;- bake(prepare, new_data = Hitters_train) # apply the blueprint to training data baked_test &lt;- bake(prepare, new_data = Hitters_test) # apply the blueprint to test data 7.21 Your Turn!!! set.seed(021423) cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications set.seed(021423) # CV with linear regression lm_cv &lt;- train(blueprint, data = Hitters_train, method = &quot;lm&quot;, trControl = cv_specs, metric = &quot;RMSE&quot;) set.seed(021423) # CV with LASSO lambda_grid &lt;- 10^seq(-2, 2, length = 100) # grid of lambda values lasso_cv &lt;- train(blueprint, data = Hitters_train, method = &quot;glmnet&quot;, trControl = cv_specs, tuneGrid = expand.grid(alpha = 1, lambda = lambda_grid), metric = &quot;RMSE&quot;) 7.22 Your Turn!!! lm_cv$results$RMSE # CV RMSE for linear regression model ## [1] 301.3199 min(lasso_cv$results$RMSE) # CV RMSE for optimal LASSO ## [1] 295.07 lasso_cv$bestTune$lambda # optimal lambda ## [1] 15.55676 7.23 Your Turn!!! X_train &lt;- model.matrix(Salary ~ ., data = baked_train)[, -1] # training features without intercept Y_train &lt;- baked_train$Salary # training response X_test &lt;- model.matrix(Salary ~ ., data = baked_test)[, -1] # test features without intercept # build optimal lasso model final_model &lt;- glmnet(x = X_train, y = Y_train, alpha = 1, # alpha = 1 builds lasso model lambda = lasso_cv$bestTune$lambda, # using optimal lambda from CV standardize = FALSE) # we have already standardized during data preprocessing final_model_preds &lt;- predict(final_model, newx = X_test) # obtain predictions sqrt(mean((final_model_preds - baked_test$Salary)^2)) # calculate test set RMSE ## [1] 510.4167 7.24 Your Turn!!! vip(object = lasso_cv, num_features = 20, method = &quot;model&quot;) 7.25 Multivariate Adaptive Regression Splines (MARS) Multivariate Adaptive Regression Splines (MARS) capture the nonlinear relationships in the data by assessing cutpoints (knots). The procedure assesses each data point for each predictor as a knot and creates a piecewise linear regression model with the candidate feature(s). This procedure continues until many knots are found, producing a (potentially) highly non-linear prediction equation. Once the full set of knots has been identified, knots that do not contribute significantly to predictive accuracy can be sequentially removed. This process is known as pruning . There are two tuning parameters associated with our MARS model: the maximum degree of interactions, and the number of terms retained in the final model after pruning. We can use CV to identify the optimal combination of these tuning parameters . 7.26 MARS: Geometry Figure 7.1: Adapted from Hands-on Machine Learning with R, Bradley Boehmke &amp; Brandon Greenwell 7.27 MARS: Implementation Ames Housing Dataset # data splitting and preprocessing already done in the previous slides set.seed(021423) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications library(earth) # for MARS technique ## Loading required package: Formula ## Loading required package: plotmo ## Loading required package: plotrix ## Loading required package: TeachingDemos set.seed(021423) # set seed param_grid &lt;- expand.grid(degree = 1:3, nprune = seq(1, 100, length.out = 10)) # grid of tuning parameters mars_cv &lt;- train(blueprint, data = ames_train, method = &quot;earth&quot;, trControl = cv_specs, tuneGrid = param_grid, metric = &quot;RMSE&quot;) ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 493 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 495 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 494 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in cuts[nterm, ipred] == 0 &amp;&amp; !is.null(xrange) &amp;&amp; xrange[1, ipred] == : ## &#39;length(x) = 496 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in train_rec(rec = x, dat = data, info = trainInfo, method = models, : ## There were missing values in resampled performance measures. 7.28 MARS: Implementation Ames Housing Dataset mars_cv$bestTune # optimal tuning parameters ## nprune degree ## 2 12 1 min(mars_cv$results$RMSE) # optimal CV RMSE ## [1] 43190.54 7.29 MARS: Implementation Ames Housing Dataset # fit final optimal model final_model &lt;- earth(Sale_Price ~ ., data = baked_train, degree = mars_cv$bestTune$degree, nprune = mars_cv$bestTune$nprune) final_model_preds &lt;- predict(final_model, newdata = baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((final_model_preds - baked_test$Sale_Price)^2)) # test RMSE ## [1] 29057.4 7.30 MARS: Implementation Ames Housing Dataset summary(final_model) ## Call: earth(formula=Sale_Price~., data=baked_train, ## degree=mars_cv$bestTune$degree, nprune=mars_cv$bestTune$nprune) ## ## coefficients ## (Intercept) 185460.891 ## MS_SubClass_Duplex_All_Styles_and_Ages -34415.007 ## h(0.628157-Overall_Qual) -16973.775 ## h(Overall_Qual-0.628157) 86687.352 ## h(-0.56119-Lot_Area) -46313.234 ## h(Lot_Area- -0.56119) 5775.321 ## h(First_Flr_SF-1.81938) 196633.878 ## h(2.22339-First_Flr_SF) -22222.077 ## h(First_Flr_SF-2.22339) -254800.210 ## h(Second_Flr_SF-0.502964) 38210.357 ## h(Second_Flr_SF-2.25174) 72488.093 ## h(Year_Built- -1.83161) 18090.264 ## ## Selected 12 of 27 terms, and 6 of 57 predictors (nprune=12) ## Termination condition: RSq changed by less than 0.001 at 27 terms ## Importance: Overall_Qual, First_Flr_SF, Second_Flr_SF, Year_Built, ... ## Number of terms at each degree of interaction: 1 11 (additive model) ## GCV 881846643 RSS 505171129102 GRSq 0.8817599 RSq 0.8900416 7.31 MARS: Implementation Ames Housing Dataset vip(object = mars_cv, num_features = 20, method = &quot;model&quot;) 7.32 MARS Advantages MARS can handle mixed types of features (quantitative and qualitative). MARS also requires minimal feature engineering (missing values need to be imputed) and performs automated feature selection. Disadvantages MARS models are typically slower to train. 7.33 MARS Without Feature Engineering: Implementation Ames Housing Dataset # create recipe and blueprint, prepare and apply blueprint set.seed(021423) # set seed ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) # set up recipe blueprint_new &lt;- ames_recipe %&gt;% step_impute_mean(Gr_Liv_Area) # impute missing entries prepare &lt;- prep(blueprint_new, data = ames_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = ames_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = ames_test) # apply the blueprint to test data for future use 7.34 MARS Without Feature Engineering: Implementation Ames Housing Dataset # data splitting and preprocessing already done in the previous slides set.seed(021423) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications library(earth) # for MARS technique set.seed(021423) # set seed param_grid &lt;- expand.grid(degree = 1:3, nprune = seq(1, 100, length.out = 10)) # grid of tuning parameters mars_cv_new &lt;- train(blueprint_new, data = ames_train, method = &quot;earth&quot;, trControl = cv_specs, tuneGrid = param_grid, metric = &quot;RMSE&quot;) ## Warning in train_rec(rec = x, dat = data, info = trainInfo, method = models, : ## There were missing values in resampled performance measures. 7.35 MARS Without Feature Engineering: Implementation Ames Housing Dataset mars_cv_new$bestTune # optimal tuning parameters ## nprune degree ## 2 12 1 min(mars_cv_new$results$RMSE) # optimal CV RMSE ## [1] 56774.49 7.36 MARS Without Feature Engineering: Implementation Ames Housing Dataset # fit final optimal model final_model_new &lt;- earth(Sale_Price ~ ., data = baked_train, degree = mars_cv_new$bestTune$degree, nprune = mars_cv_new$bestTune$nprune) final_model_preds_new &lt;- predict(final_model_new, newdata = baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((final_model_preds_new - baked_test$Sale_Price)^2)) # test RMSE ## [1] 27873.21 7.37 MARS Without Feature Engineering: Implementation Ames Housing Dataset summary(final_model_new) ## Call: earth(formula=Sale_Price~., data=baked_train, ## degree=mars_cv_new$bestTune$degree, ## nprune=mars_cv_new$bestTune$nprune) ## ## coefficients ## (Intercept) 328830.60 ## Overall_QualVery_Good 54877.14 ## Overall_QualExcellent 122860.26 ## Overall_QualVery_Excellent 200502.50 ## MS_SubClassDuplex_All_Styles_and_Ages -47623.96 ## h(5720-Lot_Area) -7.07 ## h(First_Flr_SF-2032) 960.47 ## h(2110-First_Flr_SF) -83.98 ## h(First_Flr_SF-2110) -1106.04 ## h(1277-Second_Flr_SF) -56.99 ## h(Second_Flr_SF-1277) 257.79 ## h(2000-Year_Built) -810.93 ## ## Selected 12 of 33 terms, and 8 of 74 predictors (nprune=12) ## Termination condition: RSq changed by less than 0.001 at 33 terms ## Importance: First_Flr_SF, Second_Flr_SF, Year_Built, ... ## Number of terms at each degree of interaction: 1 11 (additive model) ## GCV 938805472 RSS 537800335247 GRSq 0.8741227 RSq 0.8829393 7.38 MARS Without Feature Engineering: Implementation Ames Housing Dataset vip(object = mars_cv_new, num_features = 20, method = &quot;model&quot;) 7.39 Your Turn!!! You will work with the titanic.rds dataset which contains information on Titanic passengers. Please download the dataset from Canvas, upload it to Posit Cloud, and load it using the following code. titanic &lt;- readRDS(&quot;titanic.rds&quot;) # load dataset The variables in the dataset are pclass (passenger class - ‘1st’, ‘2nd’, ‘3rd’), survived (‘Yes’ or ‘No’), sex (‘male’ or ‘female’), age (age in years), sibsp (number of siblings or spouses aboard), parch (number of parents or children aboard). We are interested in predicting survived using the rest of the variables in the dataset. Compare the performance (in terms of Accuracy) of the following models: A logistic regression model; A \\(K\\)-NN model with optimal \\(K\\) chosen by CV; A MARS model with optimal hyperparameters chosen by CV; A MARS model with minimal feature engineering. Choose the optimal hyperparameters by CV. Perform the following tasks. Investigate the dataset and complete any necessary tasks. Split the data into training and test sets (80-20). Perform required data preprocessing and create the blueprint. If using step_dummy(), set one_hot = FALSE. Prepare the blueprint on the training data. Obtain the modified training and test datasets. Implement 5-fold CV repeated 5 times for each of the models above. Report the optimal CV Accuracy of each model. Report the optimal hyperparameters for each model. Which model performs best in this situation? Build the final model. Obtain probability and class label predictions on the test set. Create the corresponding confusion matrix and report the test set accuracy. (Check out the help page of predict.earth and the type of objects created.) 7.40 Your Turn!!! Titanic Dataset glimpse(titanic) ## Rows: 1,046 ## Columns: 6 ## $ pclass &lt;fct&gt; 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1st, 1… ## $ survived &lt;fct&gt; Yes, Yes, No, No, No, Yes, Yes, No, Yes, No, No, Yes, Yes, Ye… ## $ sex &lt;fct&gt; female, male, female, male, female, male, female, male, femal… ## $ age &lt;dbl&gt; 29.0000, 0.9167, NA, 30.0000, NA, 48.0000, 63.0000, 39.0000, … ## $ sibsp &lt;int&gt; 0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1… ## $ parch &lt;int&gt; 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 1, 1, 0, 0, 1, … sum(is.na(titanic)) ## [1] 469 7.41 Your Turn!!! Titanic Dataset summary(titanic) ## pclass survived sex age sibsp ## 1st:284 No :619 female:388 Min. : 0.1667 Min. :0.0000 ## 2nd:261 Yes:427 male :658 1st Qu.:21.0000 1st Qu.:0.0000 ## 3rd:501 Median :28.0000 Median :0.0000 ## Mean :29.7110 Mean :0.5029 ## 3rd Qu.:39.0000 3rd Qu.:1.0000 ## Max. :80.0000 Max. :8.0000 ## NA&#39;s :432 ## parch ## Min. :0.0000 ## 1st Qu.:0.0000 ## Median :0.0000 ## Mean :0.4232 ## 3rd Qu.:1.0000 ## Max. :6.0000 ## NA&#39;s :37 levels(titanic$pclass) # levels are in order ## [1] &quot;1st&quot; &quot;2nd&quot; &quot;3rd&quot; # age, sibsp, parch are numerical variables # pclass is an ordinal categorical variable # sex is a nominal categorical variable # missing entries in age and parch 7.42 Your Turn!!! Titanic Dataset # split data set.seed(021423) index &lt;- createDataPartition(titanic$survived, p = 0.8, list = FALSE) # 80-20 split titanic_train &lt;- titanic[index, ] # training data titanic_test &lt;- titanic[-index, ] # test data 7.43 Your Turn!!! Titanic Dataset # create recipe and blueprint, prepare, and bake set.seed(021423) blueprint &lt;- recipe(survived ~ ., data = titanic_train) %&gt;% # create recipe step_impute_mean(age) %&gt;% # impute age by mean step_impute_median(parch) %&gt;% # impute parch by median since discrete integers step_integer(pclass) %&gt;% # label encoding ordinal feature step_normalize(all_numeric_predictors()) %&gt;% # center and scale numerical features step_dummy(sex, one_hot = FALSE) # creating dummy variable for nominal feature prepare &lt;- prep(blueprint, training = titanic_train) # prepare blueprint on training data baked_train &lt;- bake(prepare, new_data = titanic_train) # apply the blueprint to training data baked_test &lt;- bake(prepare, new_data = titanic_test) # apply the blueprint to training data 7.44 Your Turn!!! Titanic Dataset # set up CV set.seed(021423) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications # CV with logistic regression logistic_cv &lt;- train(blueprint, data = titanic_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = cv_specs, metric = &quot;Accuracy&quot;) # CV with KNN k_grid &lt;- expand.grid(k = seq(1, 101, by = 10)) # grid of K knn_cv &lt;- train(blueprint, data = titanic_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;Accuracy&quot;) # CV with MARS param_grid &lt;- expand.grid(degree = 1:3, nprune = seq(1, 100, length.out = 10)) # grid of tuning parameters mars_cv &lt;- train(blueprint, data = titanic_train, method = &quot;earth&quot;, trControl = cv_specs, tuneGrid = param_grid, metric = &quot;Accuracy&quot;) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 7.45 Your Turn!!! Titanic Dataset # create recipe and blueprint, prepare, and bake set.seed(021423) # set seed blueprint_new &lt;- recipe(survived ~ ., data = titanic_train) %&gt;% # create recipe step_impute_mean(age) %&gt;% # impute age by mean step_impute_median(parch) # impute parch by median since discrete integers prepare_new &lt;- prep(blueprint_new, data = titanic_train) # prepare blueprint on training data baked_train_new &lt;- bake(prepare_new, new_data = titanic_train) # apply the blueprint to training data baked_test_new &lt;- bake(prepare_new, new_data = titanic_test) # apply the blueprint to test data # CV with MARS (minimal feature engineering) param_grid &lt;- expand.grid(degree = 1:3, nprune = seq(1, 100, length.out = 10)) # grid of tuning parameters mars_cv_new &lt;- train(blueprint_new, data = titanic_train, method = &quot;earth&quot;, trControl = cv_specs, tuneGrid = param_grid, metric = &quot;Accuracy&quot;) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 7.46 Your Turn!!! Titanic Dataset # optimal CV Accuracies logistic_cv$results$Accuracy # logistic regression ## [1] 0.7698763 max(knn_cv$results$Accuracy) # KNN ## [1] 0.7680305 max(mars_cv$results$Accuracy) # MARS ## [1] 0.7897419 max(mars_cv_new$results$Accuracy) # MARS (minimal feature preprocessing) ## [1] 0.7875914 # The optimal MARS model with proper feature engineering seems to perform the best. 7.47 Your Turn!!! Titanic Dataset knn_cv$bestTune$k # optimal K ## [1] 11 mars_cv$bestTune # optimal MARS hyperparameters ## nprune degree ## 12 12 2 mars_cv_new$bestTune # optimal MARS hyperparameters with minimal feature engineering ## nprune degree ## 22 12 3 7.48 Your Turn!!! Titanic Dataset # build final model final_model &lt;- earth(survived ~ ., data = baked_train, degree = mars_cv$bestTune$degree, # optimal degree of interactions nprune = mars_cv$bestTune$nprune, # optimal number of terms after pruning glm = list(family = binomial)) # obtain predictions on test data final_model_prob_preds &lt;- predict(final_model, newdata = baked_test, type = &quot;response&quot;) # probability predictions final_model_class_preds &lt;- predict(final_model, newdata = baked_test, type = &quot;class&quot;) # class label predictions 7.49 Your Turn!!! Titanic Dataset # Notice that the class label predictions are &#39;character&#39; types. # Need to convert then into &#39;factor&#39; type final_model_class_preds &lt;- factor(final_model_class_preds) # confusion matrix confusionMatrix(data = relevel(final_model_class_preds, ref = &quot;Yes&quot;), reference = relevel(baked_test$survived, ref = &quot;Yes&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Yes No ## Yes 59 13 ## No 26 110 ## ## Accuracy : 0.8125 ## 95% CI : (0.7527, 0.8631) ## No Information Rate : 0.5913 ## P-Value [Acc &gt; NIR] : 7.915e-12 ## ## Kappa : 0.6027 ## ## Mcnemar&#39;s Test P-Value : 0.05466 ## ## Sensitivity : 0.6941 ## Specificity : 0.8943 ## Pos Pred Value : 0.8194 ## Neg Pred Value : 0.8088 ## Prevalence : 0.4087 ## Detection Rate : 0.2837 ## Detection Prevalence : 0.3462 ## Balanced Accuracy : 0.7942 ## ## &#39;Positive&#39; Class : Yes ## "],["tree-based-methods.html", "Chapter 8 Tree-Based Methods 8.1 Terminology for Trees 8.2 Terminology for Trees 8.3 Building a Tree and Prediction 8.4 Building a Tree and Prediction 8.5 Building a Tree and Prediction 8.6 Building a Tree and Prediction 8.7 Building a Tree and Prediction 8.8 Building a Tree 8.9 Building a Tree 8.10 Tree Pruning 8.11 Tree Pruning 8.12 Building an Optimal Tree 8.13 Regression Tree: Implementation 8.14 Regression Tree: Implementation 8.15 Regression Tree: Implementation 8.16 Regression Tree: Implementation 8.17 Regression Tree: Implementation 8.18 Regression Tree: Implementation 8.19 Regression Tree: Implementation 8.20 Regression Tree: Implementation 8.21 Regression Tree: Implementation 8.22 Trees 8.23 Regression Tree: Implementation 8.24 Regression Tree: Implementation 8.25 Regression Tree: Implementation 8.26 Regression Tree: Implementation 8.27 Classification Trees 8.28 Your Turn!!! 8.29 Your Turn!!! 8.30 Your Turn!!! 8.31 Your Turn!!! 8.32 Your Turn!!! 8.33 Your Turn!!! 8.34 Your Turn!!! 8.35 Your Turn!!! 8.36 Your Turn!!! 8.37 Ensemble Methods 8.38 Bagging 8.39 Bagging 8.40 Bagging 8.41 Out-of-Bag Error Estimation 8.42 Variable Importance Measures 8.43 Bagging: Implementation 8.44 Bagging: Implementation 8.45 Bagging: Implementation 8.46 Bagging: Disadvantages 8.47 Bagging: Disadvantages 8.48 Random Forests 8.49 Random Forests: Implementation 8.50 Random Forests: Implementation 8.51 Random Forests: Implementation 8.52 Your Turn!!! 8.53 Your Turn!!! 8.54 Your Turn!!! 8.55 Your Turn!!! 8.56 Your Turn!!! 8.57 Your Turn!!! 8.58 Your Turn!!! 8.59 Your Turn!!! 8.60 Your Turn!!! 8.61 Your Turn!!!", " Chapter 8 Tree-Based Methods Involves stratifying or segmenting the predictor space into a number of simple regions. The set of splitting rules used to segment the predictor space can be summarized in a tree, thus, the name decision tree methods. Can be used for both classification and regression. Tree-based methods are simple and useful for interpretation, however, not the best in terms of prediction accuracy. Methods such as bagging, random forests, and boosting grow multiple trees and then combine their results. 8.1 Terminology for Trees Every split is considered to be a node. We refer to the first node at the top of the tree as the root node (this node contains all of the training data). The final nodes at the bottom of the tree are called the terminal nodes or leaves. Decision trees are typically drawn upside down, in the sense that the leaves are at the bottom of the tree. The points along the tree where the predictor space is split are referred to as internal nodes, that is, every node in between the root node and terminal nodes is referred to as an internal node. The segments of the trees that connect the nodes are known as branches. 8.2 Terminology for Trees Figure 8.1: Adapted from HMLR, Boehmke &amp; Greenwell 8.3 Building a Tree and Prediction The steps involved are: Dividing the predictor space, that is, the set of possible values for \\(X_1, X_2, \\ldots, X_p\\) into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). For every observation that falls into the region \\(R_j\\), make the same prediction, which is the mean response of the training set observations in \\(R_j\\) (for regression problems), majority vote response of the training set observations in \\(R_j\\) (for classification problems). 8.4 Building a Tree and Prediction Figure 8.2: Adapted from HMLR, Boehmke &amp; Greenwell 8.5 Building a Tree and Prediction Default Dataset 8.6 Building a Tree and Prediction Default Dataset 8.7 Building a Tree and Prediction Figure 8.3: Adapted from ISLR, James et al. 8.8 Building a Tree It is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes. For this reason, we take a top-down, greedy approach known as recursive binary splitting. top-down because it begins at the top of the tree and then successively splits the predictor space. greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. 8.9 Building a Tree First select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X|X_j &lt; s\\}\\) and \\(\\{X|X_j \\ge s\\}\\) leads to the greatest possible reduction in RSS. For any \\(j\\) and \\(s\\), define Find \\(j\\) and \\(s\\) that minimize Next, repeat the process, look for the best predictor and best cutpoint in order to split the data further. However, this time, instead of splitting the entire predictor space, split one of the two previously identified regions. The process continues until a stopping criterion is reached; say, we may continue until no region contains more than five observations. 8.10 Tree Pruning The process described above may overfit the data. A smaller tree with fewer splits (that is, fewer regions \\(R_1,\\ldots,R_J\\)) might lead to lower variance and better interpretation at the cost of a little bias. One possible alternative is to grow the tree only so long as the decrease in the \\(RSS\\) due to each split exceeds some (high) threshold. This strategy will result in smaller trees, but is too short-sighted: a seemingly worthless split early on in the tree might be followed by a very good split, that is, a split that leads to a large reduction in \\(RSS\\) later on. 8.11 Tree Pruning A better strategy is pruning. Grow a very large tree \\(T_0\\), and then prune it back to obtain a subtree. The technique uses is known as cost complexity pruning (also known as weakest link pruning). Consider a sequence of trees indexed by \\(\\alpha\\). For each \\(\\alpha\\), consider the tree \\(T \\subset T_0\\) that minimizes Choose optimal \\(\\alpha\\) by CV. 8.12 Building an Optimal Tree 8.13 Regression Tree: Implementation Ames Housing Dataset ames &lt;- readRDS(&quot;AmesHousing.rds&quot;) # load dataset ames$Overall_Qual &lt;- factor(ames$Overall_Qual, levels = c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;)) # split data set.seed(022123) # set seed index &lt;- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE) # consider 70-30 split ames_train &lt;- ames[index,] # training data ames_test &lt;- ames[-index,] # test data 8.14 Regression Tree: Implementation Ames Housing Dataset # create recipe and blueprint, prepare and apply blueprint set.seed(022123) # set seed ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) # set up recipe blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors step_scale(all_numeric(), -all_outcomes()) %&gt;% # scale (divide by standard deviation) all numeric predictors step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% # lumping required predictors step_dummy(all_nominal(), one_hot = TRUE) # one-hot/dummy encode nominal categorical predictors prepare &lt;- prep(blueprint, data = ames_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = ames_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = ames_test) # apply the blueprint to test data for future use 8.15 Regression Tree: Implementation Ames Housing Dataset Implement CV to tune the hyperparameter. set.seed(022123) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications library(rpart) tree_cv &lt;- train(blueprint, data = ames_train, method = &quot;rpart&quot;, trControl = cv_specs, tuneLength = 20, # considers a grid of 20 possible tuning parameter values metric = &quot;RMSE&quot;) ## Warning in train_rec(rec = x, dat = data, info = trainInfo, method = models, : ## There were missing values in resampled performance measures. # results from the CV procedure tree_cv$bestTune # optimal hyperparameter ## cp ## 1 0.00185082 min(tree_cv$results$RMSE) # optimal CV RMSE ## [1] 40442.78 8.16 Regression Tree: Implementation Ames Housing Dataset Results from the CV procedure. ggplot(tree_cv) 8.17 Regression Tree: Implementation Ames Housing Dataset # build final model final_model &lt;- rpart(Sale_Price ~ ., data = baked_train, cp = tree_cv$bestTune$cp, xval = 0, # no further CV method = &quot;anova&quot;) # for regression # obtain predictions and test set RMSE final_model_preds &lt;- predict(final_model, newdata = baked_test) # obtain test set predictions sqrt(mean((final_model_preds - baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 50504.24 8.18 Regression Tree: Implementation Ames Housing Dataset library(rpart.plot) rpart.plot(final_model) 8.19 Regression Tree: Implementation Ames Housing Dataset # variable importance vip(object = tree_cv, num_features = 20, method = &quot;model&quot;) 8.20 Regression Tree: Implementation Ames Housing Dataset # build full grown tree (no pruning) final_model_no_prune &lt;- rpart(Sale_Price ~ ., data = baked_train, cp = 0, # no pruning xval = 0, # no CV method = &quot;anova&quot;) # for regression # obtain predictions and test set RMSE final_model_no_prune_preds &lt;- predict(final_model_no_prune, newdata = baked_test) # obtain test set predictions sqrt(mean((final_model_no_prune_preds - baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 50106.39 8.21 Regression Tree: Implementation Ames Housing Dataset rpart.plot(final_model_no_prune) 8.22 Trees Advantages Easy to explain. Closely mirror human decision-making. Can be displayed graphically, and are easily interpreted by non-experts. Handle qualitative predictors without creating dummy variables. Does not require standardization of predictors. Disadvantages Do not have same level of prediction accuracy. Can be very non-robust. 8.23 Regression Tree: Implementation Ames Housing Dataset (minimal feature engineering) # create recipe and blueprint, prepare and apply blueprint set.seed(022123) # set seed ames_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) # set up recipe blueprint_new &lt;- ames_recipe %&gt;% step_impute_mean(Gr_Liv_Area) # impute missing entries prepare_new &lt;- prep(blueprint_new, data = ames_train) # estimate feature engineering parameters based on training data baked_train_new &lt;- bake(prepare_new, new_data = ames_train) # apply the blueprint to training data for building final/optimal model baked_test_new &lt;- bake(prepare_new, new_data = ames_test) # apply the blueprint to test data for future use 8.24 Regression Tree: Implementation Ames Housing Dataset Implement CV to tune the hyperparameter. set.seed(022123) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications tree_cv_min_fe &lt;- train(blueprint_new, data = ames_train, method = &quot;rpart&quot;, trControl = cv_specs, tuneLength = 20, # considers a grid of 20 possible tuning parameter values metric = &quot;RMSE&quot;) ## Warning in train_rec(rec = x, dat = data, info = trainInfo, method = models, : ## There were missing values in resampled performance measures. # results from the CV procedure tree_cv_min_fe$bestTune # optimal hyperparameter ## cp ## 1 0.002515201 min(tree_cv_min_fe$results$RMSE) # optimal CV RMSE ## [1] 40105.36 8.25 Regression Tree: Implementation Ames Housing Dataset # build final model final_model_min_fe &lt;- rpart(Sale_Price ~ ., data = baked_train_new, cp = tree_cv_min_fe$bestTune$cp, xval = 0, # no CV method = &quot;anova&quot;) # for regression # obtain predictions and test set RMSE final_model_min_fe_preds &lt;- predict(final_model_min_fe, newdata = baked_test_new) # obtain test set predictions sqrt(mean((final_model_min_fe_preds - baked_test_new$Sale_Price)^2)) # calculate test set RMSE ## [1] 52063.37 8.26 Regression Tree: Implementation Ames Housing Dataset # variable importance vip(object = tree_cv_min_fe, num_features = 20, method = &quot;model&quot;) 8.27 Classification Trees Still use recursive binary splitting to grow a classification tree. \\(RSS\\) can be replaced by classification error rate, the fraction of the training observations in that region that do not belong to the most common class. \\[E = 1 - \\max_k \\left(\\hat{p}_{mk}\\right)\\] Gini index, a measure of node purity—a small value indicates that a node contains predominantly observations from a single class. \\[G = \\displaystyle \\sum_{k=1}^{K} \\hat{p}_{mk}\\left(1-\\hat{p}_{mk}\\right)\\] Here \\(\\hat{p}_{mk}\\) represents the proportion of training observations in the \\(m^{th}\\) region that are from the \\(k^{th}\\) class. 8.28 Your Turn!!! You will work with the iris dataset which contains measurements in centimeters of four variables for 50 flowers from each of 3 species of iris: setosa, versicolor, and virginica. Please load the dataset using the following code. data(iris) # load dataset We are interested in predicting Species using the rest of the variables in the dataset. Compare the performance (in terms of Accuracy) of the following models: A logistic regression model; A \\(K\\)-NN model with optimal \\(K\\) chosen by CV; A classification tree with optimal hyperparameter chosen by CV. Use tuneLength = 20. A classification tree with minimal feature engineering. Use CV to choose the optimal hyperparameter. Use tuneLength = 20. Perform the following tasks. Investigate the dataset and complete any necessary tasks. Split the data into training and test sets (75-25). Perform required data preprocessing and create the blueprint. If using step_dummy(), set one_hot = FALSE. Prepare the blueprint on the training data. Obtain the modified training and test datasets. Implement 10-fold CV repeated 5 times for each of the models above. Report the optimal CV Accuracy of each model. Report the optimal hyperparameters for each model. Which model performs best in this situation? Build the final model. Obtain class label predictions on the test set. Create the corresponding confusion matrix and report the test set accuracy. Based on your optimal final model, consult the help page for either predict.knn3 or predict.rpart functions. 8.29 Your Turn!!! glimpse(iris) # all features are numerical ## Rows: 150 ## Columns: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s… summary(iris) # summary of variables ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## sum(is.na(iris)) # no missing entries ## [1] 0 8.30 Your Turn!!! set.seed(022123) # set seed # split the data into training and test sets index &lt;- createDataPartition(iris$Species, p = 0.75, list = FALSE) iris_train &lt;- iris[index, ] iris_test &lt;- iris[-index, ] nearZeroVar(iris_train, saveMetrics = TRUE) # no zv/nzv features ## freqRatio percentUnique zeroVar nzv ## Sepal.Length 1.000000 27.192982 FALSE FALSE ## Sepal.Width 2.090909 17.543860 FALSE FALSE ## Petal.Length 1.250000 35.964912 FALSE FALSE ## Petal.Width 2.300000 19.298246 FALSE FALSE ## Species 1.000000 2.631579 FALSE FALSE 8.31 Your Turn!!! set.seed(022123) # set seed # create recipe and blueprint, prepare and apply blueprint blueprint &lt;- recipe(Species ~ ., data = iris_train) %&gt;% step_normalize(all_predictors()) prepare &lt;- prep(blueprint, training = iris_train) baked_train &lt;- bake(prepare, new_data = iris_train) baked_test &lt;- bake(prepare, new_data = iris_test) 8.32 Your Turn!!! set.seed(022123) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5) # CV specifications logistic_cv &lt;- train(blueprint, data = iris_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = cv_specs, metric = &quot;Accuracy&quot;) # The code above will throw an error since this is a 3-class classification problem and # logistic regression (with family = binomial) works for a binary (2-class) problem. 8.33 Your Turn!!! set.seed(022123) # set seed # CV for KNN k_grid &lt;- expand.grid(k = seq(1, 101, by = 10)) knn_cv &lt;- train(blueprint, data = iris_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;Accuracy&quot;) set.seed(022123) # set seed # CV with tree tree_cv &lt;- train(blueprint, data = iris_train, method = &quot;rpart&quot;, trControl = cv_specs, tuneLength = 20, metric = &quot;Accuracy&quot;) 8.34 Your Turn!!! set.seed(022123) # set seed # CV with tree (minimal feature engineering, no engineering in this case) tree_cv_min_fe &lt;- train(Species ~ ., data = iris_train, method = &quot;rpart&quot;, trControl = cv_specs, tuneLength = 20, metric = &quot;Accuracy&quot;) 8.35 Your Turn!!! # optimal CV Accuracies max(knn_cv$results$Accuracy) # for KNN ## [1] 0.9688788 max(tree_cv$results$Accuracy) # for classification tree ## [1] 0.9353636 max(tree_cv_min_fe$results$Accuracy) # for classification tree with minimal feature engineering ## [1] 0.9353636 # optimal hyperparameters knn_cv$bestTune$k # for KNN ## [1] 11 tree_cv$bestTune$cp # for classification tree ## [1] 0.4210526 tree_cv_min_fe$bestTune$cp # for classification tree with minimal feature engineering ## [1] 0.4210526 8.36 Your Turn!!! # final model final_model &lt;- knn3(Species ~ ., data = baked_train, k = knn_cv$bestTune$k) # obtain probability and class label predictions final_model_prob_preds &lt;- predict(final_model, newdata = baked_test, type = &quot;prob&quot;) # probability predictions final_model_class_preds &lt;- predict(final_model, newdata = baked_test, type = &quot;class&quot;) # class label predictions confusionMatrix(data = final_model_class_preds, reference = baked_test$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 12 0 0 ## versicolor 0 11 1 ## virginica 0 1 11 ## ## Overall Statistics ## ## Accuracy : 0.9444 ## 95% CI : (0.8134, 0.9932) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : 1.728e-14 ## ## Kappa : 0.9167 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 0.9167 0.9167 ## Specificity 1.0000 0.9583 0.9583 ## Pos Pred Value 1.0000 0.9167 0.9167 ## Neg Pred Value 1.0000 0.9583 0.9583 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3056 0.3056 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 1.0000 0.9375 0.9375 8.37 Ensemble Methods Single regression or classification trees usually have poor predictive performance. Ensemble Methods use a collection of multiple trees to improve the predictive performance at the cost of interpretability. Bagging Random Forests Boosting Figure 8.4: Adapted from ISLR, James et al. 8.38 Bagging Bootstrap aggregation or bagging is a general-purpose procedure for reducing the variance of a statistical learning method. Idea: Build multiple trees and average their results. Result: Given a set of \\(n\\) independent observations (random variables) \\(Z_1, \\ldots, Z_n\\), each with variance \\(\\sigma^2\\), the variance of the mean/average \\(\\bar{Z} = \\displaystyle \\dfrac{Z_1 + Z_2 + \\cdots + Z_n}{n}\\) of the observations is \\(\\sigma^2/n\\). In other words, averaging a set of observations reduces variance. In reality, we do not have multiple training datasets. 8.39 Bagging Bootstrapping Figure 8.5: Adapted from ISLR, James et al. 8.40 Bagging Take repeated bootstrap samples (say \\(B\\)) from the original (single) available dataset. Build a tree on each bootstrap sample and obtain predictions \\(\\hat{f}^{*b}(x), \\ b=1, 2, \\ldots, B\\). Average all the predictions. Individual trees are grown deep and are not pruned. They have high variance, but low bias. For classification trees, take majority vote: the overall prediction is the most commonly occurring class among the \\(B\\) predictions. 8.41 Out-of-Bag Error Estimation A straightforward way to estimate the test error of a bagged model, without performing CV. It can be shown that on average, each bagged tree (constructed on each bootstrap sample) makes use of around two-thirds of the observations. Remaining one-third observations not used train a bagged tree are referred to as out-of-bag (OOB) observations. For \\(i^{th}\\) observation, use the trees in which that observation was OOB. This will yield around \\(B/3\\) predictions for the \\(i^{th}\\) observation. Take their average to obtain a single prediction. Equivalent to LOOCV if \\(B\\) is large. 8.42 Variable Importance Measures Bagging improves prediction accuracy at the expense of interpretability. However, one can still obtain an overall summary of the importance of each predictor. To measure feature importance, the reduction in the loss function (e.g., RSS) attributed to each variable at each split is tabulated. In some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance. A large value indicates an important predictor. 8.43 Bagging: Implementation Ames Housing Dataset Data splitting and feature engineering has been done in the previous slides. set.seed(022123) # set seed library(ipred) bag_fit &lt;- bagging(Sale_Price ~ ., data = baked_train, nbagg = 500, # number of trees to grow (bootstrap samples) usually 500 coob = TRUE, # yes to computing OOB error estimate control = rpart.control(minsplit = 2, # split a node if at least 2 observations present cp = 0, # no pruning (let the trees grow tall) xval = 0)) # no CV bag_fit ## ## Bagging regression trees with 500 bootstrap replications ## ## Call: bagging.data.frame(formula = Sale_Price ~ ., data = baked_train, ## nbagg = 500, coob = TRUE, control = rpart.control(minsplit = 2, ## cp = 0, xval = 0)) ## ## Out-of-bag estimate of root mean squared error: 28762.96 8.44 Bagging: Implementation Ames Housing Dataset CV with bagging (NOT recommended since computationally expensive) set.seed(022123) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications library(ipred) library(e1071) bagging_cv &lt;- train(blueprint, data = ames_train, method = &quot;treebag&quot;, trControl = cv_specs, nbagg = 500, control = rpart.control(minsplit = 2, cp = 0), metric = &quot;RMSE&quot;) 8.45 Bagging: Implementation # obtain predictions on the test set final_model_preds &lt;- predict(bag_fit, newdata = baked_test) # use &#39;type = &quot;class&quot;&#39; for classification trees sqrt(mean((final_model_preds - baked_test$Sale_Price)^2)) # test set RMSE ## [1] 43202.54 # variable importance imp &lt;- varImp(bag_fit) # look at the object created 8.46 Bagging: Disadvantages Bagging improves the prediction accuracy for high variance (and low bias) models at the expense of interpretability and computational speed. However, although the model building steps are independent, the trees in bagging are not completely independent of each other since all the original features are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to any underlying strong relationships. This characteristic is known as tree correlation and prevents bagging from further reducing the variance of the individual models. Random forests extend and improve upon bagged decision trees by reducing this correlation and thereby improving the accuracy of the overall ensemble. 8.47 Bagging: Disadvantages Tree Correlation in Bagging Figure 8.6: Adapted from HMLR, Boehmke &amp; Greenwell 8.48 Random Forests Provide an improvement over bagged trees by reducing the variance further (by decorrelating) when we average the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. For each tree, each time a split is considered, a random selection of \\(m\\) predictors is chosen (split candidates) from the full set of \\(p\\) predictors. The split is allowed to use only one of those \\(m\\) predictors. Note that in bagging, each split for each tree considers all \\(p\\) predictors as split candidates. A fresh sample of \\(m\\) predictors is taken at each split. Typical default values are \\(m = p/3\\) (regression) and \\(m = \\sqrt{p}\\) (classification) but this should be considered a tuning parameter, to be chosen by CV. 8.49 Random Forests: Implementation Ames Housing Dataset Data splitting and feature engineering has been done in the previous slides. set.seed(022123) # set seed cv_specs &lt;- trainControl(method = &quot;cv&quot;, number = 5) # CV specifications library(ranger) library(e1071) param_grid &lt;- expand.grid(mtry = seq(1, 30, 1), # sequence of 1 to at least half the number of predictors splitrule = &quot;variance&quot;, # use &quot;gini&quot; for classification min.node.size = 2) # for each tree rf_cv &lt;- train(blueprint, data = ames_train, method = &quot;ranger&quot;, trControl = cv_specs, tuneGrid = param_grid, metric = &quot;RMSE&quot;) rf_cv$bestTune$mtry # optimal tuning parameter ## [1] 28 min(rf_cv$results$RMSE) # optimal CV RMSE ## [1] 30948.63 8.50 Random Forests: Implementation Ames Housing Dataset # fit final model final_model &lt;- ranger(Sale_Price ~ ., data = baked_train, num.trees = 500, mtry = rf_cv$bestTune$mtry, splitrule = &quot;variance&quot;, min.node.size = 2, importance = &quot;impurity&quot;) # obtain predictions on the test set final_model_preds &lt;- predict(final_model, data = baked_test, type = &quot;response&quot;) # predictions on test set sqrt(mean((final_model_preds$predictions - baked_test$Sale_Price)^2)) # test set RMSE ## [1] 36257.25 8.51 Random Forests: Implementation Ames Housing Dataset # variable importance head(sort(final_model$variable.importance, decreasing = TRUE), 10) # top 10 most important features ## Overall_Qual First_Flr_SF Garage_Cars Gr_Liv_Area Year_Built ## 1.794339e+12 6.989855e+11 6.325767e+11 4.106476e+11 3.467222e+11 ## Garage_Area Lot_Area Second_Flr_SF Lot_Frontage Open_Porch_SF ## 2.715779e+11 1.557609e+11 1.146867e+11 6.614335e+10 4.534856e+10 8.52 Your Turn!!! You will work with the vowels.rds data. The task is to predict letter (five vowels) using the rest of the variables in the data (predictors). vowels &lt;- readRDS(&quot;vowels.rds&quot;) # load dataset Compare the performance (in terms of Accuracy) of the following models. Choose the optimal hyperparameters using CV. A \\(K\\)-NN classifier A single classification tree (use tuneLength = 20) A bagged model A random forest model Perform the following tasks. Investigate the dataset and complete any necessary tasks. Split the data into training and test sets (70-30). Perform required data preprocessing and create the blueprint. If using step_dummy(), set one_hot = FALSE. Prepare the blueprint on the training data. Obtain the modified training and test datasets. Implement 5-fold CV repeated 5 times for each of the models above. Report the optimal CV (or, OOB) Accuracy of each model. Report the optimal hyperparameters for each model. Which model performs best in this situation? Build the final model. Obtain class label predictions on the test set. Create the corresponding confusion matrix and report the test set accuracy. 8.53 Your Turn!!! glimpse(vowels) # all features are numerical ## Rows: 1,941 ## Columns: 17 ## $ letter &lt;fct&gt; I, O, E, E, E, A, E, O, U, I, E, I, U, A, O, E, U, I, A, A, E, … ## $ xbox &lt;int&gt; 5, 3, 3, 3, 6, 3, 4, 4, 7, 2, 3, 3, 4, 2, 5, 1, 3, 2, 2, 8, 3, … ## $ ybox &lt;int&gt; 12, 4, 4, 7, 9, 7, 8, 7, 11, 9, 8, 9, 7, 1, 10, 0, 3, 6, 8, 15,… ## $ width &lt;int&gt; 3, 4, 3, 4, 4, 5, 5, 5, 8, 3, 3, 4, 4, 4, 4, 1, 3, 2, 4, 7, 3, … ## $ height &lt;int&gt; 7, 3, 6, 5, 4, 5, 6, 5, 9, 7, 6, 7, 5, 2, 5, 1, 1, 4, 6, 8, 6, … ## $ onpix &lt;int&gt; 2, 2, 2, 4, 2, 3, 4, 3, 4, 2, 2, 3, 2, 1, 3, 1, 1, 1, 2, 4, 3, … ## $ xbar &lt;int&gt; 10, 8, 3, 7, 7, 12, 7, 8, 3, 8, 3, 7, 7, 8, 6, 4, 5, 9, 12, 8, … ## $ ybar &lt;int&gt; 5, 7, 8, 7, 7, 2, 7, 7, 9, 7, 7, 7, 5, 1, 6, 7, 8, 5, 2, 2, 6, … ## $ x2bar &lt;int&gt; 5, 7, 6, 5, 4, 3, 4, 8, 6, 0, 6, 0, 13, 2, 6, 5, 5, 0, 4, 3, 5,… ## $ y2bar &lt;int&gt; 4, 5, 10, 8, 7, 2, 8, 5, 7, 7, 10, 7, 5, 2, 2, 8, 7, 6, 3, 2, 9… ## $ xybar &lt;int&gt; 13, 7, 7, 8, 10, 10, 11, 10, 11, 13, 7, 13, 7, 7, 9, 7, 10, 13,… ## $ x2ybar &lt;int&gt; 3, 6, 6, 8, 6, 2, 8, 6, 11, 6, 6, 6, 14, 2, 6, 6, 9, 5, 2, 5, 4… ## $ xy2bar &lt;int&gt; 9, 8, 15, 9, 10, 9, 9, 8, 10, 9, 14, 8, 7, 8, 9, 13, 8, 9, 10, … ## $ xedge &lt;int&gt; 2, 2, 0, 3, 1, 2, 2, 3, 3, 0, 0, 0, 3, 2, 5, 0, 3, 0, 3, 5, 0, … ## $ xegvy &lt;int&gt; 8, 8, 8, 9, 9, 6, 9, 8, 9, 8, 8, 8, 9, 5, 9, 8, 10, 8, 6, 4, 8,… ## $ yedge &lt;int&gt; 4, 3, 7, 6, 7, 3, 5, 3, 2, 1, 7, 1, 0, 2, 4, 6, 2, 1, 3, 5, 6, … ## $ yegvx &lt;int&gt; 10, 8, 8, 9, 9, 8, 7, 8, 6, 8, 8, 8, 8, 7, 8, 10, 6, 8, 9, 5, 9… 8.54 Your Turn!!! sum(is.na(vowels)) # no missing entries ## [1] 0 summary(vowels) # summary of variables ## letter xbox ybox width height ## A:395 Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. :0.000 ## E:384 1st Qu.: 2.000 1st Qu.: 5.000 1st Qu.: 3.000 1st Qu.:4.000 ## I:378 Median : 3.000 Median : 7.000 Median : 5.000 Median :5.000 ## O:377 Mean : 3.594 Mean : 6.968 Mean : 4.561 Mean :5.185 ## U:407 3rd Qu.: 5.000 3rd Qu.: 9.000 3rd Qu.: 6.000 3rd Qu.:7.000 ## Max. :12.000 Max. :15.000 Max. :10.000 Max. :9.000 ## onpix xbar ybar x2bar ## Min. : 0.000 Min. : 2.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 2.000 1st Qu.: 7.000 1st Qu.: 6.000 1st Qu.: 2.000 ## Median : 3.000 Median : 7.000 Median : 7.000 Median : 4.000 ## Mean : 3.044 Mean : 7.171 Mean : 6.366 Mean : 4.648 ## 3rd Qu.: 4.000 3rd Qu.: 8.000 3rd Qu.: 8.000 3rd Qu.: 6.000 ## Max. :12.000 Max. :14.000 Max. :11.000 Max. :15.000 ## y2bar xybar x2ybar xy2bar ## Min. : 0.000 Min. : 3.000 Min. : 0.000 Min. : 4.00 ## 1st Qu.: 4.000 1st Qu.: 7.000 1st Qu.: 5.000 1st Qu.: 8.00 ## Median : 5.000 Median : 7.000 Median : 6.000 Median : 8.00 ## Mean : 5.277 Mean : 8.258 Mean : 6.013 Mean : 8.61 ## 3rd Qu.: 7.000 3rd Qu.:10.000 3rd Qu.: 7.000 3rd Qu.: 9.00 ## Max. :12.000 Max. :14.000 Max. :15.000 Max. :15.00 ## xedge xegvy yedge yegvx ## Min. : 0.000 Min. : 1.000 Min. : 0.000 Min. : 1.000 ## 1st Qu.: 2.000 1st Qu.: 8.000 1st Qu.: 2.000 1st Qu.: 7.000 ## Median : 3.000 Median : 8.000 Median : 3.000 Median : 8.000 ## Mean : 2.527 Mean : 7.931 Mean : 3.245 Mean : 7.734 ## 3rd Qu.: 3.000 3rd Qu.: 9.000 3rd Qu.: 5.000 3rd Qu.: 8.000 ## Max. :11.000 Max. :13.000 Max. :11.000 Max. :13.000 8.55 Your Turn!!! set.seed(022123) # set seed # split the data into training and test sets index &lt;- createDataPartition(vowels$letter, p = 0.7, list = FALSE) vowels_train &lt;- vowels[index, ] vowels_test &lt;- vowels[-index, ] nearZeroVar(vowels_train, saveMetrics = TRUE) # no zv/nzv features ## freqRatio percentUnique zeroVar nzv ## letter 1.028881 0.3676471 FALSE FALSE ## xbox 1.171756 0.8823529 FALSE FALSE ## ybox 1.116959 1.1764706 FALSE FALSE ## width 1.042254 0.8088235 FALSE FALSE ## height 1.121849 0.7352941 FALSE FALSE ## onpix 1.473251 0.9558824 FALSE FALSE ## xbar 1.798680 0.9558824 FALSE FALSE ## ybar 1.726950 0.8823529 FALSE FALSE ## x2bar 1.010695 1.1764706 FALSE FALSE ## y2bar 1.012712 0.9558824 FALSE FALSE ## xybar 3.465839 0.8823529 FALSE FALSE ## x2ybar 2.462766 1.1764706 FALSE FALSE ## xy2bar 1.745098 0.8823529 FALSE FALSE ## xedge 1.550162 0.8823529 FALSE FALSE ## xegvy 2.433213 0.9558824 FALSE FALSE ## yedge 1.189189 0.8823529 FALSE FALSE ## yegvx 2.688976 0.8823529 FALSE FALSE 8.56 Your Turn!!! set.seed(022123) # set seed # create recipe and blueprint, prepare and apply blueprint blueprint &lt;- recipe(letter ~ ., data = vowels_train) %&gt;% step_normalize(all_predictors()) prepare &lt;- prep(blueprint, training = vowels_train) baked_train &lt;- bake(prepare, new_data = vowels_train) baked_test &lt;- bake(prepare, new_data = vowels_test) 8.57 Your Turn!!! set.seed(022123) # set seed cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 5) # CV specifications set.seed(022123) # set seed # CV for KNN k_grid &lt;- expand.grid(k = seq(1, 101, by = 10)) knn_cv &lt;- train(blueprint, data = vowels_train, method = &quot;knn&quot;, trControl = cv_specs, tuneGrid = k_grid, metric = &quot;Accuracy&quot;) set.seed(022123) # set seed # CV with tree tree_cv &lt;- train(blueprint, data = vowels_train, method = &quot;rpart&quot;, trControl = cv_specs, tuneLength = 20, metric = &quot;Accuracy&quot;) 8.58 Your Turn!!! set.seed(022123) # set seed # bagging bag_fit &lt;- bagging(letter ~ ., data = baked_train, nbagg = 500, coob = TRUE, control = rpart.control(minsplit = 2, cp = 0, xval = 0)) set.seed(022123) # set seed # CV with random forests param_grid &lt;- expand.grid(mtry = seq(1, 16, 1), # 16 features in baked_train splitrule = &quot;gini&quot;, min.node.size = 2) rf_cv &lt;- train(blueprint, data = vowels_train, method = &quot;ranger&quot;, trControl = cv_specs, tuneGrid = param_grid, metric = &quot;Accuracy&quot;) 8.59 Your Turn!!! # optimal CV Accuracies max(knn_cv$results$Accuracy) # for KNN ## [1] 0.9866132 max(tree_cv$results$Accuracy) # for classification tree ## [1] 0.9194068 1-bag_fit$err # for bagging ## [1] 0.9772059 max(rf_cv$results$Accuracy) # for random forests ## [1] 0.9910256 # optimal hyperparameters knn_cv$bestTune$k # for KNN ## [1] 1 tree_cv$bestTune$cp # for classification tree ## [1] 0 rf_cv$bestTune$mtry # for random forests ## [1] 8 8.60 Your Turn!!! # build final model final_model &lt;- ranger(letter ~ ., data = baked_train, num.trees = 500, mtry = rf_cv$bestTune$mtry, splitrule = &quot;gini&quot;, min.node.size = 2, importance = &quot;impurity&quot;) # obtain predictions on test data final_model_preds &lt;- predict(final_model, data = baked_test, type = &quot;response&quot;) # predictions on test set 8.61 Your Turn!!! # confusion matrix confusionMatrix(data = final_model_preds$predictions, reference = baked_test$letter) ## Confusion Matrix and Statistics ## ## Reference ## Prediction A E I O U ## A 118 0 1 0 1 ## E 0 115 4 1 0 ## I 0 0 108 0 0 ## O 0 0 0 110 0 ## U 0 0 0 2 121 ## ## Overall Statistics ## ## Accuracy : 0.9845 ## 95% CI : (0.9708, 0.9929) ## No Information Rate : 0.21 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9806 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: A Class: E Class: I Class: O Class: U ## Sensitivity 1.0000 1.0000 0.9558 0.9735 0.9918 ## Specificity 0.9957 0.9893 1.0000 1.0000 0.9956 ## Pos Pred Value 0.9833 0.9583 1.0000 1.0000 0.9837 ## Neg Pred Value 1.0000 1.0000 0.9894 0.9936 0.9978 ## Prevalence 0.2031 0.1979 0.1945 0.1945 0.2100 ## Detection Rate 0.2031 0.1979 0.1859 0.1893 0.2083 ## Detection Prevalence 0.2065 0.2065 0.1859 0.1893 0.2117 ## Balanced Accuracy 0.9978 0.9946 0.9779 0.9867 0.9937 "],["support-vector-machines-svm.html", "Chapter 9 Support Vector Machines (SVM) 9.1 Hyperplane 9.2 Hyperplane 9.3 Hyperplane 9.4 Separating Hyperplane 9.5 Separating Hyperplane 9.6 Optimal Separating Hyperplane 9.7 Optimal Separating Hyperplane 9.8 Optimal Separating Hyperplane 9.9 Optimal Separating Hyperplane: Issue 1 9.10 Optimal Separating Hyperplane: Issue 2 9.11 Support Vector Classifier 9.12 Support Vector Classifier 9.13 Support Vector Classifier 9.14 Support Vector Classifier 9.15 Support Vector Classifier 9.16 Support Vector Classifier 9.17 Non-linear Boundaries 9.18 Feature Expansion 9.19 Feature Expansion 9.20 Feature Expansion 9.21 Non-linear Boundaries: Circle dataset 9.22 Non-linear Boundaries: Circle dataset 9.23 Non-linear Boundaries: Circle dataset 9.24 Non-linear Boundaries: Spirals dataset 9.25 Non-linear Boundaries: Spirals dataset 9.26 Non-linear Boundaries: Spirals dataset 9.27 Non-linear Boundaries: Spirals dataset 9.28 Summary 9.29 Your Turn!!! 9.30 Your Turn!!! 9.31 Your Turn!!! 9.32 Your Turn!!! 9.33 Your Turn!!! 9.34 Your Turn!!! 9.35 Your Turn!!! 9.36 Your Turn!!! 9.37 Your Turn!!! 9.38 Your Turn!!! 9.39 Your Turn!!! 9.40 Summary of Supervised Learning Methods 9.41 Neural Networks 9.42 Neural Networks 9.43 Biological Neural Networks 9.44 Artificial Neural Networks 9.45 Neural Networks 9.46 Neural Networks 9.47 Neural Networks 9.48 Your Turn!!! 9.49 Neural Networks 9.50 Neural Networks 9.51 Neural Networks: Input Data 9.52 Neural Networks: Network Architecture 9.53 Neural Networks: Network Architecture 9.54 Neural Networks: Network Architecture 9.55 Neural Networks: Feedback Mechanism 9.56 Neural Networks: Feedback Mechanism 9.57 Neural Networks: Model Training 9.58 Neural Networks: Model Training 9.59 Neural Networks: Further Topics", " Chapter 9 Support Vector Machines (SVM) One of the best “out of the box” classifiers. Mostly intended for two-class classification problems. Idea: Try and find a plane that separates the classes in feature space. We will talk about Maximal Margin Classifier Support Vector Classifier Support Vector Machine 9.1 Hyperplane In \\(p\\)-dimensions, a hyperplane is a flat affine subspace of dimension \\(p-1\\). Mathematical form of a hyperplane When \\(p=2\\), a hyperplane is a line. If \\(\\beta_0=0\\), the hyperplane goes through the origin, otherwise not. A hyperplane divides the \\(p\\)-dim space into 2 halves. 9.2 Hyperplane Hyperplane in 2-dimensions: \\(1+2X_1+3X_2\\)=0 9.3 Hyperplane Hyperplane in 3-dimensions: \\(1+2X_1+3X_2-X_3=0\\) 9.4 Separating Hyperplane For a two-class problem, suppose that it is possible to construct a hyperplane that separates the training observations perfectly according to their class labels. Such a hyperplane is known as a separating hyperplane. 9.5 Separating Hyperplane For a \\(p\\)-dimensional two-class problem, Equivalently, for \\(i=1,2,\\ldots,n\\) Consider a test observation \\(x^*\\), we compute \\(f(x^*)\\). A classifier based on a separating hyperplane leads to a linear decision boundary. 9.6 Optimal Separating Hyperplane This is also known as the maximal margin classifier or hard margin classifier. One that makes the biggest gap or margin between the two classes. One that is farthest from the training observations. Margin: The minimal (perpendicular) distance from the observations to the hyperplane. Denoted by \\(M\\). The maximal margin hyperplane is the separating hyperplane for which the margin is largest, that is, the hyperplane that has the farthest minimum distance to the training observations. 9.7 Optimal Separating Hyperplane ## Setting default kernel parameters 9.8 Optimal Separating Hyperplane The second constraint guarantees that each observation will be on the correct side of the hyperplane (\\(M\\) positive). The first constraint ensures that the perpendicular distance from \\(i^{th}\\) observation to the hyperplane is \\[y_i \\left( \\beta_0+\\beta_1 \\ x_{i1}+\\beta_2 \\ x_{i2} + \\ldots + \\beta_p \\ x_{ip}\\right)\\] 9.9 Optimal Separating Hyperplane: Issue 1 The optimal separating hyperplane fits the data too hard. 9.10 Optimal Separating Hyperplane: Issue 2 An optimal separating hyperplane may not always be possible to construct, that is, non-separable data. This is often the case, unless \\(n&lt;p\\). 9.11 Support Vector Classifier We might be willing to misclassify a few observations for greater robustness to individual observations, and better classify most of the observations. This leads us to the support vector classifier. Also called the soft margin classifier. The margin is soft because it can be violated by some of the training observations. 9.12 Support Vector Classifier \\(M\\): width of the margin \\(\\epsilon_1, \\ldots, \\epsilon_n\\): Slack variables \\(C\\): Budget (tuning parameter) 9.13 Support Vector Classifier set.seed(022823) # set seed library(kernlab) # load library # implement CV to find optimal C svc_cv &lt;- train(y ~ ., data = svcdata, method = &quot;svmLinear&quot;, trControl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5), tuneLength = 20, metric = &quot;Accuracy&quot;) svc_cv$bestTune # optimal C ## C ## 1 1 # fit model with optimal C final_model_svc &lt;- ksvm(y ~ ., data = svcdata, kernel = &quot;vanilladot&quot;, C = svc_cv$bestTune$C, prob.model = TRUE) # needed to obtain predicted probabilities ## Setting default kernel parameters 9.14 Support Vector Classifier 9.15 Support Vector Classifier final_model_svc # number of support vectors ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Linear (vanilla) kernel function. ## ## Number of Support Vectors : 16 ## ## Objective Function Value : -14.2888 ## Training error : 0.047619 ## Probability model included. alphaindex(final_model_svc) # which observations are support vectors ## [[1]] ## [1] 16 17 21 22 31 49 55 56 88 96 97 101 102 103 104 105 9.16 Support Vector Classifier plot(final_model_svc) 9.17 Non-linear Boundaries Why support vector classifiers fail? 9.18 Feature Expansion The problem of non-linear boundaries can be solved by enlarging the feature space (like in linear regression) using transformations of predictors. Fit a support vector classifier in the enlarged space. Results in non-linear boundaries in the original space. 9.19 Feature Expansion 9.20 Feature Expansion A kernel function quantifies the similarity between two observations. It helps in transforming the original feature space to an enlarged feature space where the data points can be separated by a linear boundary. Commonly used kernel funtions are Polynomial Kernel of degree \\(d\\) \\[k(x_i, x_{i&#39;}) = \\left(1+scale\\sum_{j=1}^{p} x_{ij} \\ x_{i&#39;j} \\right)^{degree}\\] Radial Basis Function Kernel \\[k(x_i, x_{i&#39;}) = \\text{exp}\\left(-\\sigma\\sum_{j=1}^{p} (x_{ij} - x_{i&#39;j})^2 \\right)\\] A support vector classifier with a non-linear kernel is known as a support vector machine. 9.21 Non-linear Boundaries: Circle dataset We train an SVM with the polynomial kernel. set.seed(022823) # set seed # implement CV to find optimal parameters param_grid_poly &lt;- expand.grid(degree = c(1, 2, 3, 4), scale = c(0.5, 1, 2), C = c(0.001, 0.1, 1, 10, 10)) svm_poly_cv &lt;- train(y ~ ., data = circle, method = &quot;svmPoly&quot;, trControl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5), tuneGrid = param_grid_poly, metric = &quot;Accuracy&quot;) svm_poly_cv$bestTune ## degree scale C ## 19 2 1 1 max(svm_poly_cv$results$Accuracy) ## [1] 0.9850852 9.22 Non-linear Boundaries: Circle dataset # fit model with optimal parameters final_model_svm_poly &lt;- ksvm(y ~ ., data = circle, kernel = &quot;polydot&quot;, kpar = list(degree = svm_poly_cv$bestTune$degree, scale = svm_poly_cv$bestTune$scale, offset = 1), C = svm_poly_cv$bestTune$C, prob.model = TRUE) final_model_svm_poly ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Polynomial kernel function. ## Hyperparameters : degree = 2 scale = 1 offset = 1 ## ## Number of Support Vectors : 38 ## ## Objective Function Value : -26.8156 ## Training error : 0.01 ## Probability model included. 9.23 Non-linear Boundaries: Circle dataset 9.24 Non-linear Boundaries: Spirals dataset 9.25 Non-linear Boundaries: Spirals dataset We train an SVM with the radial basis function kernel. set.seed(022823) # set seed # implement CV to find optimal parameters param_grid_radial &lt;- expand.grid(sigma = c(0.5, 1, 1.5, 2), C = c(0.001, 0.01, 1, 5, 10, 100)) svm_radial_cv &lt;- train(classes ~ ., data = spirals, method = &quot;svmRadial&quot;, tuneGrid = param_grid_radial, trControl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5), metric = &quot;Accuracy&quot;) svm_radial_cv$bestTune ## sigma C ## 24 2 100 max(svm_radial_cv$results$Accuracy) ## [1] 0.8866667 9.26 Non-linear Boundaries: Spirals dataset # fit model with optimal parameters final_model_svm_radial &lt;- ksvm(classes ~ ., data = spirals, kernel = &quot;rbfdot&quot;, kpar = list(sigma = svm_radial_cv$bestTune$sigma), C = svm_radial_cv$bestTune$C, prob.model = TRUE) final_model_svm_radial ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 100 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 2 ## ## Number of Support Vectors : 89 ## ## Objective Function Value : -4086.086 ## Training error : 0.036667 ## Probability model included. 9.27 Non-linear Boundaries: Spirals dataset 9.28 Summary SVMs are black-box algorithms. Lack interpretability. One of the best methods for two-class classification problems. If we wish to estimate probabilities, logistic regression is the way to go. For non-linear boundaries, SVMs are popular. 9.29 Your Turn!!! You will work with the Sonar data from the mlbench package. The task is to predict Class (‘R’ if the object is a rock and ‘M’ if it is a mine (metal cylinder)) using the rest of the variables in the data (predictors). library(mlbench) # load library data(Sonar) # load dataset Compare the performance of the following support vector-based models: a support vector classifier, a support vector machine with polynomial kernel, a support vector machine with radial basis function kernel. 9.30 Your Turn!!! Perform the following tasks. Investigate the dataset and complete any necessary tasks. Split the data into training and test sets (70-30). Perform required data preprocessing and create the blueprint. If using step_dummy(), set one_hot = FALSE. Prepare the blueprint on the training data. Obtain the modified training and test datasets. Implement 5-fold CV (no repeats) for each of the models above. Report the optimal CV Accuracy of each model. Report the optimal hyperparameters for each model. Which model performs best in this situation? Build the final model. Obtain class label predictions on the test set. Create the corresponding confusion matrix and report the test set accuracy. See help page for predict.ksvm function. 9.31 Your Turn!!! glimpse(Sonar) # all features are numerical (output not displayed here) sum(is.na(Sonar)) # no missing entries ## [1] 0 9.32 Your Turn!!! set.seed(022823) # set seed # split the data into training and test sets index &lt;- createDataPartition(Sonar$Class, p = 0.7, list = FALSE) Sonar_train &lt;- Sonar[index, ] Sonar_test &lt;- Sonar[-index, ] nearZeroVar(Sonar_train, saveMetrics = TRUE) # no zv/nzv features (output not displayed here) 9.33 Your Turn!!! set.seed(022823) # set seed # create recipe and blueprint, prepare and apply blueprint blueprint &lt;- recipe(Class ~ ., data = Sonar_train) %&gt;% step_normalize(all_predictors()) prepare &lt;- prep(blueprint, training = Sonar_train) baked_train &lt;- bake(prepare, new_data = Sonar_train) baked_test &lt;- bake(prepare, new_data = Sonar_test) 9.34 Your Turn!!! set.seed(022823) # set seed cv_specs &lt;- trainControl(method = &quot;cv&quot;, number = 5) # CV specifications set.seed(022823) # set seed # CV with support vector classifier param_grid_linear &lt;- expand.grid(C = c(0.001, 0.1, 1, 5, 10, 100)) svc_cv &lt;- train(blueprint, data = Sonar_train, method = &quot;svmLinear&quot;, trControl = cv_specs, tuneGrid = param_grid_linear, metric = &quot;Accuracy&quot;) 9.35 Your Turn!!! set.seed(022823) # set seed # CV with support vector machine with polynomial kernel param_grid_poly &lt;- expand.grid(degree = c(1, 2, 3, 4), scale = c(0.5, 1, 1.5, 2), C = c(0.001, 0.1, 1, 5, 10, 100)) svm_poly_cv &lt;- train(blueprint, data = Sonar_train, method = &quot;svmPoly&quot;, trControl = cv_specs, tuneGrid = param_grid_poly, metric = &quot;Accuracy&quot;) set.seed(022823) # set seed # CV with support vector machine with radial basis function kernel param_grid_radial &lt;- expand.grid(sigma = c(0.5, 1, 1.5, 2), C = c(0.001, 0.1, 1, 5, 10, 100)) svm_radial_cv &lt;- train(blueprint, data = Sonar_train, method = &quot;svmRadial&quot;, tuneGrid = param_grid_radial, trControl = cv_specs, metric = &quot;Accuracy&quot;) 9.36 Your Turn!!! # optimal CV Accuracies max(svc_cv$results$Accuracy) # SVC ## [1] 0.8150903 max(svm_poly_cv$results$Accuracy) # SVM with polynomial kernel ## [1] 0.8286535 max(svm_radial_cv$results$Accuracy) # SVM with radial basis function kernel ## [1] 0.5678325 9.37 Your Turn!!! # optimal hyperparameters svc_cv$bestTune # SVC ## C ## 2 0.1 svm_poly_cv$bestTune # SVM with polynomial kernel ## degree scale C ## 2 1 0.5 0.1 svm_radial_cv$bestTune # SVM with radial basis function kernel ## sigma C ## 4 0.5 5 9.38 Your Turn!!! # build final model final_model &lt;- ksvm(Class~., data = baked_train, kernel = &quot;polydot&quot;, kpar = list(degree = svm_poly_cv$bestTune$degree, scale = svm_poly_cv$bestTune$scale, offset = 1), C = svm_poly_cv$bestTune$C, prob.model = TRUE) # obtain predictions on test data final_model_class_preds &lt;- predict(final_model, newdata = baked_test, type = &quot;response&quot;) # predictions on test set 9.39 Your Turn!!! # confusion matrix confusionMatrix(data = final_model_class_preds, reference = baked_test$Class) ## Confusion Matrix and Statistics ## ## Reference ## Prediction M R ## M 27 11 ## R 6 18 ## ## Accuracy : 0.7258 ## 95% CI : (0.5977, 0.8315) ## No Information Rate : 0.5323 ## P-Value [Acc &gt; NIR] : 0.001435 ## ## Kappa : 0.4435 ## ## Mcnemar&#39;s Test P-Value : 0.331975 ## ## Sensitivity : 0.8182 ## Specificity : 0.6207 ## Pos Pred Value : 0.7105 ## Neg Pred Value : 0.7500 ## Prevalence : 0.5323 ## Detection Rate : 0.4355 ## Detection Prevalence : 0.6129 ## Balanced Accuracy : 0.7194 ## ## &#39;Positive&#39; Class : M ## 9.40 Summary of Supervised Learning Methods Technique Regression Classification Tuning Parameters 9.41 Neural Networks MNIST Handwritten Digits We will work with the famous MNIST handwritten digits dataset. We will attempt to use a neural network to classify images into digits. library(dslabs) # load library mnist &lt;- read_mnist() # load dataset mnist_train_x &lt;- mnist$train$images # training set features mnist_train_y &lt;- mnist$train$labels # training set responses mnist_test_x &lt;- mnist$test$images # test set features mnist_test_y &lt;- mnist$test$labels # test set responses 9.42 Neural Networks MNIST Handwritten Digits Figure 9.1: From ISLR2 9.43 Biological Neural Networks Figure 9.2: From Wikipedia 9.44 Artificial Neural Networks Deep Neural Networks (DNN) perform learning by mapping features to targets through a process of simple data transformations and feedback signals. At their most basic levels, neural networks have three layers an input layer, a hidden layer, and an output layer. The input layer consists of all of the original input features. The majority of the learning takes place in the hidden layer, and the output layer outputs the final predictions. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first artificial neural network. 9.45 Neural Networks Figure 9.3: From ISLR2 9.46 Neural Networks What is deep in deep neural nets? Most machine learning algorithms only have the ability to use one or two layers of data transformation to learn the output representation. We call these shallow models since they only use 1–2 representations of the feature space. As data sets continue to grow in the dimensions of the feature space, finding the optimal output representation with a shallow model is not always possible. Deep learning provides a multi-layer approach to learn data representations, typically performed with a multi-layer neural network. DNNs place an emphasis on learning successive layers of meaningful representations. DNNs perform successive non-linear transformations across each layer, allowing DNNs to model very complex and non-linear relationships. This can make DNNs suitable machine learning approaches for traditional regression and classification problems as well. But it is important to keep in mind that deep learning thrives when dimensions of your data are sufficiently large (e.g., very large training sets). As the number of observations \\(n\\) and feature inputs \\(p\\) decrease, shallow machine learning approaches tend to perform just as well, if not better, and are more efficient. 9.47 Neural Networks Consider a linear combination of the input features, that is, \\[w_{k0} + w_{k1} \\ X_1 + w_{k2} \\ X_2 + w_{k3} \\ X_3 + w_{k4} \\ X_4\\] This happens for each hidden layer node \\(A_k\\), \\(k=1, \\ldots, 5\\). Consider a non-linear activation function to transform the linear combination of input features, that is, \\[A_k = g\\left(w_{k0} + w_{k1} \\ X_1 + w_{k2} \\ X_2 + w_{k3} \\ X_3 + w_{k4} \\ X_4\\right)\\] Finally, the output is a linear combination of \\(A_k\\)’s, that is, \\[\\hat{Y} = \\hat{f}(X) = \\beta_0 + \\beta_1 \\ A_1 + \\beta_2 \\ A_2 + \\beta_3 \\ A_3 + \\beta_4 \\ A_4 + \\beta_5 \\ A_5\\] 9.48 Your Turn!!! Suppose we have two input features \\(X_1\\) and \\(X_2\\). Consider the parameters \\[\\beta_0 = 0 \\ \\ \\ \\ \\ \\beta_1 = \\frac{1}{4} \\ \\ \\ \\ \\ \\beta_2 = -\\frac{1}{4}\\] \\[w_{10} = 0 \\ \\ \\ \\ \\ w_{11} = 1 \\ \\ \\ \\ \\ w_{12} = 1\\] \\[w_{20} = 0 \\ \\ \\ \\ \\ w_{21} = 1 \\ \\ \\ \\ \\ w_{22} = -1\\] and the activation function \\(g(z)=z^2\\). What is \\(\\hat{f}(X_1, X_2)\\)? You can use the fact that \\((a+b)^2 = a^2 + 2ab + b^2\\). 9.49 Neural Networks There are multiple activation funtions to choose from but the most common ones include Figure 9.4: From ISLR2 9.50 Neural Networks To build a feedforward DNN we need four key components: Input data (the \\(X\\)’s); A pre-defined network architecture; A feedback mechanism to help the network learn; A model training approach. 9.51 Neural Networks: Input Data Feedforward DNNs require all feature inputs to be numeric. Due to the data transformation process that DNNs perform, they are highly sensitive to the individual scale of the feature values. Consequently, we should standardize our features first. Since we are working with a multinomial response (0–9), keras requires our response to be a one-hot encoded matrix, which can be accomplished with the keras function to_categorical. library(keras) mnist_train_y &lt;- to_categorical(mnist_train_y, num_classes = 10) # one-hot encode response p &lt;- ncol(mnist_train_x) # get number of features, to be used later 9.52 Neural Networks: Network Architecture It involves deciding the number of layers and nodes activation function Layers are considered dense (fully connected) when all the nodes in each successive layer are connected. Consequently, the more layers and nodes you add the more opportunities for new features to be learned (commonly referred to as the model’s capacity). The choice of output layer is driven by the modeling task. For regression problems, your output layer will contain one node that outputs the final predicted value. For binary classification problems, the output layer will still contain only one node and that node will predict the probability of success (however you define success). For multi-class classification probelms, the output layer will contain the same number of nodes as the number of classes being predicted. For the output layers we use the linear activation function for regression problems, the sigmoid activation function for binary classification problems, and softmax for multi-class classification problems. 9.53 Neural Networks: Network Architecture model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 256, activation = &quot;relu&quot;, input_shape = p) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) 9.54 Neural Networks: Network Architecture Figure 9.5: From ISLR2 9.55 Neural Networks: Feedback Mechanism On the first run (or forward pass), the DNN will select a batch of observations, randomly assign weights across all the node connections, and predict the output. The engine of neural networks is how it assesses its own accuracy and automatically adjusts the weights across all the node connections to improve that accuracy. This process is called backpropagation. To perform backpropagation we need two things: An objective function mean squared error (coded as mse) categorical cross entropy (coded as categorical_crossentropy) An optimizer stochastic gradient descent (coded as sgd) Adam (coded as adam) RMSProp (coded as rmsprop) 9.56 Neural Networks: Feedback Mechanism model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 256, activation = &quot;relu&quot;, input_shape = p) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, # adjust for regression task optimizer = optimizer_rmsprop(), metrics = c(&#39;accuracy&#39;)) # adjust for regression task 9.57 Neural Networks: Model Training batch_size: The DNN will take a batch of data to run through the optimizing process. Values are typically provided as a power of two. epochs: An epoch describes the number of times the algorithm sees the entire data set. validation_split: The model will hold out XX% of the data so that we can compute a more accurate estimate of an out-of-sample error rate. 9.58 Neural Networks: Model Training model_fit &lt;- model %&gt;% fit(x = mnist_train_x, y = mnist_train_y, epochs = 35, batch_size = 128, validation_split = 0.2, verbose = FALSE) 9.59 Neural Networks: Further Topics Other components in a neural net: batch normalization regularization and dropout adjusting learning rate of optimization Other variations of neural nets include Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN) "],["unsupervised-learning-1.html", "Chapter 10 Unsupervised Learning 10.1 Unsupervised Learning 10.2 Principal Components Analysis (PCA) 10.3 PCA: Example 10.4 PCA: Example 10.5 PCA: Data Requirements 10.6 PCA: Toy Example 10.7 PCA: Toy Example 10.8 PCA: Toy Example 10.9 PCA: First PC 10.10 PCA: First PC 10.11 PCA: Second PC 10.12 PCA: How Many PCs to Use? 10.13 Your Turn!!! 10.14 Your Turn!!! 10.15 Your Turn!!! 10.16 Your Turn!!! 10.17 Your Turn!!! 10.18 Principal Components Regression (PCR) 10.19 Principal Components Regression (PCR) 10.20 Clustering 10.21 Clustering: Applications 10.22 K-Means Clustering 10.23 K-Means Clustering 10.24 K-Means Clustering 10.25 K-Means Clustering Formulation 10.26 K-Means Clustering Algorithm 10.27 K-Means Clustering Algorithm 10.28 K-Means Clustering Algorithm 10.29 Hierarchical Clustering 10.30 Hierarchical Clustering 10.31 Hierarchical Clustering 10.32 Hierarchical Clustering: Types of Linkage 10.33 Hierarchical Clustering Algorithm 10.34 Hierarchical Clustering: Choice of Dissimilarity Measure 10.35 Practical Issues in Clustering 10.36 Your Turn!!! 10.37 Your Turn!!! 10.38 Your Turn!!! 10.39 To Sum It All Up 10.40 Next Steps", " Chapter 10 Unsupervised Learning Supervised learning problems involve a set of \\(p\\) features \\(X_1, X_2, \\ldots, X_p\\) and a response \\(Y\\) measured on \\(n\\) observations. Objective is prediction and explain (if possible) the relation between response and predictors. In unsupervised learning problems, we observe only the features \\(X_1, X_2, \\ldots, X_p\\). Objectives can be to visualize the data, or, discover subgroups among variables or among observations. We will discuss two methods: Principal Components Analysis (PCA): Used for data visualization and pre-processing. Clustering: Discover unknown subgroups in data. 10.1 Unsupervised Learning Unsupervised learning problems tend to be more subjective than supervised learning problems. Often performed as part of an exploratory data analysis. Difficult to assess the results obtained from unsupervised learning methods. It is easier to obtain unlabeled data. 10.2 Principal Components Analysis (PCA) PCA seeks a low-dimensional representation of a dataset that captures as much of the information as possible. PCA serves as a tool for Data compression Data visualization The principal components are linear combinations of the \\(p\\) original features subject to certain constraints. 10.3 PCA: Example USArrests dataset library(ISLR2) # load package data(&quot;USArrests&quot;) # load dataset head(USArrests) # first six observations ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 10.4 PCA: Example USArrests dataset cor(USArrests) # correlation matrix of variables ## Murder Assault UrbanPop Rape ## Murder 1.00000000 0.8018733 0.06957262 0.5635788 ## Assault 0.80187331 1.0000000 0.25887170 0.6652412 ## UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412 ## Rape 0.56357883 0.6652412 0.41134124 1.0000000 10.5 PCA: Data Requirements To perform dimension reduction techniques in R, generally, the data should be prepared as follows: Data are in tidy format per Wickham et al. (2014); Any missing values in the data must be removed or imputed; Typically, the data must all be numeric values (e.g., one-hot, label, ordinal encoding categorical features); Numeric data should be standardized (e.g., centered and scaled) to make features comparable. 10.6 PCA: Toy Example 10.7 PCA: Toy Example ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. 10.8 PCA: Toy Example Figure 10.1: PCs with 3 features. [Adapted from HMLR, Boehmke &amp; Greenwell] 10.9 PCA: First PC The first principal component \\(Z_1\\) of a set of features \\(X_1, X_2, \\ldots, X_p\\) is the normalized linear combination of the features that has the largest variance. By normalized, we mean \\(\\displaystyle \\sum_{j=1}^p \\phi^2_{j1}=1\\). For the \\(i^{th}\\) observation, The elements \\(\\phi_{11},\\phi_{21}, \\ldots, \\phi_{p1}\\) are loadings of the first PC. The loadings make up the first PC loading vector \\(\\phi_1=(\\phi_{11} \\ \\ \\phi_{21} \\ \\ \\ldots \\ \\ \\phi_{p1})^T\\). \\(z_{11}, z_{21}, \\ldots, z_{n1}\\) are the first PC scores. 10.10 PCA: First PC Suppose an \\(n \\times p\\) feature matrix \\(\\mathbf{X}\\). The first PC is obtained by solving 10.11 PCA: Second PC The second principal component \\(Z_2\\) is the linear combination of \\(X_1,\\ldots,X_p\\) that has maximal variance among all linear combinations that are uncorrelated with \\(Z_1\\). The second PC scores are \\(z_{12}, z_{22}, \\ldots, z_{n2}\\) where \\(\\phi_2\\) is the second PC loading vector with loadings \\(\\phi_{12},\\phi_{22}, \\ldots, \\phi_{p2}\\). \\(Z_2\\) uncorrelated with \\(Z_1\\) is equivalent to \\(\\phi_2\\) being orthogonal (perpendicular) with \\(\\phi_1\\). 10.12 PCA: How Many PCs to Use? We would like to use the smallest number of PCs required to get a good understanding of the data. CV cannot be implemented to answer this question. Two common approaches in helping to make this decision (depends on the objective and analytic workflow): Proportion of variance explained (PVE) Screeplot. Look for an elbow. 10.13 Your Turn!!! You will work with the iris dataset. Since we are in the unsupervised learning framework we will drop the Species variable which is commonly used as the response. data(&quot;iris&quot;) # load dataset iris &lt;- iris %&gt;% dplyr::select(-Species) # drop &#39;Species&#39; (1) Investigate the dataset. sum(is.na(iris)) # no missing entries ## [1] 0 summary(iris) # all variables are numerical (need to be standardized) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 10.14 Your Turn!!! (2) Perform PCA on the dataset after required preprocessing. pca &lt;- prcomp(iris, center = TRUE, scale = TRUE) # perform PCA on scaled dataset (3) What proportion of variance is explained cumulatively by the first two principal components? summary(pca) # The first two PCs explain approximately 96% of the total variation. ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.7084 0.9560 0.38309 0.14393 ## Proportion of Variance 0.7296 0.2285 0.03669 0.00518 ## Cumulative Proportion 0.7296 0.9581 0.99482 1.00000 10.15 Your Turn!!! (4) What is the sum of squared loadings for the second PC loading vector? pca # obtain loading vectors ## Standard deviations (1, .., p=4): ## [1] 1.7083611 0.9560494 0.3830886 0.1439265 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.5210659 -0.37741762 0.7195664 0.2612863 ## Sepal.Width -0.2693474 -0.92329566 -0.2443818 -0.1235096 ## Petal.Length 0.5804131 -0.02449161 -0.1421264 -0.8014492 ## Petal.Width 0.5648565 -0.06694199 -0.6342727 0.5235971 sum(pca$rotation[,2]^2) # sum of squared loadings for second PC ## [1] 1 10.16 Your Turn!!! (5) Create a biplot for the analysis. From the biplot, the loading vector for the first PC places most of its weight on which variable(s)? Similarly, the loading vector for the second PC places most of its weight on which variable(s)? biplot(pca, scale = 0, cex = 0.6) The first loading vector places most if its weight on the variables Sepal.Length, Petal.Length, and Petal.Width. The second loading vector places most if its weight on the variable Sepal.Width. 10.17 Your Turn!!! (6) From the biplot, which variables seem to be correlated with the variable Sepal.Length? The variables Petal.Length and Petal.Width are highly correlated with Sepal.Length. One can also verify that from the correlation matrix below. cor(iris) # correlation matrix ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 (7) From the biplot, which observation has the highest first PC score and which observation has the highest second PC score? Observation 119 has the highest score for the first PC, whereas observation 61 has the highest score for the second PC. 10.18 Principal Components Regression (PCR) PCA can be used to represent correlated variables with a smaller number of uncorrelated features (called principal components) and the resulting components can be used as predictors in a linear regression model. This two-step process is known as principal components regression (PCR). Figure 10.2: PCR Workflow. [Adapted from HMLR, Boehmke &amp; Greenwell] 10.19 Principal Components Regression (PCR) There are two equivalent ways of using PCA in supervised learning problems. Include step_pca() in blueprint and use method = \"lm\" while training/implementing CV. Final model is an MLR model with PCs as predictors. directly use method = \"pcr\" while training/implementing CV to choose for the optimal number of PCs. Final model can be built with the pcr function from pls library. Let’s implement that on the Ames Housing Dataset. 10.20 Clustering Broad class of techniques for finding subgroups or clusters in a dataset. Partition the data into distinct groups so that observations within each group are similar to each other. Definition of similarity depends on the context and the dataset being studied. We will talk about: K-means clustering Hierarchical clustering 10.21 Clustering: Applications Cancer research: \\(n\\) observations correspond to tissue samples for patients with different types of cancer, \\(p\\) features correspond to gene expression measurements. Market segmentation: \\(n\\) observations on \\(p\\) variables. Identify subgroups of people who might be more receptive to a particular form of advertising. Social network analysis, astronomical data analysis, organizing computing clusters etc. 10.22 K-Means Clustering Partition the dataset into a pre-specified number of \\(K\\) distinct, non-overlapping clusters. The coloring (ordering) of the clusters is arbitrary. 10.23 K-Means Clustering 10.24 K-Means Clustering The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The resulting clusters are such that each observation belongs to at least one cluster, and clusters are non-overlapping, no observation belongs to more than one cluster. 10.25 K-Means Clustering Formulation The within-cluster variation for cluster \\(C_k\\) is a measure \\(W(C_k)\\) of the amount by which the observations within a cluster differ from each other. Thus the objective is to where Combining, we have, 10.26 K-Means Clustering Algorithm 10.27 K-Means Clustering Algorithm The algorithm is guaranteed to decrease the value of the objective at each step since where \\(\\bar{x}_{kj}=\\frac{1}{|C_k|}\\sum_{i \\in C_k} x_{ij}\\): mean of \\(j^{th}\\) feature in cluster \\(C_k\\). The algorithm is not guaranteed to find the global optimum. Results depend on the initial (random) cluster assignments. It is recommended to run the algorithm multiple times from different random initial configurations. 10.28 K-Means Clustering Algorithm 10.29 Hierarchical Clustering K-means clustering requires us to pre-specify the number of clusters \\(K\\). This can be a disadvantage. Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of \\(K\\). Hierarchical clustering results in a tree-based representation of the observations, called a dendrogram. We discuss bottom-up or agglomerative clustering. This is the most common type of hierarchical clustering. The dendrogram is built starting from the leaves (bottom) and combining clusters up to the trunk (top). 10.30 Hierarchical Clustering 10.31 Hierarchical Clustering 10.32 Hierarchical Clustering: Types of Linkage 10.33 Hierarchical Clustering Algorithm 10.34 Hierarchical Clustering: Choice of Dissimilarity Measure Correlation-based distance considers two observations to be similar if their features are highly correlated. It focuses on the shapes of dissimilarity profiles rather than their magnitudes. 10.35 Practical Issues in Clustering Scaling of the variables matters. Choices in hierarchical clustering. Dissimilarity measure Type of linkage Where to cut the dendrogram? Choices in K-means clustering. What should be the value of \\(K\\)? Clustering methods are not very robust to perturbations to the data. Which features should be used for clustering? For more details, see Elements of Statistical Learning, Chapter 14. 10.36 Your Turn!!! You will work with the USArrests dataset. library(ISLR2) # load package data(&quot;USArrests&quot;) # load dataset (1) Using fviz_nbclust, choose an appropriate number of clusters separately for K-means and hierarchical clustering. (2) Implement K-means with your chosen number of clusters. Plot the resulting clusters. Mention any three states that are clustered together with Wisconsin. (3) Implement hierarchical clustering (both complete and single linkage) with your chosen number of clusters. Observe the respective dendrograms. What do you see? 10.37 Your Turn!!! Winter works at a juice-packing company where she uses a machine-learning model to predict the demand for different juice flavors. After training and evaluating the model, Winter was confident that it was ready for production. The model was deployed, and the company started using it to plan its production and distribution. During the first few months, everything was working as expected. But then, the company noticed that the model consistently overestimated the demand for certain flavors. What could be the cause of the problem with the model? The model is underfitting and needs more complexity. The model is overfitting and needs more regularization. The model is suffering from data drift. The model is suffering from sampling bias. 10.38 Your Turn!!! Winter’s been working on a model to classify photos of food. Her company is building an application that will let users snap a picture of a plate at a restaurant and show them a potential recipe so they can cook it at home. After a year of work, Winter’s model was working great. The company launched the model worldwide and started monitoring user feedback. Unfortunately, users from an Asian country complained because the model wasn’t working for them. What is the most likely reason for the problem? Winter’s model didn’t have enough complexity to learn all the data, so it’s normal to have problems with certain regions. Winter needed to train the model for more time to fully capture the dataset’s information. Winter’s model is suffering from data drift. Winter’s model is suffering from sampling bias. 10.39 To Sum It All Up 10.40 Next Steps Statistics and Data Science Minor Future readings Elements of Statistical Learning, HTF Deep Learning, GBC Optimization Algorithms Linear Algebra Programming in Python "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
